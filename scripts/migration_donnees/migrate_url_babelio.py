#!/usr/bin/env python3
"""Script de migration pour backfill les URL Babelio (Issue #124).

Ce script met √† jour UN livre et son auteur correspondant pour ajouter
le champ url_babelio en utilisant l'API Babelio.

IMPORTANT: Ce script respecte un d√©lai de 5 secondes entre CHAQUE requ√™te HTTP
vers Babelio pour √©viter le rate limiting et le bannissement d'IP.

Usage:
    PYTHONPATH=/workspaces/back-office-lmelp/src python scripts/migrate_url_babelio.py [--dry-run]

Options:
    --dry-run    Affiche les modifications sans les appliquer
"""

import argparse
import asyncio
import logging
import sys
import time
from datetime import UTC, datetime
from pathlib import Path

from bs4 import BeautifulSoup
from bson import ObjectId


# Ajouter le chemin src au PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from back_office_lmelp.services.babelio_service import BabelioService
from back_office_lmelp.services.mongodb_service import mongodb_service


logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# D√©lai minimum entre requ√™tes HTTP vers Babelio (en secondes)
# CRITIQUE: M√™me valeur que BabelioService.min_interval pour coh√©rence
MIN_REQUEST_INTERVAL = 5.0

# Timestamp de la derni√®re requ√™te HTTP (global pour tout le script)
last_request_time = 0.0


def normalize_title(title: str) -> str:
    """Normalise un titre pour comparaison (minuscules, sans accents, espaces, ligatures, ponctuation).

    Args:
        title: Titre √† normaliser

    Returns:
        Titre normalis√©

    Note:
        Doit √™tre coh√©rent avec BabelioService._calculate_similarity()
    """
    import unicodedata

    # Minuscules d'abord
    title_lower = title.lower()

    # Normaliser les apostrophes typographiques (', ', ‚Äõ, ‚Äö) vers '
    # CRITIQUE: Babelio utilise souvent des apostrophes typographiques
    title_lower = (
        title_lower.replace("\u2019", "'")  # ' (right single quotation mark)
        .replace("\u2018", "'")  # ' (left single quotation mark)
        .replace("\u201b", "'")  # ‚Äõ (single high-reversed-9 quotation mark)
        .replace("\u201a", "'")  # ‚Äö (single low-9 quotation mark)
    )

    # Normaliser les ligatures latines (≈ì‚Üíoe, √¶‚Üíae)
    # CRITIQUE: Doit √™tre fait AVANT la suppression des accents
    title_lower = title_lower.replace("≈ì", "oe").replace("√¶", "ae")

    # Retirer les accents
    title_no_accents = "".join(
        c
        for c in unicodedata.normalize("NFD", title_lower)
        if unicodedata.category(c) != "Mn"
    )

    # Retirer la ponctuation (virgules, points, tirets, etc.)
    # mais garder les lettres, chiffres et espaces
    import re

    title_no_punct = re.sub(r"[^\w\s]", " ", title_no_accents)

    # Espaces multiples
    return " ".join(title_no_punct.split())


async def wait_rate_limit() -> None:
    """Attend le d√©lai n√©cessaire pour respecter le rate limiting.

    CRITIQUE: Doit √™tre appel√© AVANT chaque requ√™te HTTP vers Babelio.
    """
    global last_request_time

    current_time = time.time()
    time_since_last = current_time - last_request_time

    if time_since_last < MIN_REQUEST_INTERVAL:
        wait_time = MIN_REQUEST_INTERVAL - time_since_last
        logger.info(
            f"‚è±Ô∏è  Rate limiting: attente de {wait_time:.1f}s avant prochaine requ√™te..."
        )
        await asyncio.sleep(wait_time)

    last_request_time = time.time()


async def scrape_title_from_page(
    babelio_service: BabelioService, url: str
) -> str | None:
    """Scrape le titre depuis une page Babelio.

    Args:
        babelio_service: Instance du service Babelio
        url: URL de la page Babelio

    Returns:
        Titre extrait ou None si erreur
    """
    try:
        # CRITIQUE: Attendre le d√©lai avant la requ√™te HTTP
        await wait_rate_limit()

        session = await babelio_service._get_session()
        async with session.get(url) as response:
            if response.status != 200:
                return None

            html = await response.text()
            soup = BeautifulSoup(html, "lxml")

            # Le titre est dans le premier <h1> de la page
            title_elem = soup.find("h1")
            if title_elem:
                return title_elem.get_text(strip=True)

            return None
    except Exception as e:
        logger.error(f"‚ùå Erreur scraping titre depuis {url}: {e}")
        return None


def load_problematic_book_ids() -> set[str]:
    """Charge les IDs des livres d√©j√† identifi√©s comme probl√©matiques depuis MongoDB.

    Returns:
        Set des livre_id d√©j√† logg√©s dans la collection babelio_problematic_cases
    """
    problematic_ids = set()

    # Lire depuis la collection MongoDB babelio_problematic_cases
    problematic_collection = mongodb_service.get_collection("babelio_problematic_cases")

    for case in problematic_collection.find():
        livre_id = case.get("livre_id")
        if livre_id:
            problematic_ids.add(livre_id)

    return problematic_ids


def log_problematic_case(
    livre_id: str,
    titre_attendu: str,
    titre_trouve: str | None,
    url_babelio: str,
    auteur: str,
    raison: str,
) -> None:
    """Log un cas probl√©matique dans MongoDB collection babelio_problematic_cases.

    Args:
        livre_id: ID du livre dans MongoDB
        titre_attendu: Titre attendu
        titre_trouve: Titre trouv√© sur Babelio (ou None)
        url_babelio: URL Babelio retourn√©e
        auteur: Nom de l'auteur
        raison: Raison du rejet
    """
    case = {
        "type": "livre",  # Identificateur de type
        "timestamp": datetime.now(UTC),
        "livre_id": str(livre_id),
        "titre_attendu": titre_attendu,
        "titre_trouve": titre_trouve,
        "url_babelio": url_babelio,
        "auteur": auteur,
        "raison": raison,
    }

    # √âcrire dans MongoDB collection babelio_problematic_cases
    problematic_collection = mongodb_service.get_collection("babelio_problematic_cases")
    problematic_collection.insert_one(case)

    logger.warning(f"‚ö†Ô∏è  Cas probl√©matique logg√©: {raison}")


def log_problematic_author(
    auteur_id,  # ObjectId ou str
    nom_auteur: str,
    nb_livres: int,
    livres_status: str,  # "all_not_found" | "orphelin" | "all_problematic"
    raison: str,
) -> None:
    """Log un cas probl√©matique d'AUTEUR dans MongoDB collection babelio_problematic_cases.

    Args:
        auteur_id: ID de l'auteur dans MongoDB (ObjectId ou str)
        nom_auteur: Nom de l'auteur
        nb_livres: Nombre de livres li√©s √† cet auteur
        livres_status: Statut des livres ("all_not_found", "orphelin", "all_problematic")
        raison: Raison du probl√®me
    """
    case = {
        "type": "auteur",  # Identificateur de type
        "timestamp": datetime.now(UTC),
        "auteur_id": str(auteur_id),  # Convertir ObjectId en string
        "nom_auteur": nom_auteur,
        "nb_livres": nb_livres,
        "livres_status": livres_status,
        "raison": raison,
    }

    # √âcrire dans MongoDB collection babelio_problematic_cases
    problematic_collection = mongodb_service.get_collection("babelio_problematic_cases")
    problematic_collection.insert_one(case)

    logger.warning(f"‚ö†Ô∏è  Auteur probl√©matique logg√©: {raison}")


async def migrate_one_book_and_author(
    babelio_service: BabelioService, dry_run: bool = False
) -> dict[str, bool | str]:
    """Migre UN livre et son auteur pour ajouter url_babelio.

    Args:
        babelio_service: Instance du service Babelio
        dry_run: Si True, affiche sans modifier

    Returns:
        Statut de la migration {book_updated, author_updated, error_type}
        error_type peut √™tre: None, 'http_error', 'scraping_error', 'validation_error'
    """
    logger.info("üîç Recherche d'un livre sans URL Babelio...")

    livres_collection = mongodb_service.get_collection("livres")
    auteurs_collection = mongodb_service.get_collection("auteurs")

    # Charger les IDs des livres d√©j√† identifi√©s comme probl√©matiques
    problematic_ids = load_problematic_book_ids()
    if problematic_ids:
        logger.info(
            f"‚ÑπÔ∏è  {len(problematic_ids)} livre(s) probl√©matique(s) seront ignor√©s"
        )

    # Construire la query pour exclure les cas probl√©matiques
    from bson import ObjectId

    query = {
        "$or": [{"url_babelio": None}, {"url_babelio": {"$exists": False}}],
        # CRITIQUE: Exclure les livres d√©j√† marqu√©s "not found" par l'utilisateur
        # Sinon ils seront re-trait√©s et re-ajout√©s au JSONL √† chaque migration
        "$nor": [{"babelio_not_found": True}],
    }

    if problematic_ids:
        # Exclure les livres d√©j√† logg√©s comme probl√©matiques
        excluded_object_ids = [ObjectId(id_str) for id_str in problematic_ids]
        query["_id"] = {"$nin": excluded_object_ids}

    # Trouver UN livre sans url_babelio (en excluant les probl√©matiques)
    livre = livres_collection.find_one(query)

    if not livre:
        logger.info("‚úÖ Tous les livres ont d√©j√† une URL Babelio")
        return None  # Aucun livre √† traiter - MigrationRunner arr√™tera la boucle

    titre = livre.get("titre", "")
    auteur_id = livre.get("auteur_id")

    logger.info(f"üìö Livre trouv√©: '{titre}' (ID: {livre['_id']})")

    # Trouver l'auteur correspondant
    auteur = None
    if auteur_id:
        auteur = auteurs_collection.find_one({"_id": auteur_id})
        if auteur:
            nom_auteur = auteur.get("nom", "")
            logger.info(f"‚úçÔ∏è  Auteur: '{nom_auteur}' (ID: {auteur['_id']})")
        else:
            logger.warning(f"‚ö†Ô∏è  Auteur ID {auteur_id} non trouv√© dans la collection")
            nom_auteur = ""
    else:
        logger.warning(f"‚ö†Ô∏è  Livre {livre['_id']} n'a pas d'auteur_id")
        nom_auteur = ""

    # V√©rifier le livre via Babelio
    logger.info(f"üåê V√©rification sur Babelio: '{titre}' par '{nom_auteur}'")

    # Initialiser les statuts
    author_already_linked = False

    try:
        # Note: verify_book() contient d√©j√† le rate limiting via search()
        # donc pas besoin d'appeler wait_rate_limit() ici
        result = await babelio_service.verify_book(titre, nom_auteur)
    except Exception as e:
        # Erreur r√©seau/timeout = probl√®me temporaire Babelio, PAS un probl√®me du livre
        # On ne log PAS dans problematic_cases car ce n'est pas un probl√®me de donn√©es
        logger.error(f"‚ùå Erreur lors de l'appel √† Babelio (timeout/r√©seau): {e}")
        logger.warning(
            "‚ö†Ô∏è  Ceci indique probablement que Babelio est temporairement indisponible"
        )
        return {
            "livre_updated": False,
            "auteur_updated": False,
            "titre": titre,
            "auteur": nom_auteur,
            "status": "error",
        }

    book_updated = False
    author_updated = False

    # Traiter le r√©sultat pour le livre
    if result.get("status") in ["verified", "corrected"]:
        url_babelio_livre = result.get("babelio_url")
        url_babelio_auteur = result.get("babelio_author_url")

        if url_babelio_livre:
            logger.info(f"üìñ URL Babelio livre: {url_babelio_livre}")

            # √âTAPE 1: V√©rification HTTP 200
            try:
                # CRITIQUE: Attendre le d√©lai avant la requ√™te HTTP
                await wait_rate_limit()

                session = await babelio_service._get_session()
                async with session.get(url_babelio_livre) as response:
                    if response.status != 200:
                        logger.warning(
                            f"‚ö†Ô∏è  URL livre invalide (HTTP {response.status})"
                        )
                        # Ne logger que les 404 (vraiment introuvable)
                        # 500/503 = probl√®me serveur Babelio, pas un probl√®me de donn√©es
                        if response.status == 404:
                            log_problematic_case(
                                livre["_id"],
                                titre,
                                None,
                                url_babelio_livre,
                                nom_auteur,
                                f"HTTP {response.status} (Not Found)",
                            )
                        else:
                            logger.warning(
                                "‚ö†Ô∏è  Ceci indique probablement que Babelio est temporairement indisponible"
                            )
                        return {
                            "livre_updated": False,
                            "auteur_updated": False,
                            "titre": titre,
                            "auteur": nom_auteur,
                            "status": "error",
                        }

                logger.info("‚úÖ URL livre v√©rifi√©e (HTTP 200)")

                # √âTAPE 2: Scraper le titre depuis la page et valider
                titre_page = await scrape_title_from_page(
                    babelio_service, url_babelio_livre
                )

                if titre_page is None:
                    logger.error("‚ùå Impossible de scraper le titre depuis la page")
                    logger.warning(
                        "‚ö†Ô∏è  Ceci peut indiquer que Babelio est temporairement indisponible"
                    )
                    logger.warning(
                        "‚ö†Ô∏è  Ou bien que la structure HTML de Babelio a chang√©"
                    )
                    # Ne PAS logger dans problematic_cases car on ne sait pas si c'est
                    # un probl√®me de donn√©es ou un probl√®me temporaire Babelio
                    return {
                        "livre_updated": False,
                        "auteur_updated": False,
                        "titre": titre,
                        "auteur": nom_auteur,
                        "status": "error",
                    }

                # Comparaison normalis√©e des titres
                titre_normalise_attendu = normalize_title(titre)
                titre_normalise_trouve = normalize_title(titre_page)

                logger.info(
                    f"üìñ Titre attendu: '{titre}' ‚Üí normalis√©: '{titre_normalise_attendu}'"
                )
                logger.info(
                    f"üìÑ Titre trouv√©: '{titre_page}' ‚Üí normalis√©: '{titre_normalise_trouve}'"
                )

                if titre_normalise_attendu != titre_normalise_trouve:
                    logger.error(
                        f"‚ùå TITRE INCORRECT ! Attendu: '{titre}', Trouv√©: '{titre_page}'"
                    )
                    log_problematic_case(
                        livre["_id"],
                        titre,
                        titre_page,
                        url_babelio_livre,
                        nom_auteur,
                        "Titre ne correspond pas",
                    )
                    return {
                        "livre_updated": False,
                        "auteur_updated": False,
                        "titre": titre,
                        "auteur": nom_auteur,
                        "status": "not_found",
                    }

                logger.info("‚úÖ Titre valid√© !")

                # √âTAPE 3: Mettre √† jour le livre dans MongoDB
                if not dry_run:
                    livres_collection.update_one(
                        {"_id": livre["_id"]},
                        {
                            "$set": {
                                "url_babelio": url_babelio_livre,
                                "updated_at": datetime.now(UTC),
                            }
                        },
                    )
                    logger.info("‚úÖ Livre mis √† jour dans MongoDB")
                else:
                    logger.info("üîç [DRY-RUN] Livre SERAIT mis √† jour")
                book_updated = True

            except Exception as e:
                logger.error(f"‚ùå Erreur v√©rification URL livre: {e}")
                log_problematic_case(
                    livre["_id"],
                    titre,
                    None,
                    url_babelio_livre,
                    nom_auteur,
                    f"Exception: {str(e)}",
                )
                return {
                    "livre_updated": False,
                    "auteur_updated": False,
                    "titre": titre,
                    "auteur": nom_auteur,
                    "status": "error",
                }
        else:
            logger.warning("‚ö†Ô∏è  URL Babelio livre manquante dans la r√©ponse")

        # Mettre √† jour l'auteur si trouv√©
        if auteur and url_babelio_auteur:
            # V√©rifier si l'auteur n'a pas d√©j√† une URL
            if not auteur.get("url_babelio"):
                logger.info(f"üë§ URL Babelio auteur: {url_babelio_auteur}")

                # V√©rification HTTP 200 pour l'URL auteur
                try:
                    # CRITIQUE: Attendre le d√©lai avant la requ√™te HTTP
                    await wait_rate_limit()

                    session = await babelio_service._get_session()
                    async with session.get(url_babelio_auteur) as response:
                        if response.status == 200:
                            logger.info(
                                f"‚úÖ URL auteur v√©rifi√©e (HTTP {response.status})"
                            )
                            if not dry_run:
                                auteurs_collection.update_one(
                                    {"_id": auteur["_id"]},
                                    {
                                        "$set": {
                                            "url_babelio": url_babelio_auteur,
                                            "updated_at": datetime.now(UTC),
                                        }
                                    },
                                )
                                logger.info("‚úÖ Auteur mis √† jour dans MongoDB")
                            else:
                                logger.info("üîç [DRY-RUN] Auteur SERAIT mis √† jour")
                            author_updated = True
                        else:
                            logger.warning(
                                f"‚ö†Ô∏è  URL auteur invalide (HTTP {response.status})"
                            )
                except Exception as e:
                    logger.error(f"‚ùå Erreur v√©rification URL auteur: {e}")
            else:
                logger.info(
                    f"‚ÑπÔ∏è  Auteur a d√©j√† une URL Babelio: {auteur.get('url_babelio')}"
                )
                # Indiquer que l'auteur √©tait d√©j√† li√© (pas une erreur)
                author_already_linked = True
        elif auteur and not url_babelio_auteur:
            logger.warning("‚ö†Ô∏è  URL Babelio auteur manquante dans la r√©ponse")
    else:
        # Livre not_found, error, ou autre statut non g√©r√©
        status = result.get("status")
        logger.warning(f"‚ùå Livre non trait√© (status: {status})")

        # Logger TOUS les cas non-success pour √©viter de les re-traiter ind√©finiment
        # Ceci inclut: not_found, error, et tout autre statut inattendu
        log_problematic_case(
            livre["_id"],
            titre,
            None,
            "N/A",
            nom_auteur,
            f"Livre non trait√© - status: {status}",
        )

    # Retourner les infos compl√®tes pour MigrationRunner
    return {
        "livre_updated": book_updated,
        "auteur_updated": author_updated,
        "auteur_already_linked": author_already_linked,
        "titre": titre,
        "auteur": nom_auteur,
        "status": result.get("status", "error"),
    }


async def scrape_author_url_from_book_page(book_url: str) -> str | None:
    """Scrape l'URL auteur depuis la page Babelio d'un livre.

    Args:
        book_url: URL de la page Babelio du livre

    Returns:
        URL de l'auteur ou None si non trouv√©

    Example:
        >>> url = await scrape_author_url_from_book_page(
        ...     "https://www.babelio.com/livres/Orwell-1984/1234"
        ... )
        >>> print(url)
        https://www.babelio.com/auteur/George-Orwell/5678
    """
    try:
        # CRITIQUE: Attendre le d√©lai avant la requ√™te HTTP
        await wait_rate_limit()

        # R√©cup√©rer le service Babelio pour utiliser sa session HTTP
        from back_office_lmelp.services.babelio_service import BabelioService

        babelio_service = BabelioService()
        session = await babelio_service._get_session()

        async with session.get(book_url) as response:
            if response.status != 200:
                logger.warning(
                    f"‚ö†Ô∏è  Erreur HTTP {response.status} lors du scraping de {book_url}"
                )
                return None

            html = await response.text()
            soup = BeautifulSoup(html, "lxml")

            # Chercher le lien auteur dans la page
            # Format: <a href="/auteur/Nom-Prenom/12345" class="...">
            author_link = soup.find("a", href=lambda x: x and "/auteur/" in x)
            if author_link and author_link.get("href"):
                author_path = author_link["href"]
                # Convertir path relatif en URL absolue
                if author_path.startswith("/"):
                    author_url = f"https://www.babelio.com{author_path}"
                    logger.info(f"‚úÖ URL auteur trouv√©e: {author_url}")
                    return author_url

            logger.warning(f"‚ö†Ô∏è  Aucune URL auteur trouv√©e dans {book_url}")
            return None

    except Exception as e:
        logger.error(f"‚ùå Erreur lors du scraping de {book_url}: {e}")
        return None


async def get_all_books_to_complete() -> list[dict]:
    """R√©cup√®re TOUS les livres dont les auteurs doivent √™tre compl√©t√©s.

    Approche batch: charge tous les livres en UNE FOIS pour √©viter
    de rejouer plusieurs fois sur le m√™me livre en cas d'erreur.

    Returns:
        Liste de dicts avec: livre_id, titre, auteur, auteur_id, url_babelio

    Example:
        >>> books = await get_all_books_to_complete()
        >>> print(len(books))
        15
        >>> print(books[0])
        {
            "livre_id": ObjectId("..."),
            "titre": "1984",
            "auteur": "George Orwell",
            "auteur_id": ObjectId("..."),
            "url_babelio": "https://www.babelio.com/livres/..."
        }
    """
    livres_collection = mongodb_service.get_collection("livres")
    prob_collection = mongodb_service.get_collection("babelio_problematic_cases")

    # R√©cup√©rer les IDs de livres d√©j√† logg√©s comme probl√©matiques
    problematic_cases = prob_collection.find({}, {"livre_id": 1})
    # CRITICAL: Convertir les IDs de STRING vers ObjectId pour le filtre MongoDB
    # log_problematic_case() stocke livre_id en string mais MongoDB _id est ObjectId
    problematic_livre_ids = [ObjectId(case["livre_id"]) for case in problematic_cases]

    # Chercher TOUS les livres qui ont une URL Babelio mais dont l'auteur n'en a pas
    # Pipeline d'aggregation pour joindre livres et auteurs
    pipeline = [
        # Livres avec URL Babelio ET non logg√©s dans problematic_cases
        {
            "$match": {
                "url_babelio": {"$exists": True, "$ne": None},
                "_id": {"$nin": problematic_livre_ids},
            }
        },
        # Joindre avec auteurs
        {
            "$lookup": {
                "from": "auteurs",
                "localField": "auteur_id",
                "foreignField": "_id",
                "as": "auteur_info",
            }
        },
        # D√©plier le tableau auteur_info
        {"$unwind": "$auteur_info"},
        # Filtrer: auteur SANS URL Babelio
        {
            "$match": {
                "$or": [
                    {"auteur_info.url_babelio": {"$exists": False}},
                    {"auteur_info.url_babelio": None},
                ]
            }
        },
        # Pas de limit: r√©cup√©rer TOUS les livres
    ]

    livre_cursor = livres_collection.aggregate(pipeline)

    # Construire la liste de tous les livres √† traiter
    books_to_process = []
    for livre in livre_cursor:
        auteur_info = livre.get("auteur_info", {})
        books_to_process.append(
            {
                "livre_id": livre["_id"],
                "titre": livre.get("titre", "Unknown"),
                "auteur": auteur_info.get("nom", "Unknown"),
                "auteur_id": auteur_info.get("_id"),
                "url_babelio": livre.get("url_babelio"),
            }
        )

    logger.info(f"üìã {len(books_to_process)} livres √† traiter en batch")
    return books_to_process


async def get_all_authors_to_complete() -> list[dict]:
    """R√©cup√®re TOUS les auteurs sans url_babelio √† traiter.

    Pour chaque auteur, r√©cup√®re aussi ses livres avec leurs d√©tails
    (url_babelio, babelio_not_found) pour pouvoir d√©terminer comment
    traiter l'auteur.

    Returns:
        Liste de dicts avec:
        {
            "auteur_id": ObjectId,
            "nom": str,
            "livres": [
                {
                    "livre_id": ObjectId,
                    "titre": str,
                    "url_babelio": str | None,
                    "babelio_not_found": bool | None
                }
            ]
        }
    """
    auteurs_collection = mongodb_service.get_collection("auteurs")

    # Pipeline: auteurs sans url_babelio + leurs livres
    pipeline = [
        # Auteurs sans URL Babelio ET pas marqu√©s "not found"
        {
            "$match": {
                "$and": [
                    {
                        "$or": [
                            {"url_babelio": {"$exists": False}},
                            {"url_babelio": None},
                        ]
                    },
                    # Exclure les auteurs marqu√©s "absent de Babelio"
                    {
                        "$or": [
                            {"babelio_not_found": {"$exists": False}},
                            {"babelio_not_found": False},
                        ]
                    },
                ]
            }
        },
        # Joindre avec livres
        {
            "$lookup": {
                "from": "livres",
                "localField": "_id",
                "foreignField": "auteur_id",
                "as": "livres_info",
            }
        },
        # Projeter les champs n√©cessaires
        {
            "$project": {
                "auteur_id": "$_id",
                "nom": 1,
                "livres": {
                    "$map": {
                        "input": "$livres_info",
                        "as": "livre",
                        "in": {
                            "livre_id": "$$livre._id",
                            "titre": "$$livre.titre",
                            "url_babelio": "$$livre.url_babelio",
                            "babelio_not_found": "$$livre.babelio_not_found",
                        },
                    }
                },
            }
        },
    ]

    auteur_cursor = auteurs_collection.aggregate(pipeline)

    # IMPORTANT: Convertir le curseur en liste pour √©viter qu'il soit √©puis√©
    all_authors = list(auteur_cursor)
    logger.info(f"üîç {len(all_authors)} auteurs sans URL trouv√©s dans MongoDB")

    # R√©cup√©rer les IDs des auteurs d√©j√† probl√©matiques
    problematic_collection = mongodb_service.get_collection("babelio_problematic_cases")
    problematic_authors = list(problematic_collection.find({"type": "auteur"}))
    problematic_author_ids = {
        ObjectId(doc["auteur_id"]) for doc in problematic_authors if "auteur_id" in doc
    }
    logger.info(
        f"‚ö†Ô∏è  {len(problematic_author_ids)} auteurs d√©j√† probl√©matiques √† exclure"
    )

    # Construire la liste en excluant les auteurs d√©j√† probl√©matiques
    authors_to_process = []
    for auteur in all_authors:
        auteur_id = auteur.get("auteur_id")
        if auteur_id not in problematic_author_ids:
            authors_to_process.append(auteur)
        else:
            logger.info(
                f"‚è≠Ô∏è  Auteur '{auteur.get('nom')}' (ID={auteur_id}) d√©j√† dans problematic_cases, ignor√©"
            )

    logger.info(f"üìã {len(authors_to_process)} auteurs √† traiter apr√®s filtrage")
    return authors_to_process


async def process_one_author(author_data: dict, dry_run: bool = False) -> dict:
    """Traite un auteur sans url_babelio en examinant ses livres.

    Args:
        author_data: Dict avec auteur_id, nom, livres (liste)
        dry_run: Si True, affiche sans modifier

    Returns:
        Dict avec status, auteur_updated, nom_auteur, raison

    Logique:
    1. Si l'auteur a un livre avec url_babelio ‚Üí scraper l'URL auteur
    2. Si tous les livres sont probl√©matiques ‚Üí marquer auteur probl√©matique
    3. Si tous les livres sont not_found ‚Üí marquer auteur absent de Babelio
    4. Si auteur orphelin (pas de livres) ‚Üí marquer probl√©matique
    """
    auteur_id = author_data["auteur_id"]
    nom_auteur = author_data["nom"]
    livres = author_data["livres"]

    logger.info(f"üîÑ Traitement auteur: '{nom_auteur}' ({len(livres)} livres)")

    # Cas 1: Auteur orphelin (pas de livres)
    if not livres:
        logger.warning(
            f"‚ö†Ô∏è  Auteur orphelin: '{nom_auteur}' - Logging problematic author"
        )
        log_problematic_author(
            auteur_id=auteur_id,
            nom_auteur=nom_auteur,
            nb_livres=0,
            livres_status="orphelin",
            raison="Auteur sans livres li√©s",
        )
        return {
            "status": "error",
            "auteur_updated": False,
            "nom_auteur": nom_auteur,
            "raison": "Auteur orphelin (pas de livres)",
        }

    # Chercher un livre avec url_babelio
    livre_avec_url = None
    for livre in livres:
        if livre.get("url_babelio"):
            livre_avec_url = livre
            break

    # Cas 2: Un livre a une url_babelio ‚Üí scraper l'URL de l'auteur
    if livre_avec_url:
        logger.info(
            f"üìö Livre trouv√© avec URL: '{livre_avec_url['titre']}' - "
            f"Scraping URL auteur..."
        )
        await wait_rate_limit()
        author_url = await scrape_author_url_from_book_page(
            livre_avec_url["url_babelio"]
        )

        if author_url is None:
            logger.warning(
                f"‚ùå Impossible de r√©cup√©rer l'URL auteur pour '{nom_auteur}' - "
                f"Logging problematic case"
            )
            log_problematic_case(
                livre_id=str(livre_avec_url["livre_id"]),
                titre_attendu=livre_avec_url["titre"],
                titre_trouve=None,
                url_babelio=livre_avec_url["url_babelio"],
                auteur=nom_auteur,
                raison="Impossible de r√©cup√©rer l'URL de l'auteur depuis la page du livre",
            )
            return {
                "status": "error",
                "auteur_updated": False,
                "nom_auteur": nom_auteur,
                "raison": "Scraping failed",
            }

        # Succ√®s - mettre √† jour l'auteur
        if dry_run:
            logger.info(
                f"[DRY RUN] Mettre √† jour auteur '{nom_auteur}' avec URL: {author_url}"
            )
            return {
                "status": "success",
                "auteur_updated": False,
                "nom_auteur": nom_auteur,
                "raison": "Dry run",
            }

        auteurs_collection = mongodb_service.get_collection("auteurs")
        result = auteurs_collection.update_one(
            {"_id": auteur_id}, {"$set": {"url_babelio": author_url}}
        )

        if result.matched_count > 0:
            logger.info(f"‚úÖ Auteur '{nom_auteur}' mis √† jour avec URL: {author_url}")
            return {
                "status": "success",
                "auteur_updated": True,
                "nom_auteur": nom_auteur,
                "raison": "URL trouv√©e et mise √† jour",
            }
        else:
            logger.error(f"‚ùå √âchec mise √† jour auteur '{nom_auteur}'")
            return {
                "status": "error",
                "auteur_updated": False,
                "nom_auteur": nom_auteur,
                "raison": "Update failed",
            }

    # Cas 3: V√©rifier si tous les livres sont not_found
    all_not_found = all(livre.get("babelio_not_found") for livre in livres)
    if all_not_found:
        logger.warning(
            f"‚ö†Ô∏è  Tous les livres de '{nom_auteur}' sont absents de Babelio - "
            f"Logging problematic author (traitement manuel requis)"
        )
        # Ne PAS marquer l'auteur comme not_found car l'auteur peut exister
        # sur Babelio m√™me si ses livres n'y sont pas
        log_problematic_author(
            auteur_id=auteur_id,
            nom_auteur=nom_auteur,
            nb_livres=len(livres),
            livres_status="all_not_found",
            raison=f"Tous les livres sont absents de Babelio: {[livre['titre'] for livre in livres]}",
        )
        return {
            "status": "error",
            "auteur_updated": False,
            "nom_auteur": nom_auteur,
            "raison": "Tous les livres sont absents de Babelio",
        }

    # Cas 4: Livres sont dans problematic_cases ou mix
    logger.warning(
        f"‚ö†Ô∏è  Auteur '{nom_auteur}': livres probl√©matiques ou mix - "
        f"Logging problematic author"
    )
    log_problematic_author(
        auteur_id=auteur_id,
        nom_auteur=nom_auteur,
        nb_livres=len(livres),
        livres_status="all_problematic",
        raison=f"Livres probl√©matiques: {[livre['titre'] for livre in livres]}",
    )
    return {
        "status": "error",
        "auteur_updated": False,
        "nom_auteur": nom_auteur,
        "raison": "Livres probl√©matiques",
    }


async def process_one_book_author(book_data: dict, dry_run: bool = False) -> dict:
    """Traite un livre du batch et compl√®te l'URL Babelio de son auteur.

    Cette fonction est appel√©e pour chaque livre dans la liste retourn√©e
    par get_all_books_to_complete(). Elle ne fait PAS de requ√™te MongoDB
    pour chercher le livre, elle utilise directement les donn√©es fournies.

    Args:
        book_data: Dict contenant livre_id, titre, auteur, auteur_id, url_babelio
        dry_run: Si True, affiche les modifications sans les appliquer

    Returns:
        Dict avec status, auteur_updated, titre, auteur

    Example:
        >>> book = {
        ...     "livre_id": ObjectId("..."),
        ...     "titre": "1984",
        ...     "auteur": "George Orwell",
        ...     "auteur_id": ObjectId("..."),
        ...     "url_babelio": "https://www.babelio.com/livres/Orwell-1984/1234"
        ... }
        >>> result = await process_one_book_author(book, dry_run=False)
        >>> result["status"]
        'success'
    """
    livre_id = book_data["livre_id"]
    titre = book_data["titre"]
    auteur = book_data["auteur"]
    auteur_id = book_data["auteur_id"]
    url_babelio = book_data["url_babelio"]

    logger.info(f"üîÑ Traitement livre: '{titre}' par {auteur}")

    # Scraper l'URL de l'auteur depuis la page du livre
    await wait_rate_limit()
    author_url = await scrape_author_url_from_book_page(url_babelio)

    if author_url is None:
        # √âchec du scraping - logger le cas probl√©matique
        logger.warning(
            f"‚ùå Impossible de r√©cup√©rer l'URL auteur pour '{titre}' - "
            f"Logging problematic case"
        )
        log_problematic_case(
            livre_id=str(livre_id),
            titre_attendu=titre,
            titre_trouve=None,
            url_babelio=url_babelio,
            auteur=auteur,
            raison="Impossible de r√©cup√©rer l'URL de l'auteur depuis la page du livre",
        )
        return {
            "status": "error",
            "auteur_updated": False,
            "titre": titre,
            "auteur": auteur,
        }

    # Succ√®s - mettre √† jour l'auteur dans MongoDB
    if dry_run:
        logger.info(f"[DRY RUN] Mettre √† jour auteur '{auteur}' avec URL: {author_url}")
        return {
            "status": "success",
            "auteur_updated": False,  # False car pas vraiment mis √† jour en dry_run
            "titre": titre,
            "auteur": auteur,
        }

    auteurs_collection = mongodb_service.get_collection("auteurs")
    result = auteurs_collection.update_one(
        {"_id": auteur_id}, {"$set": {"url_babelio": author_url}}
    )

    if result.matched_count > 0:
        logger.info(f"‚úÖ Auteur '{auteur}' mis √† jour avec URL: {author_url}")
        return {
            "status": "success",
            "auteur_updated": True,
            "titre": titre,
            "auteur": auteur,
        }
    else:
        logger.error(f"‚ùå √âchec mise √† jour auteur '{auteur}' (ID: {auteur_id})")
        return {
            "status": "error",
            "auteur_updated": False,
            "titre": titre,
            "auteur": auteur,
        }


async def complete_missing_authors(dry_run: bool = False) -> dict | None:
    """Compl√®te les auteurs manquants pour les livres qui ont d√©j√† une URL Babelio.

    Cette fonction traite les livres qui ont d√©j√† une url_babelio
    mais dont l'auteur n'en a pas encore. Elle scrape la page du livre
    pour r√©cup√©rer l'URL de l'auteur.

    Args:
        dry_run: Si True, affiche les modifications sans les appliquer

    Returns:
        Dict avec les infos de traitement ou None si aucun auteur √† compl√©ter

    Example:
        >>> result = await complete_missing_authors(dry_run=False)
        >>> print(result)
        {
            "auteur_updated": True,
            "titre": "1984",
            "auteur": "George Orwell",
            "status": "success"
        }
    """
    livres_collection = mongodb_service.get_collection("livres")
    auteurs_collection = mongodb_service.get_collection("auteurs")
    prob_collection = mongodb_service.get_collection("babelio_problematic_cases")

    # R√©cup√©rer les IDs de livres d√©j√† logg√©s comme probl√©matiques
    problematic_cases = prob_collection.find({}, {"livre_id": 1})
    # CRITICAL: Convertir les IDs de STRING vers ObjectId pour le filtre MongoDB
    # log_problematic_case() stocke livre_id en string (line 189) mais MongoDB _id est ObjectId
    problematic_livre_ids = [ObjectId(case["livre_id"]) for case in problematic_cases]

    # Chercher un livre qui a une URL Babelio mais dont l'auteur n'en a pas
    # Pipeline d'aggregation pour joindre livres et auteurs
    pipeline = [
        # Livres avec URL Babelio ET non logg√©s dans problematic_cases
        {
            "$match": {
                "url_babelio": {"$exists": True, "$ne": None},
                "_id": {"$nin": problematic_livre_ids},
            }
        },
        # Joindre avec auteurs
        {
            "$lookup": {
                "from": "auteurs",
                "localField": "auteur_id",
                "foreignField": "_id",
                "as": "auteur_info",
            }
        },
        # D√©plier le tableau auteur_info
        {"$unwind": "$auteur_info"},
        # Filtrer: auteur SANS URL Babelio
        {
            "$match": {
                "$or": [
                    {"auteur_info.url_babelio": {"$exists": False}},
                    {"auteur_info.url_babelio": None},
                ]
            }
        },
        # Limiter √† 1 r√©sultat
        {"$limit": 1},
    ]

    livre_cursor = livres_collection.aggregate(pipeline)
    livre = None
    for doc in livre_cursor:
        livre = doc
        break

    if not livre:
        logger.info("‚úÖ Aucun auteur √† compl√©ter")
        return None

    titre = livre.get("titre", "Unknown")
    auteur_info = livre.get("auteur_info", {})
    nom_auteur = auteur_info.get("nom", "Unknown")
    auteur_id = auteur_info.get("_id")
    url_babelio_livre = livre.get("url_babelio")

    logger.info(f"üìö Livre: {titre} ({nom_auteur})")
    logger.info(f"üîó URL livre: {url_babelio_livre}")
    logger.info(f"üë§ Auteur sans URL Babelio: {nom_auteur}")

    # Scraper la page du livre pour r√©cup√©rer l'URL auteur
    url_babelio_auteur = await scrape_author_url_from_book_page(url_babelio_livre)

    if not url_babelio_auteur:
        logger.warning("‚ùå Impossible de r√©cup√©rer l'URL auteur")
        # Logger le cas probl√©matique pour √©viter de le re-traiter en boucle
        log_problematic_case(
            livre["_id"],
            titre,
            None,
            url_babelio_livre,
            nom_auteur,
            "Failed to scrape author URL from book page",
        )
        return {
            "auteur_updated": False,
            "titre": titre,
            "auteur": nom_auteur,
            "status": "error",
        }

    # Mettre √† jour l'auteur dans MongoDB
    if not dry_run:
        auteurs_collection.update_one(
            {"_id": auteur_id},
            {
                "$set": {
                    "url_babelio": url_babelio_auteur,
                    "updated_at": datetime.now(UTC),
                }
            },
        )
        logger.info(f"‚úÖ Auteur {nom_auteur} mis √† jour avec URL Babelio")
    else:
        logger.info(f"üîç [DRY-RUN] Auteur SERAIT mis √† jour: {url_babelio_auteur}")

    return {
        "auteur_updated": not dry_run,
        "titre": titre,
        "auteur": nom_auteur,
        "status": "success",
    }


async def main():
    """Point d'entr√©e principal du script de migration."""
    parser = argparse.ArgumentParser(
        description="Backfill URL Babelio pour UN livre et son auteur"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Mode dry-run: affiche sans modifier",
    )
    args = parser.parse_args()

    if args.dry_run:
        logger.info("üîç MODE DRY-RUN ACTIV√â - Aucune modification ne sera appliqu√©e")

    # Initialiser la connexion MongoDB
    if not mongodb_service.connect():
        logger.error("‚ùå Impossible de se connecter √† MongoDB")
        return

    logger.info("‚úÖ Connexion MongoDB √©tablie")

    # Initialiser le service Babelio
    babelio_service = BabelioService()

    try:
        # Migrer un livre et son auteur
        result = await migrate_one_book_and_author(babelio_service, args.dry_run)

        # Afficher le r√©sum√©
        logger.info("\n" + "=" * 60)
        logger.info("R√âSUM√â DE LA MIGRATION")
        logger.info("=" * 60)
        logger.info(
            f"üìö Livre mis √† jour: {'‚úÖ Oui' if result['book_updated'] else '‚ùå Non'}"
        )
        logger.info(
            f"‚úçÔ∏è  Auteur mis √† jour: {'‚úÖ Oui' if result['author_updated'] else '‚ùå Non'}"
        )
        logger.info("=" * 60)

    finally:
        await babelio_service.close()


if __name__ == "__main__":
    asyncio.run(main())
