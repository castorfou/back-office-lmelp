"""Service de v√©rification orthographique via Babelio.com

Ce service utilise l'API AJAX de Babelio pour v√©rifier et corriger l'orthographe
des auteurs, livres et √©diteurs extraits des avis critiques.

Architecture :
- Endpoint : POST https://www.babelio.com/aj_recherche.php
- Format : JSON {"term": "search_term", "isMobile": false}
- Headers : Content-Type: application/json, X-Requested-With: XMLHttpRequest
- Cookies : disclaimer=1, p=FR (n√©cessaires pour √©viter les blocages)
- Rate limiting : 5.0 sec entre requ√™tes (√©viter le rate limiting Babelio - Issue #124)

D√©couverte technique bas√©e sur l'analyse DevTools :
- Babelio tol√®re les fautes d'orthographe (Houllebeck -> Houellebecq)
- Retourne des r√©sultats mixtes : auteurs, livres, s√©ries
- Format de r√©ponse : Array JSON avec type "auteurs"/"livres"/"series"
- IDs uniques Babelio pour construire les URLs canoniques

Usage :
    service = BabelioService()
    result = await service.verify_author("Michel Houellebecq")
    # result['status'] = 'verified'|'corrected'|'not_found'|'error'
    await service.close()  # Fermer la session HTTP
"""

import asyncio
import json
import logging
import os
from difflib import SequenceMatcher
from typing import Any

import aiohttp
from bs4 import BeautifulSoup


logger = logging.getLogger(__name__)


class BabelioService:
    """Service de v√©rification orthographique via l'API AJAX de Babelio.

    Ce service respecte les bonnes pratiques :
    - Rate limiting √† 5.0 sec entre requ√™tes (√©viter blocage - Issue #124)
    - Headers et cookies appropri√©s pour √©viter les blocages
    - Gestion d'erreur robuste (timeout, r√©seau, parsing JSON)
    - Session HTTP r√©utilisable et fermeture propre
    - Calcul de scores de confiance pour les suggestions

    Attributes:
        base_url: URL de base de Babelio
        search_endpoint: Endpoint AJAX pour la recherche
        session: Session aiohttp r√©utilisable
        rate_limiter: Semaphore pour limiter √† 1 req simultan√©e
        min_interval: D√©lai minimum entre requ√™tes (5.0 sec par d√©faut)
    """

    def __init__(self):
        """Initialise le service Babelio avec configuration appropri√©e."""
        self.base_url = "https://www.babelio.com"
        self.search_endpoint = "/aj_recherche.php"
        self.session: aiohttp.ClientSession | None = None
        self.rate_limiter = asyncio.Semaphore(1)  # 1 requ√™te simultan√©e max
        self.last_request_time = 0  # Timestamp de la derni√®re requ√™te
        self.min_interval = 5.0  # D√©lai minimum de 5.0 secondes entre requ√™tes (Issue #124: √©viter rate limiting)
        self._cache = {}  # Cache simple terme -> r√©sultats (limit√© en taille)
        # Optional disk-backed cache service injected at app startup
        self.cache_service: Any | None = None
        # Contr√¥le des logs temporaires pour le cache disque (utile en dev)
        # Si la variable d'environnement BABELIO_CACHE_LOG est '1' ou 'true',
        # on exposera des logs plus verbeux (info) pour hit/miss et √©criture.
        self._cache_log_enabled = os.getenv("BABELIO_CACHE_LOG", "0").lower() in (
            "1",
            "true",
            "yes",
        )
        # Logs de debug pour analyser les requ√™tes et r√©sultats Babelio
        # Activ√© avec BABELIO_DEBUG_LOG=1 (utile pour diagnostiquer probl√®mes de matching)
        self._debug_log_enabled = os.getenv("BABELIO_DEBUG_LOG", "0").lower() in (
            "1",
            "true",
            "yes",
        )
        # If verbose cache logging is enabled, make sure our module logger
        # will emit INFO messages to stdout (useful when running under uvicorn)
        if self._cache_log_enabled or self._debug_log_enabled:
            try:
                logger.setLevel(logging.INFO)
                # add a stdout handler only if no handlers are configured for this logger
                if not logger.handlers:
                    handler = logging.StreamHandler()
                    handler.setLevel(logging.INFO)
                    handler.setFormatter(
                        logging.Formatter(
                            "%(asctime)s %(levelname)s %(name)s: %(message)s"
                        )
                    )
                    logger.addHandler(handler)
            except Exception:
                # Don't fail startup if logging setup has issues
                pass

    async def _get_session(self) -> aiohttp.ClientSession:
        """R√©cup√®re ou cr√©e la session HTTP avec configuration appropri√©e."""
        if self.session is None or self.session.closed:
            # Timeout configuration respectueuse
            timeout = aiohttp.ClientTimeout(total=10, connect=5)

            self.session = aiohttp.ClientSession(
                timeout=timeout,
                headers=self._get_default_headers(),
                cookies=self._get_default_cookies(),
            )
        return self.session

    def _get_default_headers(self) -> dict[str, str]:
        """Retourne les headers n√©cessaires pour Babelio.

        Bas√© sur l'analyse des vraies requ√™tes DevTools.
        """
        return {
            "Content-Type": "application/json",
            "Accept": "application/json, text/javascript, */*; q=0.01",
            "User-Agent": "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:142.0) Gecko/20100101 Firefox/142.0",
            "X-Requested-With": "XMLHttpRequest",
            "Referer": "https://www.babelio.com/recherche.php",
            "Origin": "https://www.babelio.com",
            "Accept-Language": "fr,en-US;q=0.7,en;q=0.3",
            "DNT": "1",
            "Sec-GPC": "1",
        }

    def _get_default_cookies(self) -> dict[str, str]:
        """Retourne les cookies n√©cessaires pour Babelio.

        Ces cookies √©vitent les blocages et disclaimers.
        """
        return {
            "p": "FR",  # Pays = France
            "disclaimer": "1",  # Disclaimer accept√©
            "g_state": '{"i_l":0}',  # √âtat Google (JSON string)
            "bbacml": "0",  # Cookie marketing
        }

    async def search(self, term: str) -> list[dict[str, Any]]:
        """Effectue une recherche sur Babelio.

        Args:
            term: Terme √† rechercher (auteur, livre, etc.)

        Returns:
            list[dict]: R√©sultats JSON de Babelio, format:
                [
                    {
                        "id": "2180",
                        "prenoms": "Michel",
                        "nom": "Houellebecq",
                        "type": "auteurs",
                        "url": "/auteur/Michel-Houellebecq/2180",
                        "ca_oeuvres": "38",
                        "ca_membres": "30453"
                    },
                    {
                        "id_oeuvre": "1770",
                        "titre": "Les particules √©l√©mentaires",
                        "nom": "Houellebecq",
                        "prenoms": "Michel",
                        "type": "livres",
                        "url": "/livres/Houellebecq-Les-particules-elementaires/1770"
                    }
                ]

        Raises:
            Aucune exception - retourne [] en cas d'erreur
        """
        if not term or not term.strip():
            return []

        # V√©rifier le cache disque/in-memory d'abord
        cache_key = term.strip().lower()

        # Support pour un service de cache disque inject√© (BabelioCacheService)
        cache_service = getattr(self, "cache_service", None)
        if cache_service is not None:
            try:
                # disk IO can block; run in threadpool to avoid blocking event loop
                wrapper = await asyncio.to_thread(cache_service.get_cached, term)
                # choose log function based on env control
                log_fn = (
                    logger.info
                    if getattr(self, "_cache_log_enabled", False)
                    else logger.debug
                )

                if wrapper is not None:
                    # original-term hit
                    items = None
                    if isinstance(wrapper, dict):
                        items = len(wrapper.get("data") or [])
                    log_fn(
                        f"[BabelioCache] HIT (orig) key='{term}' items={items} ts={getattr(wrapper, 'get', lambda *_: None)('ts')}"
                    )
                    if self._debug_log_enabled:
                        logger.info(
                            f"üîç [DEBUG] search: CACHE HIT (orig) - returning {items} cached result(s)"
                        )
                    # Ensure we always return a list (function annotated -> list[dict])
                    if isinstance(wrapper, dict):
                        return list(wrapper.get("data") or [])
                    return list(wrapper or [])

                # try lowercased key too for compatibility
                wrapper = await asyncio.to_thread(cache_service.get_cached, cache_key)
                if wrapper is not None:
                    items = None
                    if isinstance(wrapper, dict):
                        items = len(wrapper.get("data") or [])
                    log_fn(
                        f"[BabelioCache] HIT (norm) key='{cache_key}' items={items} ts={getattr(wrapper, 'get', lambda *_: None)('ts')}"
                    )
                    if self._debug_log_enabled:
                        logger.info(
                            f"üîç [DEBUG] search: CACHE HIT (norm) - returning {items} cached result(s)"
                        )
                    # Ensure we always return a list (function annotated -> list[dict])
                    if isinstance(wrapper, dict):
                        return list(wrapper.get("data") or [])
                    return list(wrapper or [])

                # both misses
                log_fn(f"[BabelioCache] MISS keys=(orig='{term}', norm='{cache_key}')")
            except Exception:
                # on any cache error, treat as cache miss and continue
                logger.exception(
                    "Erreur lors de l'acc√®s au cache disque; fallback r√©seau"
                )

        # Backwards-compatible in-memory cache
        if cache_key in self._cache:
            logger.debug(f"Cache m√©moire hit pour: {term}")
            return self._cache[cache_key]  # type: ignore[no-any-return]

        # Respect du rate limiting avec d√©lai obligatoire
        async with self.rate_limiter:
            # Calculer le temps d'attente n√©cessaire
            import time

            current_time = time.time()
            time_since_last = current_time - self.last_request_time

            if time_since_last < self.min_interval:
                wait_time = self.min_interval - time_since_last
                logger.debug(f"Rate limiting: attente de {wait_time:.1f}s")
                await asyncio.sleep(wait_time)

            self.last_request_time = time.time()
            session = await self._get_session()
            url = f"{self.base_url}{self.search_endpoint}"

            # Format JSON exact d√©couvert via DevTools
            payload = {"term": term.strip(), "isMobile": False}

            try:
                logger.debug(f"Recherche Babelio pour: {term}")
                if self._debug_log_enabled:
                    logger.info(f"üîç [DEBUG] search: POST {url} payload={payload}")

                async with session.post(url, json=payload) as response:
                    if self._debug_log_enabled:
                        logger.info(
                            f"üîç [DEBUG] search: Response status={response.status}"
                        )

                    if response.status == 200:
                        try:
                            # Babelio retourne du JSON valide mais avec le mauvais Content-Type
                            # On r√©cup√®re le texte brut puis on parse le JSON manuellement
                            text_content = await response.text()
                            results: list[dict[str, Any]] = json.loads(text_content)

                            if self._debug_log_enabled:
                                logger.info(
                                    f"üîç [DEBUG] search: Parsed {len(results)} result(s)"
                                )
                            logger.debug(f"Babelio retourne {len(results)} r√©sultats")

                            # Mettre en cache m√©moire (limiter la taille du cache)
                            if len(self._cache) < 100:  # Limite √† 100 entr√©es
                                self._cache[cache_key] = results

                            # √âcrire dans le cache disque si disponible
                            cache_service = getattr(self, "cache_service", None)
                            if cache_service is not None:
                                try:
                                    # store both original term and normalized key for robustness
                                    # write cache in threadpool to avoid blocking event loop
                                    await asyncio.to_thread(
                                        cache_service.set_cached, term, results
                                    )
                                    await asyncio.to_thread(
                                        cache_service.set_cached, cache_key, results
                                    )
                                    # log write with same verbosity control
                                    log_fn = (
                                        logger.info
                                        if getattr(self, "_cache_log_enabled", False)
                                        else logger.debug
                                    )
                                    log_fn(
                                        f"[BabelioCache] WROTE keys=(orig='{term}', norm='{cache_key}') items={len(results)}"
                                    )
                                except Exception:
                                    logger.exception(
                                        "Impossible d'√©crire dans le cache disque (ignored)"
                                    )

                            return results
                        except json.JSONDecodeError:
                            # Si ce n'est vraiment pas du JSON, log pour debugging
                            content_type = response.headers.get(
                                "content-type", "unknown"
                            )
                            logger.error(f"Babelio r√©ponse non-JSON pour {term}")
                            logger.error(f"Content-Type: {content_type}")
                            logger.error(f"D√©but: {text_content[:200]}...")
                            return []
                    elif response.status == 503:
                        logger.warning(
                            f"Babelio HTTP 503 (Service Unavailable) pour: {term} - rate limited"
                        )
                        return []
                    else:
                        logger.warning(f"Babelio HTTP {response.status} pour: {term}")
                        return []

            except TimeoutError as e:
                logger.error(f"Timeout Babelio pour: {term}")
                # Propager l'erreur pour que l'appelant sache qu'il y a eu un probl√®me r√©seau
                raise TimeoutError(
                    f"Timeout lors de la recherche Babelio: {term}"
                ) from e
            except aiohttp.ClientError as e:
                logger.error(f"Erreur r√©seau Babelio pour {term}: {e}")
                # Propager l'erreur pour que l'appelant sache qu'il y a eu un probl√®me r√©seau
                raise aiohttp.ClientError(
                    f"Erreur r√©seau lors de la recherche Babelio: {term}"
                ) from e
            except Exception as e:
                logger.error(f"Erreur inattendue Babelio pour {term}: {e}")
                # Pour les autres erreurs, on retourne [] pour compatibilit√©
                return []

    async def verify_author(self, author_name: str) -> dict[str, Any]:
        """V√©rifie et corrige l'orthographe d'un nom d'auteur.

        Args:
            author_name: Nom de l'auteur √† v√©rifier

        Returns:
            Dict avec format standardis√©:
                {
                    'status': 'verified'|'corrected'|'not_found'|'error',
                    'original': str,
                    'babelio_suggestion': str|None,
                    'confidence_score': float (0.0-1.0),
                    'babelio_data': dict|None,  # Donn√©es brutes Babelio
                    'babelio_url': str|None,   # URL canonique
                    'error_message': str|None  # Si status='error'
                }
        """
        if not author_name or not author_name.strip():
            return self._create_error_result(author_name, "Nom d'auteur vide")

        try:
            results = await self.search(author_name)

            # Chercher le meilleur match auteur
            best_author = self._find_best_author_match(results, author_name)

            if best_author is None:
                return {
                    "status": "not_found",
                    "original": author_name,
                    "babelio_suggestion": None,
                    "confidence_score": 0.0,
                    "babelio_data": None,
                    "babelio_url": None,
                    "error_message": None,
                }

            # Construire le nom sugg√©r√©
            suggested_name = self._format_author_name(
                best_author.get("prenoms"), best_author.get("nom")
            )

            # Calculer score de confiance
            confidence = self._calculate_similarity(author_name, suggested_name)

            # D√©terminer le statut
            status = "verified" if confidence >= 0.95 else "corrected"

            return {
                "status": status,
                "original": author_name,
                "babelio_suggestion": suggested_name,
                "confidence_score": confidence,
                "babelio_data": best_author,
                "babelio_url": self._build_full_url(best_author.get("url", "")),
                "error_message": None,
            }

        except (TimeoutError, aiohttp.ClientError):
            # Propager les erreurs r√©seau/timeout pour que le script de migration
            # puisse les d√©tecter et arr√™ter le traitement
            raise
        except Exception as e:
            logger.error(f"Erreur verify_author pour {author_name}: {e}")
            return self._create_error_result(author_name, str(e))

    async def verify_book(
        self, title: str, author: str | None = None
    ) -> dict[str, Any]:
        """V√©rifie et corrige l'orthographe d'un titre de livre.

        Args:
            title: Titre du livre √† v√©rifier
            author: Nom de l'auteur (optionnel, am√©liore la pr√©cision)

        Returns:
            Dict avec format:
                {
                    'status': 'verified'|'corrected'|'not_found'|'error',
                    'original_title': str,
                    'babelio_suggestion_title': str|None,
                    'original_author': str|None,
                    'babelio_suggestion_author': str|None,
                    'confidence_score': float,
                    'babelio_data': dict|None,
                    'babelio_url': str|None,
                    'error_message': str|None
                }
        """
        if not title or not title.strip():
            return self._create_book_error_result(title, author, "Titre vide")

        try:
            # Recherche combin√©e titre + auteur si disponible
            search_term = f"{title} {author}" if author else title

            if self._debug_log_enabled:
                logger.info(f"üîç [DEBUG] verify_book: search_term='{search_term}'")

            results = await self.search(search_term)

            if self._debug_log_enabled:
                books_count = len([r for r in results if r.get("type") == "livres"])
                authors_count = len([r for r in results if r.get("type") == "auteurs"])
                logger.info(
                    f"üîç [DEBUG] verify_book: {len(results)} r√©sultat(s) - {books_count} livre(s), {authors_count} auteur(s)"
                )

            # Chercher le meilleur match livre
            best_book = self._find_best_book_match(results, title, author)

            if self._debug_log_enabled:
                if best_book:
                    logger.info(
                        f"üîç [DEBUG] verify_book: Match '{best_book.get('titre')}' par {self._format_author_name(best_book.get('prenoms'), best_book.get('nom'))}"
                    )
                else:
                    logger.info(
                        "üîç [DEBUG] verify_book: Aucun match dans _find_best_book_match"
                    )

            if best_book is None:
                # Strat√©gie de fallback: Si "titre + auteur" ne donne rien ET qu'on a un auteur,
                # on r√©essaye avec juste le titre, puis on v√©rifie l'auteur par scraping
                if author and len(results) == 0:
                    if self._debug_log_enabled:
                        logger.info(
                            f"üîç [DEBUG] verify_book: Fallback - recherche avec titre seul '{title}'"
                        )

                    # Recherche avec titre seul
                    title_only_results = await self.search(title)

                    if self._debug_log_enabled:
                        books_count = len(
                            [r for r in title_only_results if r.get("type") == "livres"]
                        )
                        logger.info(
                            f"üîç [DEBUG] verify_book: Fallback trouv√© {books_count} livre(s)"
                        )

                    # Pour chaque livre trouv√©, scraper la page pour v√©rifier l'auteur
                    books = [r for r in title_only_results if r.get("type") == "livres"]
                    for book in books:
                        book_url_relative = book.get("url")
                        if not book_url_relative:
                            continue

                        book_url = self._build_full_url(book_url_relative)

                        # Scraper l'auteur depuis la page
                        scraped_author = await self._scrape_author_from_book_page(
                            book_url
                        )

                        if scraped_author:
                            # Calculer la similarit√© entre l'auteur attendu et l'auteur scraped
                            similarity = self._calculate_similarity(
                                author, scraped_author
                            )

                            if self._debug_log_enabled:
                                logger.info(
                                    f"üîç [DEBUG] verify_book: Fallback - '{book.get('titre')}' author '{author}' vs scraped '{scraped_author}' = {similarity:.2f}"
                                )

                            # Si l'auteur correspond (seuil > 0.7), on a trouv√© le bon livre
                            if similarity > 0.7:
                                if self._debug_log_enabled:
                                    logger.info(
                                        "üîç [DEBUG] verify_book: Fallback SUCCESS - match trouv√© avec auteur scraped"
                                    )

                                # Utiliser le livre trouv√© comme best_book
                                best_book = book
                                # Ajouter les champs auteur manquants (puisque scraping retourne juste le nom)
                                # On ne peut pas extraire pr√©nom/nom s√©par√©ment du scraping, donc on utilise le nom complet
                                best_book["prenoms"] = None
                                best_book["nom"] = scraped_author
                                break

                # Si toujours pas de r√©sultat apr√®s fallback
                if best_book is None:
                    return {
                        "status": "not_found",
                        "original_title": title,
                        "babelio_suggestion_title": None,
                        "original_author": author,
                        "babelio_suggestion_author": None,
                        "confidence_score": 0.0,
                        "babelio_data": None,
                        "babelio_url": None,
                        "babelio_author_url": None,
                        "error_message": None,
                    }

            # Extraire titre et auteur sugg√©r√©s
            # Nettoyer les sauts de ligne et espaces multiples (Issue #96)
            suggested_title_raw = best_book.get("titre", title)
            suggested_title = (
                " ".join(suggested_title_raw.split()) if suggested_title_raw else title
            )

            # Nettoyer les sauts de ligne dans les champs auteur avant formatage
            prenoms_raw = best_book.get("prenoms")
            nom_raw = best_book.get("nom")
            prenoms_clean = " ".join(str(prenoms_raw).split()) if prenoms_raw else None
            nom_clean = " ".join(str(nom_raw).split()) if nom_raw else None

            suggested_author = self._format_author_name(prenoms_clean, nom_clean)

            # Nettoyer aussi les donn√©es brutes pour √©viter les newlines dans babelio_data
            babelio_data_clean = {}
            for key, value in best_book.items():
                if isinstance(value, str):
                    babelio_data_clean[key] = " ".join(value.split())
                else:
                    babelio_data_clean[key] = value

            # Score bas√© sur le titre principalement
            title_confidence = self._calculate_similarity(title, suggested_title)

            # Bonus si l'auteur match aussi
            author_confidence = 1.0
            if author and suggested_author:
                author_confidence = self._calculate_similarity(author, suggested_author)

            # Score combin√© (titre poids 70%, auteur 30%)
            confidence = 0.7 * title_confidence + 0.3 * author_confidence

            status = "verified" if confidence >= 0.90 else "corrected"

            # Construire l'URL compl√®te Babelio
            babelio_url = self._build_full_url(best_book.get("url", ""))

            # Enrichissement √©diteur: scraper si confiance >= 0.90 (Issue #85)
            babelio_publisher = None
            if confidence >= 0.90 and babelio_url:
                try:
                    babelio_publisher = await self.fetch_publisher_from_url(babelio_url)
                except Exception as e:
                    # Erreur de scraping non fatale, on continue sans √©diteur
                    logger.debug(
                        f"Impossible de scraper l'√©diteur pour {babelio_url}: {e}"
                    )

            # Enrichissement titre: scraper si titre tronqu√© d√©tect√© (Issue #88)
            if self._is_title_truncated(suggested_title) and babelio_url:
                try:
                    full_title = await self.fetch_full_title_from_url(babelio_url)
                    if full_title:
                        suggested_title = full_title
                        # Mettre √† jour aussi babelio_data pour le frontend (Issue #88)
                        babelio_data_clean["titre"] = full_title
                except Exception as e:
                    # Erreur de scraping non fatale, on continue avec titre tronqu√©
                    logger.debug(
                        f"Impossible de scraper le titre complet pour {babelio_url}: {e}"
                    )

            # Enrichissement URL auteur: scraper depuis la page du livre (Issue #124)
            # On scrape toujours l'URL auteur si on a trouv√© un livre sur Babelio,
            # ind√©pendamment du score de confiance (contrairement √† l'√©diteur/titre)
            babelio_author_url = None
            if babelio_url:
                try:
                    babelio_author_url = await self.fetch_author_url_from_page(
                        babelio_url
                    )
                except Exception as e:
                    # Erreur de scraping non fatale, on continue sans URL auteur
                    logger.debug(
                        f"Impossible de scraper l'URL auteur pour {babelio_url}: {e}"
                    )

            return {
                "status": status,
                "original_title": title,
                "babelio_suggestion_title": suggested_title,
                "original_author": author,
                "babelio_suggestion_author": suggested_author,
                "confidence_score": confidence,
                "babelio_data": babelio_data_clean,
                "babelio_url": babelio_url,
                "babelio_author_url": babelio_author_url,
                "babelio_publisher": babelio_publisher,
                "error_message": None,
            }

        except (TimeoutError, aiohttp.ClientError):
            # Propager les erreurs r√©seau/timeout pour que le script de migration
            # puisse les d√©tecter et arr√™ter le traitement
            raise
        except Exception as e:
            logger.error(f"Erreur verify_book pour {title}/{author}: {e}")
            return self._create_book_error_result(title, author, str(e))

    def _is_title_truncated(self, title: str | None) -> bool:
        """D√©tecte si un titre est tronqu√© (se termine par '...').

        Args:
            title: Titre √† v√©rifier

        Returns:
            True si le titre se termine par '...', False sinon
        """
        if not title:
            return False
        return title.strip().endswith("...")

    async def fetch_full_title_from_url(self, babelio_url: str) -> str | None:
        """Scrape le titre complet depuis une page Babelio.

        Args:
            babelio_url: URL compl√®te Babelio (ex: https://www.babelio.com/livres/...)

        Returns:
            Titre complet ou None si non trouv√©

        Exemple:
            full_title = await service.fetch_full_title_from_url(
                "https://www.babelio.com/livres/Villanova-Le-Chemin-continue--Biographie-de-Georges-Lambric/1498118"
            )
            # ‚Üí "Le Chemin continue : Biographie de Georges Lambrichs"

        Note:
            Utilise BeautifulSoup4 avec les s√©lecteurs:
            1. <meta property="og:title"> (prioritaire, nettoie le suffixe " - Babelio")
            2. <h1> (fallback)
        """
        if not babelio_url or not babelio_url.strip():
            return None

        try:
            session = await self._get_session()

            async with session.get(babelio_url) as response:
                if response.status != 200:
                    logger.warning(
                        f"Babelio HTTP {response.status} pour scraping titre: {babelio_url}"
                    )
                    return None

                html = await response.text()
                soup = BeautifulSoup(html, "lxml")

                # Priorit√© 1: og:title (plus fiable, contient le titre complet)
                og_title_tag = soup.find("meta", property="og:title")
                if og_title_tag and hasattr(og_title_tag, "get"):
                    content = og_title_tag.get("content")
                    if content:
                        title_raw = str(content)
                        # Nettoyer le suffixe " - Babelio" et les espaces multiples
                        title_clean = title_raw.replace(" - Babelio", "").strip()
                        title_final = " ".join(title_clean.split())
                        logger.debug(
                            f"Titre complet trouv√© (og:title) pour {babelio_url}: {title_final}"
                        )
                        return title_final

                # Priorit√© 2: h1 (fallback)
                h1_tag = soup.find("h1")
                if h1_tag:
                    title_raw = str(h1_tag.get_text())
                    # Nettoyer les sauts de ligne et espaces multiples
                    title_final = " ".join(title_raw.split())
                    logger.debug(
                        f"Titre complet trouv√© (h1) pour {babelio_url}: {title_final}"
                    )
                    return title_final

                logger.debug(f"Titre complet non trouv√© pour {babelio_url}")
                return None

        except Exception as e:
            logger.error(f"Erreur scraping titre pour {babelio_url}: {e}")
            return None

    async def fetch_publisher_from_url(self, babelio_url: str) -> str | None:
        """Scrape l'√©diteur depuis une page Babelio.

        Args:
            babelio_url: URL compl√®te Babelio (ex: https://www.babelio.com/livres/...)

        Returns:
            Nom de l'√©diteur ou None si non trouv√©

        Exemple:
            publisher = await service.fetch_publisher_from_url(
                "https://www.babelio.com/livres/Assouline-Des-visages-et-des-mains-150-portraits-decrivain/1635414"
            )
            # ‚Üí "Herscher"

        Note:
            Utilise BeautifulSoup4 avec le s√©lecteur CSS:
            a.tiny_links.dark[href*="/editeur/"]
        """
        if not babelio_url or not babelio_url.strip():
            return None

        try:
            session = await self._get_session()

            async with session.get(babelio_url) as response:
                if response.status != 200:
                    logger.warning(
                        f"Babelio HTTP {response.status} pour scraping √©diteur: {babelio_url}"
                    )
                    return None

                html = await response.text()
                soup = BeautifulSoup(html, "lxml")

                # S√©lecteur CSS pour l'√©diteur: lien avec classe tiny_links dark
                # pointant vers /editeur/
                editeur_link = soup.select_one('a.tiny_links.dark[href*="/editeur/"]')

                if editeur_link:
                    # Nettoyer les sauts de ligne et espaces multiples
                    # (le HTML peut contenir des <br> ou des retours √† la ligne)
                    publisher_raw: str = str(editeur_link.text)
                    publisher = " ".join(
                        publisher_raw.split()
                    )  # Split + join pour nettoyer
                    logger.debug(f"√âditeur trouv√© pour {babelio_url}: {publisher}")
                    return publisher
                else:
                    logger.debug(f"√âditeur non trouv√© pour {babelio_url}")
                    return None

        except Exception as e:
            logger.error(f"Erreur scraping √©diteur pour {babelio_url}: {e}")
            return None

    async def verify_publisher(self, publisher_name: str) -> dict[str, Any]:
        """V√©rifie l'orthographe d'un nom d'√©diteur.

        Note: Babelio ne semble pas avoir d'API sp√©cifique pour les √©diteurs,
        cette fonction cherche dans les r√©sultats g√©n√©raux.

        Args:
            publisher_name: Nom de l'√©diteur √† v√©rifier

        Returns:
            Dict au m√™me format que verify_author
        """
        # Pour l'instant, utilise la m√™me logique que les auteurs
        # Pourrait √™tre affin√©e plus tard si Babelio expose mieux les √©diteurs
        return await self.verify_author(publisher_name)

    async def verify_batch(self, items: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """V√©rifie plusieurs √©l√©ments en lot tout en respectant le rate limiting.

        Args:
            items: Liste d'√©l√©ments au format:
                [
                    {"type": "author", "name": "Michel Houellebecq"},
                    {"type": "book", "title": "L'√âtranger", "author": "Albert Camus"},
                    {"type": "publisher", "name": "Gallimard"}
                ]

        Returns:
            list[dict]: R√©sultats de v√©rification dans le m√™me ordre
        """

        async def _verify_one(item: dict[str, Any]) -> dict[str, Any]:
            item_type = item.get("type")

            if item_type == "author":
                return await self.verify_author(item.get("name", ""))
            elif item_type == "book":
                return await self.verify_book(item.get("title", ""), item.get("author"))
            elif item_type == "publisher":
                return await self.verify_publisher(item.get("name", ""))
            else:
                return self._create_error_result(
                    str(item), f"Type non support√©: {item_type}"
                )

        # Dispatch all verifications concurrently; each verification still
        # respects the internal rate_limiter in search(). Using gather keeps
        # ordering of results consistent with the input list.
        tasks = [asyncio.create_task(_verify_one(it)) for it in items]
        results = await asyncio.gather(*tasks)
        return list(results)

    def _find_best_author_match(
        self, results: list[dict], search_term: str
    ) -> dict | None:
        """Trouve le meilleur match auteur dans les r√©sultats Babelio."""
        authors = [r for r in results if r.get("type") == "auteurs"]

        if not authors:
            return None

        # Trier par pertinence (nombre de membres/≈ìuvres si disponible)
        authors.sort(
            key=lambda a: (
                int(a.get("ca_membres", 0) or 0),
                int(a.get("ca_oeuvres", 0) or 0),
            ),
            reverse=True,
        )

        return authors[0]

    def _find_best_book_match(
        self, results: list[dict], title: str, author: str | None = None
    ) -> dict | None:
        """Trouve le meilleur match livre dans les r√©sultats Babelio."""
        books = [r for r in results if r.get("type") == "livres"]

        if not books:
            if self._debug_log_enabled:
                logger.info("üîç [DEBUG] _find_best_book_match: Aucun livre trouv√©")
            return None

        if self._debug_log_enabled:
            logger.info(
                f"üîç [DEBUG] _find_best_book_match: {len(books)} livre(s) avant filtrage"
            )

        # Si on a l'auteur, filtrer par auteur d'abord
        if author:
            author_filtered = []
            for book in books:
                book_author = self._format_author_name(
                    book.get("prenoms"), book.get("nom")
                )
                similarity = (
                    self._calculate_similarity(author, book_author)
                    if book_author
                    else 0.0
                )

                if self._debug_log_enabled:
                    logger.info(
                        f"üîç [DEBUG] _find_best_book_match: '{book.get('titre')}' - author '{author}' vs '{book_author}' = {similarity:.2f}"
                    )

                if book_author and similarity > 0.7:
                    author_filtered.append(book)

            if self._debug_log_enabled:
                logger.info(
                    f"üîç [DEBUG] _find_best_book_match: {len(author_filtered)} livre(s) apr√®s filtre auteur (seuil>0.7)"
                )

            if author_filtered:
                books = author_filtered

        # Trier par pertinence (copies, notes si disponibles)
        books.sort(
            key=lambda b: (
                int(b.get("ca_copies", 0) or 0),
                float(b.get("ca_note", 0) or 0),
            ),
            reverse=True,
        )

        return books[0]

    def _calculate_similarity(self, str1: str, str2: str) -> float:
        """Calcule la similarit√© entre deux cha√Ænes (0.0 √† 1.0).

        Utilise difflib.SequenceMatcher qui impl√©mente l'algorithme
        de Ratcliff-Obershelp pour la similarit√© de s√©quences.
        """
        if not str1 or not str2:
            return 0.0

        # Normaliser : minuscules, espaces supprim√©s
        s1 = str1.lower().strip()
        s2 = str2.lower().strip()

        if s1 == s2:
            return 1.0

        return SequenceMatcher(None, s1, s2).ratio()

    def _format_author_name(self, prenoms: Any, nom: Any) -> str:
        """Formate un nom d'auteur √† partir des donn√©es Babelio."""
        prenoms_str = str(prenoms).strip() if prenoms else ""
        nom_str = str(nom).strip() if nom else ""

        # Nettoyer les virgules et espaces en fin de cha√Æne (bug Babelio)
        prenoms_str = prenoms_str.rstrip(", ")
        nom_str = nom_str.rstrip(", ")

        if prenoms_str and nom_str:
            return f"{prenoms_str} {nom_str}"
        elif nom_str:
            return nom_str
        elif prenoms_str:
            return prenoms_str
        else:
            return ""

    def _build_full_url(self, relative_url: str) -> str:
        """Construit une URL compl√®te Babelio."""
        if not relative_url:
            return ""
        if relative_url.startswith("http"):
            return relative_url
        return f"{self.base_url}{relative_url}"

    async def fetch_author_url_from_page(self, babelio_url: str) -> str | None:
        """Scrape l'URL auteur depuis une page livre Babelio (Issue #124).

        Args:
            babelio_url: URL compl√®te Babelio du livre

        Returns:
            URL compl√®te de l'auteur ou None si non trouv√©e

        Exemple:
            author_url = await service.fetch_author_url_from_page(
                "https://www.babelio.com/livres/Garreta-Sphinx/149981"
            )
            # ‚Üí "https://www.babelio.com/auteur/Anne-F-Garreta/20464"

        Note:
            Utilise BeautifulSoup4 avec le s√©lecteur CSS:
            a[href*="/auteur/"]
        """
        if not babelio_url or not babelio_url.strip():
            return None

        try:
            session = await self._get_session()

            async with session.get(babelio_url) as response:
                if response.status != 200:
                    logger.warning(
                        f"Babelio HTTP {response.status} pour scraping auteur: {babelio_url}"
                    )
                    return None

                html = await response.text()
                soup = BeautifulSoup(html, "lxml")

                # S√©lecteur CSS pour l'auteur: premier lien pointant vers /auteur/
                auteur_link = soup.select_one('a[href*="/auteur/"]')

                if auteur_link and hasattr(auteur_link, "get"):
                    href = auteur_link.get("href")
                    if href and isinstance(href, str):
                        # Construire l'URL compl√®te
                        author_url = self._build_full_url(href)
                        logger.debug(
                            f"URL auteur trouv√©e pour {babelio_url}: {author_url}"
                        )
                        return author_url
                    else:
                        logger.debug(f"URL auteur sans href pour {babelio_url}")
                        return None
                else:
                    logger.debug(f"URL auteur non trouv√©e pour {babelio_url}")
                    return None

        except Exception as e:
            logger.error(f"Erreur scraping URL auteur pour {babelio_url}: {e}")
            return None

    def _create_error_result(self, original: str, error_message: str) -> dict[str, Any]:
        """Cr√©e un r√©sultat d'erreur standardis√© pour auteur."""
        return {
            "status": "error",
            "original": original,
            "babelio_suggestion": None,
            "confidence_score": 0.0,
            "babelio_data": None,
            "babelio_url": None,
            "error_message": error_message,
        }

    def _create_book_error_result(
        self, title: str, author: str | None, error_message: str
    ) -> dict[str, Any]:
        """Cr√©e un r√©sultat d'erreur standardis√© pour livre."""
        return {
            "status": "error",
            "original_title": title,
            "babelio_suggestion_title": None,
            "original_author": author,
            "babelio_suggestion_author": None,
            "confidence_score": 0.0,
            "babelio_data": None,
            "babelio_url": None,
            "babelio_author_url": None,
            "error_message": error_message,
        }

    async def _scrape_author_from_book_page(self, babelio_url: str) -> str | None:
        """Scrape le nom de l'auteur depuis une page livre Babelio.

        Args:
            babelio_url: URL compl√®te Babelio du livre

        Returns:
            Nom de l'auteur ou None si non trouv√©

        Note:
            Utilise BeautifulSoup pour extraire le nom de l'auteur
            depuis le lien avec classe 'livre_auteur' ou href contenant '/auteur/'.
        """
        if not babelio_url or not babelio_url.strip():
            return None

        try:
            session = await self._get_session()

            if self._debug_log_enabled:
                logger.info(
                    f"üîç [DEBUG] _scrape_author_from_book_page: Fetching {babelio_url}"
                )

            async with session.get(babelio_url) as response:
                if response.status != 200:
                    logger.warning(
                        f"Babelio HTTP {response.status} pour scraping auteur: {babelio_url}"
                    )
                    return None

                html = await response.text()
                soup = BeautifulSoup(html, "lxml")

                # Strat√©gie 1: Chercher un lien avec classe 'livre_auteur'
                auteur_link = soup.select_one("a.livre_auteur")

                # Strat√©gie 2: Si non trouv√©, chercher le premier lien vers /auteur/
                if not auteur_link:
                    auteur_link = soup.select_one('a[href*="/auteur/"]')

                if auteur_link:
                    author_name = auteur_link.get_text(strip=True)
                    if author_name:
                        if self._debug_log_enabled:
                            logger.info(
                                f"üîç [DEBUG] _scrape_author_from_book_page: Found author '{author_name}'"
                            )
                        return author_name

                if self._debug_log_enabled:
                    logger.info(
                        f"üîç [DEBUG] _scrape_author_from_book_page: No author found on {babelio_url}"
                    )
                return None

        except Exception as e:
            logger.error(
                f"Erreur scraping auteur pour {babelio_url}: {e}", exc_info=True
            )
            return None

    async def close(self):
        """Ferme proprement la session HTTP.

        √Ä appeler √† la fin de l'utilisation pour √©viter les warnings asyncio.
        """
        if self.session and not self.session.closed:
            await self.session.close()


# Instance globale du service (pattern singleton pour r√©utiliser la session)
babelio_service = BabelioService()
