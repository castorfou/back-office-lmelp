"""Application FastAPI principale."""

import logging
import os
import re
import socket
from collections.abc import AsyncGenerator
from contextlib import asynccontextmanager, suppress
from datetime import datetime
from typing import Any

from bson import ObjectId
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, Response
from pydantic import BaseModel
from rapidfuzz import fuzz
from thefuzz import process

from .middleware import EnrichedLoggingMiddleware
from .models.critique import Critique
from .models.emission import Emission
from .models.episode import Episode
from .services.babelio_cache_service import BabelioCacheService
from .services.babelio_migration_service import BabelioMigrationService
from .services.babelio_service import babelio_service
from .services.books_extraction_service import books_extraction_service
from .services.calibre_service import calibre_service
from .services.collections_management_service import collections_management_service
from .services.critiques_extraction_service import critiques_extraction_service
from .services.fixture_updater import FixtureUpdaterService
from .services.livres_auteurs_cache_service import livres_auteurs_cache_service
from .services.mongodb_service import mongodb_service
from .services.radiofrance_service import RadioFranceService
from .services.stats_service import stats_service
from .utils.memory_guard import memory_guard
from .utils.port_discovery import PortDiscovery


logger = logging.getLogger(__name__)


class BabelioVerificationRequest(BaseModel):
    """ModÃ¨le pour les requÃªtes de vÃ©rification Babelio."""

    type: str  # "author", "book", ou "publisher"
    name: str | None = None  # Pour author ou publisher
    title: str | None = None  # Pour book
    author: str | None = None  # Auteur du livre (optionnel pour book)


class FuzzySearchRequest(BaseModel):
    """ModÃ¨le pour les requÃªtes de recherche fuzzy dans les Ã©pisodes."""

    episode_id: str
    query_title: str
    query_author: str | None = None


class CapturedCall(BaseModel):
    """ModÃ¨le pour un appel API capturÃ©."""

    service: str  # 'babelioService' | 'fuzzySearchService'
    method: str  # 'verifyAuthor' | 'verifyBook' | 'searchEpisode'
    input: dict[str, Any]
    output: dict[str, Any]
    timestamp: int


class BookValidationResult(BaseModel):
    """ModÃ¨le pour un livre avec rÃ©sultat de validation du frontend."""

    auteur: str
    titre: str
    editeur: str
    programme: bool
    validation_status: str  # 'verified' | 'suggested' | 'not_found'
    suggested_author: str | None = None
    suggested_title: str | None = None
    # Issue #85: Champs d'enrichissement Babelio automatique
    babelio_url: str | None = None
    babelio_publisher: str | None = None


class ValidationResultsRequest(BaseModel):
    """ModÃ¨le pour recevoir les rÃ©sultats de validation du frontend."""

    episode_oid: str
    avis_critique_id: str
    books: list[BookValidationResult]


class FixtureUpdateRequest(BaseModel):
    """ModÃ¨le pour les requÃªtes de mise Ã  jour de fixtures."""

    calls: list[CapturedCall]


# Nouveaux modÃ¨les pour l'Issue #66 - Gestion des collections auteurs/livres
class ValidateSuggestionRequest(BaseModel):
    """ModÃ¨le pour la validation d'une suggestion."""

    cache_id: str | None = None
    episode_oid: str
    avis_critique_id: str
    auteur: str
    titre: str
    editeur: str | None = None
    user_validated_author: str
    user_validated_title: str
    user_validated_publisher: str | None = None
    babelio_publisher: str | None = None  # Issue #85: Enrichissement Babelio
    babelio_url: str | None = None  # Issue #85: URL Babelio du livre


class AddManualBookRequest(BaseModel):
    """ModÃ¨le pour l'ajout manuel d'un livre."""

    id: str
    user_entered_author: str
    user_entered_title: str
    user_entered_publisher: str | None = None


# Nouveaux modÃ¨les pour la migration Babelio (Issue #124)
class AcceptSuggestionRequest(BaseModel):
    """ModÃ¨le pour accepter une suggestion Babelio."""

    livre_id: str
    babelio_url: str
    babelio_author_url: str | None = None
    corrected_title: str | None = None


class MarkNotFoundRequest(BaseModel):
    """ModÃ¨le pour marquer un livre ou auteur comme non trouvÃ© sur Babelio."""

    item_id: str
    reason: str
    item_type: str = "livre"  # "livre" ou "auteur"


class CorrectTitleRequest(BaseModel):
    """ModÃ¨le pour corriger le titre d'un livre."""

    livre_id: str
    new_title: str


class RetryWithTitleRequest(BaseModel):
    """ModÃ¨le pour rÃ©essayer une recherche avec un nouveau titre."""

    livre_id: str
    new_title: str
    author: str | None = None


class UpdateFromBabelioUrlRequest(BaseModel):
    """ModÃ¨le pour mettre Ã  jour depuis une URL Babelio manuelle."""

    item_id: str
    babelio_url: str
    item_type: str = "livre"  # "livre" ou "auteur"


class ExtractFromBabelioUrlRequest(BaseModel):
    """ModÃ¨le pour extraire les donnÃ©es depuis une URL Babelio (Issue #159)."""

    babelio_url: str


@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    """Gestion du cycle de vie de l'application."""
    try:
        # Afficher les informations de dÃ©marrage (Issue #136)
        from .utils.startup_logging import log_startup_info

        log_startup_info()

        # DÃ©marrage
        if not mongodb_service.connect():
            raise Exception("Impossible de se connecter Ã  MongoDB")
        print("Connexion MongoDB Ã©tablie")

        # Attach a persistent disk cache for Babelio lookups so restarts benefit
        try:
            cache_enabled = os.environ.get("BABELIO_CACHE_ENABLED", "1").lower() in (
                "1",
                "true",
                "yes",
            )

            if cache_enabled:
                cache_dir = os.environ.get(
                    "BABELIO_CACHE_DIR",
                    os.path.join(os.getcwd(), "data", "processed", "babelio_cache"),
                )
                babelio_service.cache_service = BabelioCacheService(
                    cache_dir=cache_dir, ttl_hours=24
                )
                print(f"Babelio disk cache attached at {cache_dir}")
            else:
                print("Babelio disk cache disabled via BABELIO_CACHE_ENABLED")
        except Exception as e:
            print(f"Unable to attach Babelio disk cache: {e}")

        yield

    except Exception as e:
        print(f"Erreur dans le cycle de vie: {e}")
        raise
    finally:
        # ArrÃªt garanti mÃªme en cas d'erreur
        try:
            mongodb_service.disconnect()
            print("Connexion MongoDB fermÃ©e")
        except Exception as e:
            print(f"Erreur lors de la fermeture: {e}")


app = FastAPI(
    title="Back-office LMELP",
    description="Interface de gestion pour la base de donnÃ©es du Masque et la Plume",
    version="0.1.0",
    lifespan=lifespan,
)

# Initialize migration service (Issue #124)
babelio_migration_service = BabelioMigrationService(
    mongodb_service=mongodb_service,
    babelio_service=babelio_service,
)

# Configure logging for enriched access logs (Issue #115)
access_logger = logging.getLogger("back_office_lmelp.access")
access_logger.setLevel(logging.INFO)
# Add console handler if not already present
if not access_logger.handlers:
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    access_logger.addHandler(console_handler)
    access_logger.propagate = False  # Ã‰viter la duplication avec le logger root


# Configuration CORS pour le frontend
def get_cors_configuration():
    """Retourne la configuration CORS selon l'environnement."""
    env = os.getenv("ENVIRONMENT", "development")

    if env == "development":
        # En dÃ©veloppement, autoriser toutes les origines pour faciliter l'accÃ¨s mobile
        return {
            "allow_origins": ["*"],
            "allow_credentials": False,  # Must be False when allow_origins is "*"
            "allow_methods": ["*"],
            "allow_headers": ["*"],
        }
    else:
        # En production, Ãªtre restrictif
        return {
            "allow_origins": [
                "http://localhost:3000",
                "http://localhost:5173",
            ],
            "allow_credentials": True,
            "allow_methods": ["*"],
            "allow_headers": ["*"],
        }


cors_config = get_cors_configuration()
app.add_middleware(CORSMiddleware, **cors_config)

# Add enriched logging middleware (Issue #115)
app.add_middleware(EnrichedLoggingMiddleware)


@app.get("/")
async def root() -> dict[str, str]:
    """Point d'entrÃ©e de l'API."""
    return {"message": "Back-office LMELP API", "version": "0.1.0"}


@app.get("/health")
async def health() -> dict[str, str]:
    """Health check endpoint for Docker healthchecks (Issue #115).

    This endpoint is designed for automated monitoring and Docker healthchecks.
    It returns a simple status without database connectivity check to keep
    response time minimal and avoid polluting logs.
    """
    return {"status": "ok"}


@app.get("/api/episodes", response_model=list[dict[str, Any]])
async def get_episodes() -> list[dict[str, Any]]:
    """RÃ©cupÃ¨re la liste de tous les Ã©pisodes."""
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        episodes_data = mongodb_service.get_all_episodes()
        episodes = [Episode(data).to_summary_dict() for data in episodes_data]
        return episodes
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.get("/api/episodes/all", response_model=list[dict[str, Any]])
async def get_all_episodes_including_masked() -> list[dict[str, Any]]:
    """RÃ©cupÃ¨re tous les Ã©pisodes y compris les masquÃ©s (Issue #107).

    Cette route est utilisÃ©e par la page de gestion des Ã©pisodes masquÃ©s.
    """
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        episodes_data = mongodb_service.get_all_episodes(include_masked=True)
        episodes = [Episode(data).to_summary_dict() for data in episodes_data]
        return episodes
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.get("/api/episodes/{episode_id}", response_model=dict[str, Any])
async def get_episode(episode_id: str) -> dict[str, Any]:
    """RÃ©cupÃ¨re un Ã©pisode par son ID."""
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        episode_data = mongodb_service.get_episode_by_id(episode_id)
        if not episode_data:
            raise HTTPException(status_code=404, detail="Ã‰pisode non trouvÃ©")

        episode = Episode(episode_data)
        return episode.to_dict()
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.delete("/api/episodes/{episode_id}", response_model=dict[str, Any])
async def delete_episode(episode_id: str) -> dict[str, Any]:
    """Supprime un Ã©pisode et toutes ses donnÃ©es associÃ©es.

    Effectue une suppression en cascade :
    - Supprime les avis critiques liÃ©s
    - Retire les rÃ©fÃ©rences de l'Ã©pisode des livres
    - Supprime l'Ã©pisode lui-mÃªme
    """
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        # Tenter de supprimer l'Ã©pisode
        success = mongodb_service.delete_episode(episode_id)

        if not success:
            raise HTTPException(status_code=404, detail="Episode not found")

        return {
            "success": True,
            "episode_id": episode_id,
            "message": f"Episode {episode_id} deleted successfully",
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.put("/api/episodes/{episode_id}")
async def update_episode_description(
    episode_id: str, request: Request
) -> dict[str, str]:
    """Met Ã  jour la description corrigÃ©e d'un Ã©pisode."""
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        # Lire le body de la requÃªte (text/plain)
        description_corrigee = (await request.body()).decode("utf-8")

        # VÃ©rifier que l'Ã©pisode existe
        episode_data = mongodb_service.get_episode_by_id(episode_id)
        if not episode_data:
            raise HTTPException(status_code=404, detail="Ã‰pisode non trouvÃ©")

        # Mettre Ã  jour la description avec la nouvelle logique
        success = mongodb_service.update_episode_description_new(
            episode_id, description_corrigee
        )
        if not success:
            raise HTTPException(status_code=400, detail="Ã‰chec de la mise Ã  jour")

        return {"message": "Description mise Ã  jour avec succÃ¨s"}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.put("/api/episodes/{episode_id}/title")
async def update_episode_title(episode_id: str, request: Request) -> dict[str, str]:
    """Met Ã  jour le titre corrigÃ© d'un Ã©pisode."""
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        # Lire le body de la requÃªte (text/plain)
        titre_corrige = (await request.body()).decode("utf-8")

        # VÃ©rifier que l'Ã©pisode existe
        episode_data = mongodb_service.get_episode_by_id(episode_id)
        if not episode_data:
            raise HTTPException(status_code=404, detail="Ã‰pisode non trouvÃ©")

        # Mettre Ã  jour le titre avec la nouvelle logique
        success = mongodb_service.update_episode_title_new(episode_id, titre_corrige)
        if not success:
            raise HTTPException(status_code=400, detail="Ã‰chec de la mise Ã  jour")

        return {"message": "Titre mis Ã  jour avec succÃ¨s"}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.post("/api/episodes/{episode_id}/fetch-page-url")
async def fetch_episode_page_url(episode_id: str) -> dict[str, Any]:
    """Fetch l'URL de la page RadioFrance pour un Ã©pisode et la persiste en DB.

    Utilise RadioFranceService pour scraper RadioFrance et trouver l'URL de la page
    de l'Ã©pisode Ã  partir de son titre. L'URL trouvÃ©e est ensuite persistÃ©e dans le
    champ episode_page_url de l'Ã©pisode.

    Args:
        episode_id: ID de l'Ã©pisode

    Returns:
        Dict avec episode_id, episode_page_url, et success

    Raises:
        404: Si l'Ã©pisode n'existe pas en DB ou si l'URL n'est pas trouvÃ©e sur RadioFrance
        500: En cas d'erreur serveur
    """
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        # VÃ©rifier que l'Ã©pisode existe
        episode_data = mongodb_service.get_episode_by_id(episode_id)
        if not episode_data:
            raise HTTPException(status_code=404, detail="Ã‰pisode non trouvÃ©")

        # RÃ©cupÃ©rer le titre de l'Ã©pisode
        episode_title = episode_data.get("titre", "")
        if not episode_title:
            raise HTTPException(status_code=400, detail="L'Ã©pisode n'a pas de titre")

        # RÃ©cupÃ©rer la date de l'Ã©pisode
        # CRITIQUE: MongoDB retourne datetime.datetime, PAS une chaÃ®ne ISO
        # Convertir en YYYY-MM-DD pour RadioFranceService
        episode_date_raw = episode_data.get("date", None)
        episode_date = None
        if episode_date_raw:
            # MongoDB retourne datetime.datetime â†’ convertir en "YYYY-MM-DD"
            if isinstance(episode_date_raw, datetime):
                episode_date = episode_date_raw.strftime("%Y-%m-%d")
            else:
                # Fallback si string (ne devrait pas arriver avec vraie DB)
                date_str = str(episode_date_raw)
                episode_date = date_str.split("T")[0].split(" ")[0]

        # Chercher l'URL de la page sur RadioFrance
        radiofrance_service = RadioFranceService()
        episode_page_url = await radiofrance_service.search_episode_page_url(
            episode_title, episode_date
        )

        if not episode_page_url:
            raise HTTPException(
                status_code=404,
                detail=f"URL de la page non trouvÃ©e sur RadioFrance pour: {episode_title[:50]}...",
            )

        # Persister l'URL en base de donnÃ©es
        success = mongodb_service.update_episode(
            episode_id, {"episode_page_url": episode_page_url}
        )

        if not success:
            raise HTTPException(
                status_code=500, detail="Ã‰chec de la mise Ã  jour de l'Ã©pisode en base"
            )

        return {
            "episode_id": episode_id,
            "episode_page_url": episode_page_url,
            "success": True,
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.patch("/api/episodes/{episode_id}/masked")
async def update_episode_masked(episode_id: str, request: Request) -> dict[str, Any]:
    """Met Ã  jour le statut masked d'un Ã©pisode (Issue #107).

    Args:
        episode_id: ID de l'Ã©pisode
        request: Corps de la requÃªte contenant {"masked": true/false}

    Returns:
        Dict avec message de succÃ¨s
    """
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        # Lire le body de la requÃªte
        body = await request.json()
        masked = body.get("masked")

        if masked is None:
            raise HTTPException(status_code=400, detail="Le champ 'masked' est requis")

        if not isinstance(masked, bool):
            raise HTTPException(
                status_code=400, detail="Le champ 'masked' doit Ãªtre un boolÃ©en"
            )

        # VÃ©rifier que l'Ã©pisode existe
        episode_data = mongodb_service.get_episode_by_id(episode_id)
        if not episode_data:
            raise HTTPException(status_code=404, detail="Ã‰pisode non trouvÃ©")

        # Mettre Ã  jour le statut
        success = mongodb_service.update_episode_masked_status(episode_id, masked)
        if not success:
            raise HTTPException(status_code=400, detail="Ã‰chec de la mise Ã  jour")

        action = "masquÃ©" if masked else "rendu visible"
        return {"message": f"Ã‰pisode {action} avec succÃ¨s"}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.get("/api/statistics", response_model=dict[str, Any])
async def get_statistics() -> dict[str, Any]:
    """RÃ©cupÃ¨re les statistiques de l'application."""
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        stats_data = mongodb_service.get_statistics()

        # Transformer les clÃ©s pour correspondre au format frontend
        return {
            "totalEpisodes": stats_data["total_episodes"],
            "maskedEpisodes": stats_data["masked_episodes_count"],
            "episodesWithCorrectedTitles": stats_data["episodes_with_corrected_titles"],
            "episodesWithCorrectedDescriptions": stats_data[
                "episodes_with_corrected_descriptions"
            ],
            "criticalReviews": stats_data["critical_reviews_count"],
            "lastUpdateDate": stats_data["last_update_date"],
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


# ========== ENDPOINTS CALIBRE ==========


@app.get("/api/calibre/status")
async def get_calibre_status() -> dict[str, Any]:
    """RÃ©cupÃ¨re le statut de l'intÃ©gration Calibre."""
    try:
        status = calibre_service.get_status()
        return status.model_dump()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.get("/api/calibre/books")
async def get_calibre_books(
    limit: int = 50,
    offset: int = 0,
    read_filter: bool | None = None,
    search: str | None = None,
) -> dict[str, Any]:
    """RÃ©cupÃ¨re la liste des livres Calibre."""
    try:
        books_list = calibre_service.get_books(
            limit=limit, offset=offset, read_filter=read_filter, search=search
        )
        return books_list.model_dump()
    except RuntimeError as e:
        raise HTTPException(status_code=503, detail=str(e)) from e
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.get("/api/calibre/books/{book_id}")
async def get_calibre_book(book_id: int) -> dict[str, Any]:
    """RÃ©cupÃ¨re les dÃ©tails d'un livre Calibre."""
    try:
        book = calibre_service.get_book(book_id)
        if book is None:
            raise HTTPException(status_code=404, detail=f"Livre {book_id} non trouvÃ©")
        return book.model_dump()
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.get("/api/calibre/authors")
async def get_calibre_authors(
    limit: int = 100, offset: int = 0
) -> list[dict[str, Any]]:
    """RÃ©cupÃ¨re la liste des auteurs Calibre."""
    try:
        authors = calibre_service.get_authors(limit=limit, offset=offset)
        return [author.model_dump() for author in authors]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.get("/api/calibre/statistics")
async def get_calibre_statistics() -> dict[str, Any]:
    """RÃ©cupÃ¨re les statistiques de la bibliothÃ¨que Calibre."""
    try:
        stats = calibre_service.get_statistics()
        return stats.model_dump()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.get("/api/livres-auteurs", response_model=list[dict[str, Any]])
async def get_livres_auteurs(
    episode_oid: str, limit: int | None = None
) -> list[dict[str, Any]]:
    """RÃ©cupÃ¨re la liste des livres/auteurs extraits des avis critiques via parsing des tableaux markdown (format simplifiÃ© : auteur/titre/Ã©diteur)."""
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    # Validation de episode_oid
    if not episode_oid or not episode_oid.strip():
        raise HTTPException(
            status_code=422, detail="episode_oid is required and cannot be empty"
        )

    # Validation du format episode_oid (ObjectId MongoDB : 24 caractÃ¨res hexadÃ©cimaux)
    if len(episode_oid) != 24 or not all(
        c in "0123456789abcdefABCDEF" for c in episode_oid
    ):
        raise HTTPException(
            status_code=422,
            detail="episode_oid must be a valid MongoDB ObjectId (24 hex characters)",
        )

    try:
        # RÃ©cupÃ©rer seulement les avis critiques de cet Ã©pisode (episode_oid obligatoire)
        avis_critiques = mongodb_service.get_critical_reviews_by_episode_oid(
            episode_oid
        )

        if not avis_critiques:
            return []

        # Phase 1: VÃ©rifier le cache global pour cet episode_oid
        cached_books = livres_auteurs_cache_service.get_books_by_episode_oid(
            episode_oid
        )

        # Utiliser les donnÃ©es du cache ou initialiser une liste vide
        all_books = cached_books or []

        # Phase 1.5: Ramasse-miettes automatique pour livres mongo non corrigÃ©s (Issue #67 - Phase 2)
        try:
            cleanup_stats = collections_management_service.cleanup_uncorrected_summaries_for_episode(
                episode_oid
            )
            # Logger les stats pour suivre la progression du cleanup global
            if cleanup_stats["corrected"] > 0:
                print(
                    f"ðŸ§¹ Cleanup Ã©pisode {episode_oid}: {cleanup_stats['corrected']} summaries corrigÃ©s"
                )
        except Exception as cleanup_error:
            # Ne pas bloquer l'affichage en cas d'erreur de cleanup
            print(f"âš ï¸ Erreur cleanup Ã©pisode {episode_oid}: {cleanup_error}")

        # Phase 2: Extraction si cache miss global
        if not cached_books:
            try:
                extracted_books = (
                    await books_extraction_service.extract_books_from_reviews(
                        avis_critiques
                    )
                )
                all_books.extend(extracted_books)

            except Exception as extraction_error:
                print(f"âš ï¸ Erreur extraction: {extraction_error}")
                # En cas d'erreur d'extraction, continuer avec les donnÃ©es du cache uniquement

        # Pas de filtrage - afficher tous les livres (systÃ¨me unifiÃ©)
        books_for_frontend = all_books

        # Formater pour l'affichage simplifiÃ©
        formatted_books = books_extraction_service.format_books_for_simplified_display(
            books_for_frontend
        )

        return formatted_books

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.get("/api/episodes-with-reviews", response_model=list[dict[str, Any]])
async def get_episodes_with_reviews() -> list[dict[str, Any]]:
    """RÃ©cupÃ¨re la liste des Ã©pisodes qui ont des avis critiques associÃ©s."""
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        # RÃ©cupÃ©rer tous les avis critiques pour obtenir les episode_oid uniques
        avis_critiques = mongodb_service.get_all_critical_reviews()

        # Extraire les episode_oid uniques
        unique_episode_oids = list(
            {avis["episode_oid"] for avis in avis_critiques if avis.get("episode_oid")}
        )

        # RÃ©cupÃ©rer les dÃ©tails des Ã©pisodes correspondants avec avis_critique_id
        episodes_with_reviews = []
        for episode_oid in unique_episode_oids:
            episode_data = mongodb_service.get_episode_by_id(episode_oid)
            if episode_data:
                episode = Episode(episode_data)

                # Issue #107: Filtrer les Ã©pisodes masquÃ©s
                if episode.masked:
                    continue

                episode_dict = episode.to_summary_dict()

                # Ajouter l'avis_critique_id correspondant Ã  cet Ã©pisode
                avis_critique = next(
                    (
                        avis
                        for avis in avis_critiques
                        if avis["episode_oid"] == episode_oid
                    ),
                    None,
                )
                if avis_critique:
                    episode_dict["avis_critique_id"] = str(avis_critique["_id"])

                # Ajouter le flag has_cached_books pour indiquer si l'Ã©pisode a dÃ©jÃ  Ã©tÃ© affichÃ©
                # (prÃ©sence de livres dans livresauteurs_cache)
                cached_books = livres_auteurs_cache_service.get_books_by_episode_oid(
                    episode_oid
                )
                episode_dict["has_cached_books"] = len(cached_books) > 0

                # Ajouter le flag has_incomplete_books pour identifier les Ã©pisodes avec livres non validÃ©s
                # (au moins un livre avec status != 'mongo')
                has_incomplete = any(
                    book.get("status") != "mongo" for book in cached_books
                )
                episode_dict["has_incomplete_books"] = has_incomplete

                episodes_with_reviews.append(episode_dict)

        # Trier par date dÃ©croissante
        episodes_with_reviews.sort(key=lambda x: x.get("date", ""), reverse=True)

        return episodes_with_reviews

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


# ========== EMISSIONS ENDPOINTS (Issue #154) ==========


@app.get("/api/emissions", response_model=list[dict[str, Any]])
async def get_all_emissions() -> list[dict[str, Any]]:
    """
    RÃ©cupÃ¨re toutes les Ã©missions.
    Si la collection emissions est vide, dÃ©clenche l'auto-conversion.

    Returns:
        Liste des Ã©missions avec donnÃ©es enrichies (episode, avis_critique)
    """
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        emissions = mongodb_service.get_all_emissions()

        # Auto-conversion si collection vide
        if len(emissions) == 0:
            logger.info("Collection emissions vide - dÃ©clenchement auto-conversion")
            await auto_convert_episodes_to_emissions()
            emissions = mongodb_service.get_all_emissions()

        # Enrichir avec donnÃ©es episode et avis_critique
        enriched_emissions = []
        for emission in emissions:
            episode_data = mongodb_service.get_episode_by_id(
                str(emission["episode_id"])
            )
            avis_data = mongodb_service.get_avis_critique_by_id(
                str(emission["avis_critique_id"])
            )

            emission_dict = Emission(emission).to_dict()
            emission_dict["episode"] = (
                Episode(episode_data).to_summary_dict() if episode_data else None
            )
            emission_dict["has_avis_critique"] = avis_data is not None

            enriched_emissions.append(emission_dict)

        return enriched_emissions

    except Exception as e:
        logger.error(f"Erreur get_all_emissions: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.post("/api/emissions/auto-convert", response_model=dict[str, Any])
async def auto_convert_episodes_to_emissions() -> dict[str, Any]:
    """
    Convertit automatiquement tous les Ã©pisodes (avec avis critiques) en Ã©missions.

    Logique :
    1. RÃ©cupÃ©rer tous les avis_critiques
    2. Pour chaque avis, crÃ©er une Ã©mission si elle n'existe pas dÃ©jÃ 
    3. DÃ©tecter animateur_id en cherchant les critiques avec animateur=true

    Returns:
        Statistiques de conversion (created, skipped, errors)
    """
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        created_count = 0
        skipped_count = 0
        errors = []

        # 1. RÃ©cupÃ©rer tous les avis_critiques
        all_avis = mongodb_service.get_all_critical_reviews()

        for avis in all_avis:
            try:
                episode_oid = avis.get("episode_oid")
                if not episode_oid:
                    continue

                # 2. VÃ©rifier si Ã©mission existe dÃ©jÃ 
                existing = mongodb_service.get_emission_by_episode_id(episode_oid)
                if existing:
                    skipped_count += 1
                    continue

                # 3. RÃ©cupÃ©rer donnÃ©es Ã©pisode
                episode_data = mongodb_service.get_episode_by_id(episode_oid)
                if not episode_data:
                    errors.append(f"Ã‰pisode {episode_oid} non trouvÃ©")
                    continue

                episode = Episode(episode_data)

                # 3b. Skip Ã©pisodes masquÃ©s
                if episode.masked:
                    skipped_count += 1
                    continue

                # 4. DÃ©tecter animateur_id
                critiques = mongodb_service.get_critiques_by_episode(episode_oid)
                animateur_id = None
                for critique in critiques:
                    if critique.get("animateur", False):
                        animateur_id = critique["id"]
                        break  # Prendre le premier animateur trouvÃ©

                # 5. CrÃ©er Ã©mission
                emission_data = Emission.for_mongodb_insert(
                    {
                        "episode_id": episode_oid,
                        "avis_critique_id": str(avis["_id"]),
                        "date": episode.date,
                        "duree": episode.duree,
                        "animateur_id": animateur_id,
                        "avis_ids": [],  # Vide pour l'instant (future issue)
                    }
                )

                mongodb_service.create_emission(emission_data)
                created_count += 1

            except Exception as e:
                errors.append(f"Erreur pour avis {avis.get('_id')}: {str(e)}")

        return {
            "success": True,
            "created": created_count,
            "skipped": skipped_count,
            "total_processed": len(all_avis),
            "errors": errors,
        }

    except Exception as e:
        logger.error(f"Erreur auto_convert_episodes_to_emissions: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.get("/api/emissions/by-date/{date_str}", response_model=dict[str, Any])
async def get_emission_by_date(date_str: str) -> dict[str, Any]:
    """
    RÃ©cupÃ¨re une Ã©mission par sa date (format YYYYMMDD).

    Args:
        date_str: Date au format YYYYMMDD (ex: "20251212")

    Returns:
        MÃªmes donnÃ©es que /api/emissions/{emission_id}/details
    """
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    # Valider format date YYYYMMDD
    if not re.match(r"^\d{8}$", date_str):
        raise HTTPException(
            status_code=400, detail="Format de date invalide. Attendu: YYYYMMDD"
        )

    try:
        # Convertir string YYYYMMDD en datetime
        year = int(date_str[:4])
        month = int(date_str[4:6])
        day = int(date_str[6:8])
        target_date_start = datetime(year, month, day)
        target_date_end = datetime(year, month, day, 23, 59, 59)

        # Chercher Ã©mission avec cette date (range query pour matcher le jour complet)
        if mongodb_service.emissions_collection is None:
            raise HTTPException(
                status_code=500, detail="Service MongoDB non disponible"
            )
        emission_data = mongodb_service.emissions_collection.find_one(
            {"date": {"$gte": target_date_start, "$lte": target_date_end}}
        )

        if not emission_data:
            raise HTTPException(
                status_code=404,
                detail=f"Aucune Ã©mission trouvÃ©e pour la date {date_str}",
            )

        emission_id_str = str(emission_data["_id"])

        # RÃ©utiliser la logique de get_emission_details
        result = await get_emission_details(emission_id_str)
        return dict(result) if isinstance(result, dict) else result

    except HTTPException:
        raise
    except ValueError:
        raise HTTPException(status_code=400, detail="Date invalide") from None
    except Exception as e:
        logger.error(f"Erreur get_emission_by_date: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.get("/api/emissions/{emission_id}/details", response_model=dict[str, Any])
async def get_emission_details(emission_id: str) -> dict[str, Any]:
    """
    RÃ©cupÃ¨re les dÃ©tails complets d'une Ã©mission.

    Args:
        emission_id: ID de l'Ã©mission (ObjectId string)

    Returns:
        Dictionnaire avec toutes les donnÃ©es (emission, episode, books, critiques, summary)
    """
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    # Valider format ObjectId
    if not re.match(r"^[0-9a-fA-F]{24}$", emission_id):
        raise HTTPException(status_code=400, detail="Format d'ID invalide")

    try:
        # 1. RÃ©cupÃ©rer Ã©mission
        if mongodb_service.emissions_collection is None:
            raise HTTPException(
                status_code=500, detail="Service MongoDB non disponible"
            )
        emission_data = mongodb_service.emissions_collection.find_one(
            {"_id": ObjectId(emission_id)}
        )
        if not emission_data:
            raise HTTPException(status_code=404, detail="Ã‰mission non trouvÃ©e")

        emission = Emission(emission_data)

        # 2. RÃ©cupÃ©rer Ã©pisode
        episode_data = mongodb_service.get_episode_by_id(emission.episode_id)
        if not episode_data:
            raise HTTPException(status_code=404, detail="Ã‰pisode associÃ© non trouvÃ©")

        episode = Episode(episode_data)

        # 3. RÃ©cupÃ©rer avis_critique
        avis_data = mongodb_service.get_avis_critique_by_id(emission.avis_critique_id)
        if not avis_data:
            raise HTTPException(
                status_code=404, detail="Avis critique associÃ© non trouvÃ©"
            )

        # 4. RÃ©cupÃ©rer livres via endpoint existant
        books = await get_livres_auteurs(episode_oid=emission.episode_id)

        # 5. RÃ©cupÃ©rer critiques de cet Ã©pisode
        critiques = mongodb_service.get_critiques_by_episode(emission.episode_id)

        # 6. Construire rÃ©ponse
        return {
            "emission": emission.to_dict(),
            "episode": {
                "id": episode.id,
                "titre": episode.titre,
                "date": episode.date.isoformat() if episode.date else None,
                "description": episode.description,
                "duree": episode.duree,
                "episode_page_url": episode.episode_page_url,
            },
            "summary": avis_data.get("summary", ""),
            "books": books,
            "critiques": critiques,
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur get_emission_details: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.post("/api/verify-babelio", response_model=dict[str, Any])
async def verify_babelio(request: BabelioVerificationRequest) -> dict[str, Any]:
    """VÃ©rifie et corrige l'orthographe via le service Babelio."""
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        if request.type == "author":
            if not request.name:
                raise HTTPException(
                    status_code=400, detail="Le nom de l'auteur est requis"
                )
            result = await babelio_service.verify_author(request.name)

        elif request.type == "book":
            if not request.title:
                raise HTTPException(
                    status_code=400, detail="Le titre du livre est requis"
                )
            result = await babelio_service.verify_book(request.title, request.author)

        elif request.type == "publisher":
            if not request.name:
                raise HTTPException(
                    status_code=400, detail="Le nom de l'Ã©diteur est requis"
                )
            result = await babelio_service.verify_publisher(request.name)

        else:
            raise HTTPException(
                status_code=400,
                detail="Type invalide. Doit Ãªtre 'author', 'book' ou 'publisher'",
            )

        return result

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.post("/api/set-validation-results", response_model=dict[str, Any])
async def set_validation_results(request: ValidationResultsRequest) -> dict[str, Any]:
    """ReÃ§oit les rÃ©sultats de validation biblio du frontend et les stocke."""
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        books_processed = 0

        for book_result in request.books:
            # Convertir le statut de validation frontend vers statut cache unifiÃ©
            if book_result.validation_status == "verified":
                cache_status = "verified"
            elif book_result.validation_status == "suggestion":
                cache_status = "suggested"
            else:  # not_found
                cache_status = "not_found"

            # PrÃ©parer les donnÃ©es pour le cache
            book_data = {
                "episode_oid": request.episode_oid,
                "auteur": book_result.auteur,
                "titre": book_result.titre,
                "editeur": book_result.editeur,
                "programme": book_result.programme,
                "status": cache_status,
            }

            # Ajouter les suggestions si disponibles
            if book_result.suggested_author:
                book_data["suggested_author"] = book_result.suggested_author
            if book_result.suggested_title:
                book_data["suggested_title"] = book_result.suggested_title

            # Issue #85: Ajouter les enrichissements Babelio si disponibles
            if book_result.babelio_url:
                book_data["babelio_url"] = book_result.babelio_url
            if book_result.babelio_publisher:
                book_data["babelio_publisher"] = book_result.babelio_publisher

            # CrÃ©er l'entrÃ©e cache
            from bson import ObjectId

            avis_critique_id = ObjectId(request.avis_critique_id)
            cache_entry_id = livres_auteurs_cache_service.create_cache_entry(
                avis_critique_id, book_data
            )

            # Auto-processing pour les livres verified
            if cache_status == "verified":
                try:
                    # Utiliser le nom validÃ© (suggested_author si disponible, sinon auteur original)
                    validated_author = (
                        book_result.suggested_author or book_result.auteur
                    )
                    validated_title = book_result.suggested_title or book_result.titre

                    # CrÃ©er auteur en base
                    author_id = mongodb_service.create_author_if_not_exists(
                        validated_author
                    )

                    # CrÃ©er livre en base
                    # Issue #85: Utiliser babelio_publisher si disponible (source plus fiable)
                    editeur_value = book_result.babelio_publisher or book_result.editeur
                    book_data_for_mongo = {
                        "titre": validated_title,
                        "auteur_id": author_id,
                        "editeur": editeur_value,
                        "episodes": [request.episode_oid],
                        "avis_critiques": [request.avis_critique_id],
                    }
                    book_id = mongodb_service.create_book_if_not_exists(
                        book_data_for_mongo
                    )

                    # Issue #85: Passer babelio_publisher en metadata pour Ã©craser editeur dans le cache
                    # C'est la source la plus fiable (enrichissement Babelio)
                    cache_metadata = {}
                    if book_result.babelio_publisher:
                        cache_metadata["babelio_publisher"] = (
                            book_result.babelio_publisher
                        )

                    # Marquer comme traitÃ© (mongo) avec metadata pour override cache editeur
                    livres_auteurs_cache_service.mark_as_processed(
                        cache_entry_id, author_id, book_id, metadata=cache_metadata
                    )

                    # Issue #85: Mettre Ã  jour l'avis_critique avec le nouvel Ã©diteur enrichi par Babelio
                    # Mettre Ã  jour le summary markdown pour remplacer l'ancien Ã©diteur par le nouvel
                    if (
                        book_result.babelio_publisher
                        and book_result.babelio_publisher != book_result.editeur
                    ):
                        print(
                            f"ðŸ“ [Issue #85] Updating avis_critique {request.avis_critique_id} with Babelio publisher={book_result.babelio_publisher}"
                        )

                        # RÃ©cupÃ©rer l'avis critique actuel pour accÃ©der au summary
                        avis_critique = mongodb_service.get_avis_critique_by_id(
                            request.avis_critique_id
                        )
                        if avis_critique:
                            # Importer la fonction pour mettre Ã  jour le summary
                            from .utils.summary_updater import replace_book_in_summary

                            # Mettre Ã  jour le summary markdown avec le nouvel Ã©diteur
                            original_summary = avis_critique.get("summary", "")
                            updated_summary = replace_book_in_summary(
                                summary=original_summary,
                                original_author=book_result.auteur,
                                original_title=book_result.titre,
                                corrected_author=book_result.auteur,  # Pas de changement d'auteur
                                corrected_title=book_result.titre,  # Pas de changement de titre
                                original_publisher=book_result.editeur,
                                corrected_publisher=book_result.babelio_publisher,
                            )

                            # Mettre Ã  jour l'avis_critique avec le summary et l'Ã©diteur mis Ã  jour
                            mongodb_service.update_avis_critique(
                                request.avis_critique_id,
                                {
                                    "summary": updated_summary,
                                    "editeur": book_result.babelio_publisher,
                                },
                            )
                            print("   âœ… Summary and editeur updated in avis_critique")

                except Exception as auto_processing_error:
                    # Ne pas faire Ã©chouer l'endpoint si l'auto-processing Ã©choue
                    print(
                        f"âš ï¸ Erreur auto-processing pour {book_result.auteur}: {auto_processing_error}"
                    )

            books_processed += 1

        return {"success": True, "books_processed": books_processed}

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


def extract_ngrams(text: str, n: int) -> list[str]:
    """
    Extrait des sÃ©quences de n mots consÃ©cutifs.

    Args:
        text: Le texte source
        n: Le nombre de mots par n-gram

    Returns:
        Liste des n-grams extraits

    Example:
        >>> extract_ngrams("L'invention de Tristan", 2)
        ["L'invention de", "de Tristan"]
    """
    words = text.split()
    if len(words) < n:
        return []
    return [" ".join(words[i : i + n]) for i in range(len(words) - n + 1)]


def smart_fuzzy_score(query: str, candidate: str) -> float:
    """
    Scorer intelligent qui pÃ©nalise les matches partiels trop courts.

    ProblÃ¨me avec ratio/WRatio : "Adrien" vs "Adrien Bosc" pour query "Adrien Bosque"
    - "Adrien" â†’ score Ã©levÃ© (petit dÃ©nominateur)
    - "Adrien Bosc" â†’ score plus bas (dÃ©nominateur plus grand)

    Solution : Utiliser token_sort_ratio ET pÃ©naliser si le candidat est trop court
    par rapport Ã  la query.
    """
    # Score de base avec token_sort_ratio (ignore l'ordre des mots)
    base_score: float = float(fuzz.token_sort_ratio(query, candidate))

    # PÃ©nalitÃ© si le candidat est significativement plus court que la query
    query_len = len(query)
    candidate_len = len(candidate)

    # Si le candidat est < 60% de la longueur de la query, appliquer une pÃ©nalitÃ©
    if candidate_len < query_len * 0.6:
        # PÃ©nalitÃ© proportionnelle Ã  la diffÃ©rence de longueur
        length_ratio = candidate_len / query_len
        penalty = (1 - length_ratio) * 15  # PÃ©nalitÃ© max 15 points
        return float(max(0, base_score - penalty))

    return float(base_score)


@app.post("/api/fuzzy-search-episode", response_model=dict[str, Any])
async def fuzzy_search_episode(request: FuzzySearchRequest) -> dict[str, Any]:
    """Recherche fuzzy dans le titre et description d'un Ã©pisode."""
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        # RÃ©cupÃ©rer l'Ã©pisode
        episode_data = mongodb_service.get_episode_by_id(request.episode_id)
        if not episode_data:
            raise HTTPException(status_code=404, detail="Ã‰pisode non trouvÃ©")

        episode = Episode(episode_data)

        # Combiner titre et description pour la recherche
        full_text = f"{episode.titre} {episode.description}"

        # Nettoyer et diviser le texte en mots/segments
        import re

        # Extraire segments entre guillemets (prioritÃ© haute - titres potentiels)
        quoted_segments_raw = re.findall(r'"([^"]+)"', full_text)
        # Issue #96: Nettoyer les sauts de ligne dans les segments extraits
        quoted_segments = [" ".join(seg.split()) for seg in quoted_segments_raw]

        # NOUVEAU : Extraire n-grams de diffÃ©rentes tailles (Issue #76)
        # Pour dÃ©tecter les titres multi-mots comme "L'invention de Tristan"
        bigrams = extract_ngrams(full_text, 2)  # "L'invention de", "de Tristan"
        trigrams = extract_ngrams(full_text, 3)  # "L'invention de Tristan"
        quadrigrams = extract_ngrams(full_text, 4)  # "L'invention de Tristan Adrien"

        # Extraire mots individuels de plus de 3 caractÃ¨res
        words = [word for word in full_text.split() if len(word) > 3]

        # Nettoyer les mots (enlever ponctuation)
        clean_words = [re.sub(r"[^\w\-\'Ã Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¶Ã¹Ã»Ã¼Ã¿Ã§]", "", word) for word in words]
        clean_words = [word for word in clean_words if len(word) > 3]

        # Candidats par prioritÃ© : guillemets > 4-grams > 3-grams > 2-grams > mots
        # Filtrer n-grams trop courts pour Ã©viter le bruit
        search_candidates = (
            quoted_segments
            + [ng for ng in quadrigrams if len(ng) > 10]  # Filtrer n-grams courts
            + [ng for ng in trigrams if len(ng) > 8]
            + [ng for ng in bigrams if len(ng) > 6]
            + clean_words
        )

        # Recherche fuzzy pour le titre
        # D'abord chercher dans les segments entre guillemets (prioritÃ© haute)
        quoted_matches = (
            process.extract(
                request.query_title, quoted_segments, scorer=smart_fuzzy_score, limit=5
            )
            if quoted_segments
            else []
        )

        # Puis chercher dans tous les candidats
        all_matches = process.extract(
            request.query_title, search_candidates, scorer=smart_fuzzy_score, limit=10
        )

        # Retourner TOUS les segments entre guillemets (potentiels titres) + bons matches gÃ©nÃ©raux
        title_matches = []

        # Ajouter tous les segments entre guillemets avec marqueur ðŸ“–
        if quoted_matches:
            title_matches.extend(
                [("ðŸ“– " + match, score) for match, score in quoted_matches]
            )

        # Ajouter les autres bons matches gÃ©nÃ©raux (seuil plus strict)
        title_matches.extend(
            [
                (match, score)
                for match, score in all_matches
                if score >= 60 and match not in [q[0] for q in quoted_matches]
            ]
        )

        # Recherche fuzzy pour l'auteur (si fourni)
        author_matches = []
        if request.query_author:
            author_matches_raw = process.extract(
                request.query_author,
                search_candidates,
                scorer=smart_fuzzy_score,
                limit=10,
            )
            # Filtrer manuellement par score
            author_matches = [
                (match, score) for match, score in author_matches_raw if score >= 75
            ]

        # Nettoyer la ponctuation en fin de chaÃ®ne pour tous les matches
        def clean_trailing_punctuation(text: str) -> str:
            """Nettoie la ponctuation en fin de chaÃ®ne (virgules, points, etc.)"""
            return text.rstrip(",.;:!? ")

        title_matches = [
            (clean_trailing_punctuation(match), score) for match, score in title_matches
        ]
        author_matches = [
            (clean_trailing_punctuation(match), score)
            for match, score in author_matches
        ]

        # Trier les rÃ©sultats par score dÃ©croissant
        title_matches.sort(key=lambda x: x[1], reverse=True)
        author_matches.sort(key=lambda x: x[1], reverse=True)

        return {
            "episode_id": request.episode_id,
            "episode_title": episode.titre,
            "query_title": request.query_title,
            "query_author": request.query_author,
            "title_matches": title_matches,
            "author_matches": author_matches,
            "found_suggestions": len(title_matches) > 0 or len(author_matches) > 0,
            "debug_candidates": search_candidates[:10],  # Pour debug
            "debug_quoted_matches": quoted_matches[:3]
            if quoted_matches
            else [],  # Pour debug
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.get("/api/search", response_model=dict[str, Any])
async def search_text(q: str, limit: int = 10) -> dict[str, Any]:
    """Recherche textuelle multi-entitÃ©s avec support de recherche floue."""
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    # Validation des paramÃ¨tres
    if len(q.strip()) < 3:
        raise HTTPException(
            status_code=400,
            detail="La recherche nÃ©cessite au moins 3 caractÃ¨res minimum",
        )

    if limit < 1 or limit > 100:
        raise HTTPException(
            status_code=400, detail="La limite doit Ãªtre entre 1 et 100"
        )

    try:
        # Recherche dans les Ã©pisodes
        episodes_search_result = mongodb_service.search_episodes(q, limit)
        episodes_list = episodes_search_result.get("episodes", [])
        episodes_total_count = episodes_search_result.get("total_count", 0)

        # Recherche dans les collections dÃ©diÃ©es auteurs et livres
        auteurs_search_result = mongodb_service.search_auteurs(q, limit)
        auteurs_list = auteurs_search_result.get("auteurs", [])
        auteurs_total_count = auteurs_search_result.get("total_count", 0)

        livres_search_result = mongodb_service.search_livres(q, limit)
        livres_list = livres_search_result.get("livres", [])
        livres_total_count = livres_search_result.get("total_count", 0)

        # Recherche dans les Ã©diteurs
        editeurs_search_result = mongodb_service.search_editeurs(q, limit)
        editeurs_list = editeurs_search_result.get("editeurs", [])

        # Structure de la rÃ©ponse - utilise les collections dÃ©diÃ©es en prioritÃ©
        response = {
            "query": q,
            "results": {
                "auteurs": auteurs_list,
                "auteurs_total_count": auteurs_total_count,
                "livres": livres_list,
                "livres_total_count": livres_total_count,
                "editeurs": editeurs_list,
                "episodes": [
                    {
                        "titre": episode.get("titre_corrige")
                        or episode.get("titre", ""),
                        "description": episode.get("description_corrigee")
                        or episode.get("description", ""),
                        "date": episode.get("date", ""),
                        "score": episode.get("score", 0),
                        "match_type": episode.get("match_type", "none"),
                        "search_context": episode.get("search_context", ""),
                        "_id": episode.get("_id", ""),
                    }
                    for episode in episodes_list
                ],
                "episodes_total_count": episodes_total_count,
            },
        }

        return response

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.get("/api/advanced-search", response_model=dict[str, Any])
async def advanced_search(
    q: str,
    entities: str | None = None,
    page: int = 1,
    limit: int = 20,
) -> dict[str, Any]:
    """
    Recherche avancÃ©e avec filtres par entitÃ©s et pagination.

    Args:
        q: Terme de recherche (minimum 3 caractÃ¨res)
        entities: EntitÃ©s Ã  rechercher (auteurs,livres,editeurs,episodes) sÃ©parÃ©es par virgule
                 Si None, recherche dans toutes les entitÃ©s
        page: NumÃ©ro de page (commence Ã  1)
        limit: Nombre de rÃ©sultats par page (max 100)

    Returns:
        RÃ©sultats de recherche avec pagination et compteurs totaux
    """
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    # Validation des paramÃ¨tres
    if len(q.strip()) < 3:
        raise HTTPException(
            status_code=400,
            detail="La recherche nÃ©cessite au moins 3 caractÃ¨res minimum",
        )

    if page < 1:
        raise HTTPException(status_code=400, detail="Le numÃ©ro de page doit Ãªtre >= 1")

    if limit < 1 or limit > 100:
        raise HTTPException(
            status_code=400, detail="La limite doit Ãªtre entre 1 et 100"
        )

    # Parser les entitÃ©s demandÃ©es
    valid_entities = {"auteurs", "livres", "editeurs", "episodes"}
    requested_entities = valid_entities.copy()  # Par dÃ©faut, toutes les entitÃ©s

    if entities:
        requested_entities = {
            entity.strip() for entity in entities.split(",") if entity.strip()
        }
        # Valider que les entitÃ©s sont valides
        invalid_entities = requested_entities - valid_entities
        if invalid_entities:
            raise HTTPException(
                status_code=400,
                detail=f"EntitÃ© invalide: {', '.join(invalid_entities)}. "
                f"EntitÃ©s valides: {', '.join(valid_entities)}",
            )

    try:
        # Calculer l'offset pour la pagination
        offset = (page - 1) * limit

        # Initialiser les rÃ©sultats
        results: dict[str, Any] = {
            "auteurs": [],
            "auteurs_total_count": 0,
            "livres": [],
            "livres_total_count": 0,
            "editeurs": [],
            "editeurs_total_count": 0,
            "episodes": [],
            "episodes_total_count": 0,
        }

        # Rechercher dans les entitÃ©s demandÃ©es avec offset et limit
        if "episodes" in requested_entities:
            episodes_search_result = mongodb_service.search_episodes(q, limit, offset)
            episodes_list = episodes_search_result.get("episodes", [])
            results["episodes"] = [
                {
                    "titre": episode.get("titre_corrige") or episode.get("titre", ""),
                    "description": episode.get("description_corrigee")
                    or episode.get("description", ""),
                    "date": episode.get("date", ""),
                    "score": episode.get("score", 0),
                    "match_type": episode.get("match_type", "none"),
                    "search_context": episode.get("search_context", ""),
                    "_id": episode.get("_id", ""),
                }
                for episode in episodes_list
            ]
            results["episodes_total_count"] = episodes_search_result.get(
                "total_count", 0
            )

        if "auteurs" in requested_entities:
            auteurs_search_result = mongodb_service.search_auteurs(q, limit, offset)
            results["auteurs"] = auteurs_search_result.get("auteurs", [])
            results["auteurs_total_count"] = auteurs_search_result.get("total_count", 0)

        if "livres" in requested_entities:
            livres_search_result = mongodb_service.search_livres(q, limit, offset)
            results["livres"] = livres_search_result.get("livres", [])
            results["livres_total_count"] = livres_search_result.get("total_count", 0)

        if "editeurs" in requested_entities:
            # Recherche dans la collection editeurs
            editeurs_search_result = mongodb_service.search_editeurs(q, limit, offset)
            results["editeurs"] = editeurs_search_result.get("editeurs", [])
            results["editeurs_total_count"] = editeurs_search_result.get(
                "total_count", 0
            )

        # Calculer le nombre total de pages (basÃ© sur la plus grande collection)
        max_total = max(
            results["episodes_total_count"],
            results["auteurs_total_count"],
            results["livres_total_count"],
            results["editeurs_total_count"],
        )
        total_pages = (max_total + limit - 1) // limit if max_total > 0 else 1

        response = {
            "query": q,
            "results": results,
            "pagination": {"page": page, "limit": limit, "total_pages": total_pages},
        }

        return response

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.post("/api/update-fixtures", response_model=dict[str, Any])
async def update_fixtures(request: FixtureUpdateRequest) -> dict[str, Any]:
    """Met Ã  jour les fixtures YAML avec les appels API capturÃ©s."""
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        updater = FixtureUpdaterService()
        result = updater.update_from_captured_calls(
            [call.model_dump() for call in request.calls]
        )

        return {
            "success": True,
            "updated_files": result.updated_files,
            "added_cases": result.added_cases,
            "updated_cases": result.updated_cases,
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


# Nouveaux endpoints pour l'Issue #96 - Pages de visualisation Auteur et Livre


@app.get("/api/auteur/{auteur_id}", response_model=dict[str, Any])
async def get_auteur_detail(auteur_id: str) -> dict[str, Any]:
    """RÃ©cupÃ¨re les dÃ©tails d'un auteur avec la liste de ses livres (Issue #96 - Phase 1).

    Args:
        auteur_id: ID de l'auteur (MongoDB ObjectId)

    Returns:
        Dict avec auteur_id, nom, nombre_oeuvres, et livres triÃ©s alphabÃ©tiquement

    Raises:
        404: Si l'auteur n'existe pas
        400: Si l'ID est invalide
        500: En cas d'erreur serveur
    """
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    # Validation du format ObjectId
    if len(auteur_id) != 24:
        raise HTTPException(status_code=404, detail="Auteur non trouvÃ©")

    try:
        from bson import ObjectId

        # VÃ©rifier que c'est un ObjectId valide
        ObjectId(auteur_id)
    except Exception as e:
        raise HTTPException(status_code=404, detail="Auteur non trouvÃ©") from e

    try:
        auteur_data = mongodb_service.get_auteur_with_livres(auteur_id)
        if not auteur_data:
            raise HTTPException(status_code=404, detail="Auteur non trouvÃ©")

        return auteur_data
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.get("/api/livre/{livre_id}", response_model=dict[str, Any])
async def get_livre_detail(livre_id: str) -> dict[str, Any]:
    """RÃ©cupÃ¨re les dÃ©tails d'un livre avec la liste de ses Ã©pisodes (Issue #96 - Phase 2).

    Args:
        livre_id: ID du livre (MongoDB ObjectId)

    Returns:
        Dict avec livre_id, titre, auteur_id, auteur_nom, editeur, nombre_episodes,
        et episodes triÃ©s par date dÃ©croissante

    Raises:
        404: Si le livre n'existe pas
        500: En cas d'erreur serveur
    """
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    # Validation du format ObjectId
    if len(livre_id) != 24:
        raise HTTPException(status_code=404, detail="Livre non trouvÃ©")

    try:
        from bson import ObjectId

        # VÃ©rifier que c'est un ObjectId valide
        ObjectId(livre_id)
    except Exception as e:
        raise HTTPException(status_code=404, detail="Livre non trouvÃ©") from e

    try:
        livre_data = mongodb_service.get_livre_with_episodes(livre_id)
        if not livre_data:
            raise HTTPException(status_code=404, detail="Livre non trouvÃ©")

        return livre_data
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


# Nouveaux endpoints pour l'Issue #66 - Gestion des collections auteurs/livres


@app.get("/api/livres-auteurs/statistics", response_model=dict[str, Any])
async def get_livres_auteurs_statistics() -> dict[str, Any]:
    """RÃ©cupÃ¨re les statistiques pour la page livres-auteurs (Issue #124: includes Babelio metrics)."""
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        # Utiliser stats_service qui contient toutes les mÃ©triques, y compris Babelio (Issue #124)
        stats = stats_service.get_cache_statistics()
        return stats
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.post("/api/livres-auteurs/auto-process-verified", response_model=dict[str, Any])
async def auto_process_verified_books() -> dict[str, Any]:
    """Traite automatiquement les livres avec statut 'verified'."""
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        result = collections_management_service.auto_process_verified_books()
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.get("/api/livres-auteurs/books/{status}", response_model=list[dict[str, Any]])
async def get_books_by_validation_status(status: str) -> list[dict[str, Any]]:
    """RÃ©cupÃ¨re les livres par statut de validation."""
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        books = collections_management_service.get_books_by_validation_status(status)
        return books
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.post("/api/livres-auteurs/validate-suggestion", response_model=dict[str, Any])
async def validate_suggestion(request: ValidateSuggestionRequest) -> dict[str, Any]:
    """Valide manuellement une suggestion d'auteur/livre."""
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        result = collections_management_service.handle_book_validation(
            request.model_dump()
        )
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.delete(
    "/api/livres-auteurs/cache/episode/{episode_oid}", response_model=dict[str, Any]
)
async def delete_cache_by_episode(episode_oid: str) -> dict[str, Any]:
    """Supprime toutes les entrÃ©es de cache pour un Ã©pisode donnÃ©."""
    try:
        deleted_count = livres_auteurs_cache_service.delete_cache_by_episode(
            episode_oid
        )
        return {"deleted_count": deleted_count, "episode_oid": episode_oid}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


# Note: add_manual_book endpoint removed - functionality unified with validate_suggestion


@app.get("/api/authors", response_model=list[dict[str, Any]])
async def get_all_authors() -> list[dict[str, Any]]:
    """RÃ©cupÃ¨re tous les auteurs de la collection."""
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        authors = collections_management_service.get_all_authors()
        return authors
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.get("/api/books", response_model=list[dict[str, Any]])
async def get_all_books() -> list[dict[str, Any]]:
    """RÃ©cupÃ¨re tous les livres de la collection."""
    # VÃ©rification mÃ©moire
    memory_check = memory_guard.check_memory_limit()
    if memory_check:
        if "LIMITE MÃ‰MOIRE DÃ‰PASSÃ‰E" in memory_check:
            memory_guard.force_shutdown(memory_check)
        print(f"âš ï¸ {memory_check}")

    try:
        books = collections_management_service.get_all_books()
        return books
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur: {str(e)}") from e


@app.get("/api/stats", response_model=dict[str, Any])
async def get_cache_statistics() -> dict[str, Any] | JSONResponse:
    """RÃ©cupÃ¨re les statistiques de base du cache livres/auteurs."""
    try:
        return stats_service.get_cache_statistics()
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})


@app.get("/api/stats/detailed", response_model=list[dict[str, Any]])
async def get_detailed_breakdown() -> list[dict[str, Any]] | JSONResponse:
    """RÃ©cupÃ¨re la rÃ©partition dÃ©taillÃ©e par biblio_verification_status."""
    try:
        result = stats_service.get_detailed_breakdown()
        return list(result) if result else []
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})


@app.get("/api/stats/recent", response_model=list[dict[str, Any]])
async def get_recent_processed_books(
    limit: int = 10,
) -> list[dict[str, Any]] | JSONResponse:
    """RÃ©cupÃ¨re les livres rÃ©cemment auto-traitÃ©s."""
    try:
        result = stats_service.get_recent_processed_books(limit)
        return list(result) if result else []
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})


@app.get("/api/stats/summary")
async def get_human_readable_summary() -> Response:
    """RÃ©cupÃ¨re un rÃ©sumÃ© lisible des statistiques."""
    try:
        summary = stats_service.get_human_readable_summary()
        return Response(content=summary, media_type="text/plain; charset=utf-8")
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})


@app.get("/api/stats/validation", response_model=list[dict[str, Any]])
async def get_validation_status_breakdown() -> list[dict[str, Any]] | JSONResponse:
    """RÃ©cupÃ¨re la rÃ©partition par validation_status."""
    try:
        result = stats_service.get_validation_status_breakdown()
        return list(result) if result else []
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})


# Endpoints pour la migration Babelio (Issue #124)
@app.get("/api/babelio-migration/status")
async def get_migration_status() -> dict[str, Any]:
    """RÃ©cupÃ¨re le statut de la migration Babelio."""
    return babelio_migration_service.get_migration_status()


@app.get("/api/babelio-migration/problematic-cases")
async def get_problematic_cases() -> list[dict[str, Any]]:
    """RÃ©cupÃ¨re la liste des cas problÃ©matiques de la migration."""
    return babelio_migration_service.get_problematic_cases()


@app.post("/api/babelio-migration/accept-suggestion")
async def accept_suggestion(request: AcceptSuggestionRequest) -> JSONResponse:
    """Accepte la suggestion Babelio et met Ã  jour MongoDB."""
    try:
        success = babelio_migration_service.accept_suggestion(
            livre_id=request.livre_id,
            babelio_url=request.babelio_url,
            babelio_author_url=request.babelio_author_url,
            corrected_title=request.corrected_title,
        )

        if success:
            return JSONResponse(
                status_code=200,
                content={
                    "status": "success",
                    "message": f"Suggestion acceptÃ©e pour livre {request.livre_id}",
                },
            )
        else:
            return JSONResponse(
                status_code=404,
                content={"status": "error", "message": "Livre non trouvÃ©"},
            )
    except Exception as e:
        return JSONResponse(
            status_code=500, content={"status": "error", "message": str(e)}
        )


@app.post("/api/babelio-migration/mark-not-found")
async def mark_not_found(request: MarkNotFoundRequest) -> JSONResponse:
    """Marque un livre ou auteur comme non trouvÃ© sur Babelio."""
    try:
        success = babelio_migration_service.mark_as_not_found(
            item_id=request.item_id,
            reason=request.reason,
            item_type=request.item_type,
        )

        if success:
            item_label = "Livre" if request.item_type == "livre" else "Auteur"
            return JSONResponse(
                status_code=200,
                content={
                    "status": "success",
                    "message": f"{item_label} {request.item_id} marquÃ© comme not found",
                },
            )
        else:
            item_label = "Livre" if request.item_type == "livre" else "Auteur"
            return JSONResponse(
                status_code=404,
                content={"status": "error", "message": f"{item_label} non trouvÃ©"},
            )
    except Exception as e:
        return JSONResponse(
            status_code=500, content={"status": "error", "message": str(e)}
        )


@app.post("/api/babelio-migration/update-from-url")
async def update_from_babelio_url(request: UpdateFromBabelioUrlRequest) -> JSONResponse:
    """Met Ã  jour un livre/auteur depuis une URL Babelio manuelle."""
    try:
        result = await babelio_migration_service.update_from_babelio_url(
            item_id=request.item_id,
            babelio_url=request.babelio_url,
            item_type=request.item_type,
        )

        if result["success"]:
            item_label = "Livre" if request.item_type == "livre" else "Auteur"
            return JSONResponse(
                status_code=200,
                content={
                    "status": "success",
                    "message": f"{item_label} mis Ã  jour depuis URL Babelio",
                    "data": result["scraped_data"],
                },
            )
        else:
            return JSONResponse(
                status_code=400,
                content={
                    "status": "error",
                    "message": result.get("error", "Erreur inconnue"),
                },
            )
    except Exception as e:
        return JSONResponse(
            status_code=500, content={"status": "error", "message": str(e)}
        )


@app.post("/api/babelio-migration/correct-title")
async def correct_title(request: CorrectTitleRequest) -> JSONResponse:
    """Corrige le titre d'un livre et le retire des cas problÃ©matiques."""
    try:
        success = babelio_migration_service.correct_title(
            livre_id=request.livre_id,
            new_title=request.new_title,
        )

        if success:
            return JSONResponse(
                status_code=200,
                content={
                    "status": "success",
                    "message": f"Titre corrigÃ©: '{request.new_title}'",
                },
            )
        else:
            return JSONResponse(
                status_code=404,
                content={"status": "error", "message": "Livre non trouvÃ©"},
            )
    except Exception as e:
        return JSONResponse(
            status_code=500, content={"status": "error", "message": str(e)}
        )


@app.post("/api/babelio-migration/retry-with-title")
async def retry_with_title(request: RetryWithTitleRequest) -> dict[str, Any]:
    """RÃ©essaie la recherche Babelio avec un nouveau titre."""
    return await babelio_migration_service.retry_with_new_title(
        livre_id=request.livre_id,
        new_title=request.new_title,
        author=request.author,
    )


@app.post("/api/babelio/extract-from-url")
async def extract_from_babelio_url(
    request: ExtractFromBabelioUrlRequest,
) -> JSONResponse:
    """Extrait les donnÃ©es depuis une URL Babelio sans mise Ã  jour (Issue #159)."""
    try:
        # Scraper les donnÃ©es depuis l'URL Babelio
        scraped_data = await babelio_migration_service.extract_from_babelio_url(
            request.babelio_url
        )

        return JSONResponse(
            status_code=200,
            content={
                "status": "success",
                "data": scraped_data,
            },
        )
    except ValueError as e:
        # Erreur de validation d'URL
        return JSONResponse(
            status_code=400,
            content={
                "status": "error",
                "message": str(e),
            },
        )
    except Exception as e:
        # Autres erreurs (scraping, rÃ©seau, etc.)
        logger.error(f"Error extracting from Babelio URL: {e}")
        return JSONResponse(
            status_code=500,
            content={
                "status": "error",
                "message": str(e),
            },
        )


@app.post("/api/babelio-migration/start")
async def start_migration_process() -> JSONResponse:
    """Lance le processus de migration automatique des URL Babelio."""
    try:
        from .utils.migration_runner import migration_runner

        result = await migration_runner.start_migration()
        return JSONResponse(content=result)
    except Exception as e:
        logger.error(f"Error starting migration: {e}")
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": str(e)},
        )


@app.get("/api/babelio-migration/progress")
async def get_migration_progress() -> JSONResponse:
    """RÃ©cupÃ¨re la progression actuelle de la migration."""
    try:
        from .utils.migration_runner import migration_runner

        status = migration_runner.get_status()
        return JSONResponse(content=status)
    except Exception as e:
        logger.error(f"Error getting migration progress: {e}")
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": str(e)},
        )


@app.get("/api/babelio-migration/logs")
async def get_migration_logs() -> JSONResponse:
    """RÃ©cupÃ¨re tous les logs dÃ©taillÃ©s de la migration."""
    try:
        from .utils.migration_runner import migration_runner

        logs = migration_runner.get_detailed_logs()
        return JSONResponse(content={"logs": logs})
    except Exception as e:
        logger.error(f"Error getting migration logs: {e}")
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": str(e)},
        )


@app.post("/api/babelio-migration/stop")
async def stop_migration_process() -> JSONResponse:
    """ArrÃªte le processus de migration en cours."""
    try:
        from .utils.migration_runner import migration_runner

        result = await migration_runner.stop_migration()
        return JSONResponse(content=result)
    except Exception as e:
        logger.error(f"Error stopping migration: {e}")
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": str(e)},
        )


@app.get("/openapi_reduced.json")
async def openapi_reduced() -> JSONResponse:
    """Retourne une version allÃ©gÃ©e de la spec OpenAPI (chemins, mÃ©thodes, summary, params, responses).

    Utile pour un client qui veut lister rapidement les endpoints et leurs usages sans charger
    toute la dÃ©finition des schÃ©mas.
    """
    try:
        schema = app.openapi()

        reduced: dict[str, object] = {
            "openapi": schema.get("openapi"),
            "info": {
                "title": schema.get("info", {}).get("title"),
                "version": schema.get("info", {}).get("version"),
            },
            "paths": {},
        }

        for path, methods in (schema.get("paths") or {}).items():
            reduced_methods: dict[str, object] = {}
            for method, op in methods.items():
                params = []
                for p in op.get("parameters", []) if op.get("parameters") else []:
                    p_schema = p.get("schema", {})
                    params.append(
                        {
                            "name": p.get("name"),
                            "in": p.get("in"),
                            "required": p.get("required", False),
                            "type": p_schema.get("type"),
                        }
                    )

                responses = {
                    status: (resp.get("description") if isinstance(resp, dict) else "")
                    for status, resp in (op.get("responses") or {}).items()
                }

                reduced_methods[method] = {
                    "summary": op.get("summary"),
                    "description": op.get("description"),
                    "tags": op.get("tags", []),
                    "parameters": params,
                    "responses": responses,
                }

            paths_dict = reduced["paths"]
            assert isinstance(paths_dict, dict)
            paths_dict[path] = reduced_methods

        return JSONResponse(content=reduced)
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})


# ============================================================================
# ENDPOINTS CRITIQUES ET EMISSIONS (Issue #154)
# ============================================================================


@app.get("/api/episodes/{episode_id}/critiques-detectes")
async def get_detected_critiques(episode_id: str) -> JSONResponse:
    """
    Extrait les critiques dÃ©tectÃ©s dans l'avis critique d'un Ã©pisode.

    Returns:
        Liste des critiques dÃ©tectÃ©s avec leur statut de matching
    """
    try:
        if (
            mongodb_service.avis_critiques_collection is None
            or mongodb_service.critiques_collection is None
        ):
            raise HTTPException(
                status_code=500, detail="Service MongoDB non disponible"
            )

        # RÃ©cupÃ©rer l'avis critique pour cet Ã©pisode
        # Note: episode_oid est stockÃ© en STRING dans MongoDB, pas en ObjectId
        avis_critique = mongodb_service.avis_critiques_collection.find_one(
            {"episode_oid": episode_id}
        )

        if not avis_critique:
            raise HTTPException(
                status_code=404, detail="Aucun avis critique trouvÃ© pour cet Ã©pisode"
            )

        # Extraire les critiques depuis le summary
        summary = avis_critique.get("summary", "")
        detected_names = critiques_extraction_service.extract_critiques_from_summary(
            summary
        )

        # RÃ©cupÃ©rer tous les critiques existants
        existing_critiques = list(mongodb_service.critiques_collection.find({}))

        # Pour chaque nom dÃ©tectÃ©, chercher une correspondance
        results = []
        for detected_name in detected_names:
            match = critiques_extraction_service.find_matching_critique(
                detected_name, existing_critiques
            )

            if match:
                # Critique existant trouvÃ©
                results.append(
                    {
                        "detected_name": detected_name,
                        "status": "existing",
                        "match_type": match["match_type"],
                        "matched_critique": match["nom"],
                    }
                )
            else:
                # Nouveau critique
                results.append(
                    {
                        "detected_name": detected_name,
                        "status": "new",
                        "match_type": None,
                        "matched_critique": None,
                    }
                )

        return JSONResponse(
            content={
                "episode_id": episode_id,
                "avis_critique_id": str(avis_critique["_id"]),
                "detected_critiques": results,
            }
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur lors de l'extraction des critiques: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.get("/api/stats/critiques-manquants")
async def get_critiques_manquants_count() -> JSONResponse:
    """
    Compte le nombre d'Ã©pisodes avec des critiques manquants.

    Returns:
        {"count": nombre d'Ã©pisodes avec critiques manquants}
    """
    try:
        if (
            mongodb_service.episodes_collection is None
            or mongodb_service.avis_critiques_collection is None
            or mongodb_service.critiques_collection is None
        ):
            raise HTTPException(
                status_code=500, detail="Service MongoDB non disponible"
            )

        # RÃ©cupÃ©rer tous les episode_oid qui ont des avis critiques (strings)
        episode_ids_with_reviews = set(
            mongodb_service.avis_critiques_collection.distinct("episode_oid")
        )

        # Convertir les episode_oid en ObjectId pour rÃ©cupÃ©rer les Ã©pisodes
        from bson import ObjectId

        episode_object_ids = [
            ObjectId(ep_id) for ep_id in episode_ids_with_reviews if ep_id
        ]

        # RÃ©cupÃ©rer les Ã©pisodes pour vÃ©rifier le statut masquÃ©
        # Issue #107: Filtrer les Ã©pisodes masquÃ©s
        episodes_data = list(
            mongodb_service.episodes_collection.find(
                {"_id": {"$in": episode_object_ids}}
            )
        )

        # Compter combien d'Ã©pisodes ont au moins un critique "new" (manquant)
        episodes_with_missing_critiques = 0

        # RÃ©cupÃ©rer tous les critiques existants une seule fois
        existing_critiques = list(mongodb_service.critiques_collection.find({}))

        for episode_data in episodes_data:
            # Filtrer les Ã©pisodes masquÃ©s
            episode = Episode(episode_data)
            if episode.masked:
                continue

            episode_id = str(episode_data["_id"])

            # RÃ©cupÃ©rer l'avis critique
            avis_critique = mongodb_service.avis_critiques_collection.find_one(
                {"episode_oid": episode_id}
            )

            if not avis_critique:
                continue

            # Extraire les critiques dÃ©tectÃ©s
            summary = avis_critique.get("summary", "")
            detected_names = (
                critiques_extraction_service.extract_critiques_from_summary(summary)
            )

            # VÃ©rifier si au moins un critique est "new" (manquant)
            has_missing = False
            for detected_name in detected_names:
                match = critiques_extraction_service.find_matching_critique(
                    detected_name, existing_critiques
                )
                if not match:
                    # Ce critique est "new" (manquant)
                    has_missing = True
                    break

            if has_missing:
                episodes_with_missing_critiques += 1

        return JSONResponse(content={"count": episodes_with_missing_critiques})

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur lors du comptage des critiques manquants: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.get("/api/episodes-with-avis-critiques")
async def get_episodes_with_avis_critiques() -> JSONResponse:
    """
    RÃ©cupÃ¨re la liste des Ã©pisodes qui ont des avis critiques associÃ©s.

    Returns:
        Liste des Ã©pisodes avec avis critiques
    """
    try:
        if (
            mongodb_service.episodes_collection is None
            or mongodb_service.avis_critiques_collection is None
        ):
            raise HTTPException(
                status_code=500, detail="Service MongoDB non disponible"
            )

        from bson import ObjectId

        # RÃ©cupÃ©rer tous les episode_oid qui ont des avis critiques (ce sont des strings)
        episode_ids_with_reviews = set(
            mongodb_service.avis_critiques_collection.distinct("episode_oid")
        )

        # Convertir les strings en ObjectId pour la recherche dans episodes
        episode_object_ids = [
            ObjectId(ep_id) for ep_id in episode_ids_with_reviews if ep_id
        ]

        # RÃ©cupÃ©rer les Ã©pisodes correspondants
        episodes = list(
            mongodb_service.episodes_collection.find(
                {"_id": {"$in": episode_object_ids}}
            )
        )

        # Convertir en format JSON compatible avec EpisodeDropdown
        # (mÃªme structure que /api/episodes-with-reviews)
        episodes_json = []
        for episode_data in episodes:
            from back_office_lmelp.models.episode import Episode

            episode = Episode(episode_data)

            # Issue #107: Filtrer les Ã©pisodes masquÃ©s
            if episode.masked:
                continue

            # Utiliser to_summary_dict() pour cohÃ©rence avec /api/episodes-with-reviews
            episode_dict = episode.to_summary_dict()

            # Ajouter les flags pour les pastilles de couleur dans EpisodeDropdown
            episode_oid = str(episode_data["_id"])

            # Pour la page Identification Critiques, les pastilles reflÃ¨tent UNIQUEMENT
            # le statut des critiques (pas des livres):
            # - ðŸŸ¢ Vert: TOUS les critiques ont Ã©tÃ© crÃ©Ã©s
            # - âšª Gris: AUCUN critique n'a Ã©tÃ© crÃ©Ã©
            # - ðŸ”´ Rouge: CERTAINS critiques crÃ©Ã©s ET CERTAINS "new"
            num_new_critiques = 0
            num_existing_critiques = 0

            avis_critique = mongodb_service.avis_critiques_collection.find_one(
                {"episode_oid": episode_oid}
            )
            if (
                avis_critique
                and avis_critique.get("summary")
                and mongodb_service.critiques_collection is not None
            ):
                # Extraire les critiques dÃ©tectÃ©s
                detected_critiques = (
                    critiques_extraction_service.extract_critiques_from_summary(
                        avis_critique["summary"]
                    )
                )
                if detected_critiques:
                    # RÃ©cupÃ©rer les critiques existants
                    existing_critiques = list(
                        mongodb_service.critiques_collection.find()
                    )
                    # Compter les critiques "new" vs existants
                    for detected_name in detected_critiques:
                        match = critiques_extraction_service.find_matching_critique(
                            detected_name, existing_critiques
                        )
                        if match:
                            num_existing_critiques += 1
                        else:
                            num_new_critiques += 1

            total_critiques = num_new_critiques + num_existing_critiques

            # DÃ©terminer les flags selon la logique:
            # - Si aucun critique: gris (has_cached_books=False)
            # - Si tous crÃ©Ã©s: vert (has_cached_books=True, has_incomplete_books=False)
            # - Si certains crÃ©Ã©s et certains new: rouge (has_cached_books=True, has_incomplete_books=True)
            if total_critiques == 0:
                # Aucun critique dÃ©tectÃ© â†’ Gris
                episode_dict["has_cached_books"] = False
                episode_dict["has_incomplete_books"] = False
            elif num_new_critiques == 0:
                # Tous les critiques ont Ã©tÃ© crÃ©Ã©s â†’ Vert
                episode_dict["has_cached_books"] = True
                episode_dict["has_incomplete_books"] = False
            else:
                # Au moins un critique "new" â†’ Rouge
                episode_dict["has_cached_books"] = True
                episode_dict["has_incomplete_books"] = True

            episodes_json.append(episode_dict)

        # Issue #154: Trier par date dÃ©croissante (le plus rÃ©cent en premier)
        episodes_json.sort(key=lambda ep: ep.get("date") or "", reverse=True)

        return JSONResponse(content=episodes_json)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur lors de la rÃ©cupÃ©ration des Ã©pisodes: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.post("/api/critiques")
async def create_critique(request: Request) -> JSONResponse:
    """
    CrÃ©e un nouveau critique.

    Body: {
        "nom": "PrÃ©nom Nom",
        "variantes": ["Variante 1", "Variante 2"],  # Optionnel
        "animateur": false
    }
    """
    try:
        if mongodb_service.critiques_collection is None:
            raise HTTPException(
                status_code=500, detail="Service MongoDB non disponible"
            )

        data = await request.json()

        # Valider les donnÃ©es
        if not data.get("nom"):
            raise HTTPException(status_code=400, detail="Le champ 'nom' est requis")

        # PrÃ©parer les variantes (enlever les doublons et les vides)
        variantes = data.get("variantes", [])
        if variantes:
            # Enlever les doublons et les chaÃ®nes vides
            variantes = list({v.strip() for v in variantes if v and v.strip()})
            # Enlever la variante si elle est identique au nom
            variantes = [v for v in variantes if v != data["nom"]]

        # VÃ©rifier si un critique avec ce nom existe dÃ©jÃ 
        existing = mongodb_service.critiques_collection.find_one({"nom": data["nom"]})
        if existing:
            # Comportement: Au lieu de rejeter, ajouter les variantes au critique existant
            # (Issue #154: permettre d'ajouter des variantes via l'interface de crÃ©ation)
            existing_variantes = set(existing.get("variantes", []))
            new_variantes = existing_variantes.union(set(variantes))

            # Mettre Ã  jour le critique avec les nouvelles variantes
            from datetime import datetime

            mongodb_service.critiques_collection.update_one(
                {"_id": existing["_id"]},
                {
                    "$set": {
                        "variantes": list(new_variantes),
                        "updated_at": datetime.now(),
                    }
                },
            )

            # RÃ©cupÃ©rer le critique mis Ã  jour
            updated_critique = mongodb_service.critiques_collection.find_one(
                {"_id": existing["_id"]}
            )

            if not updated_critique:
                raise HTTPException(
                    status_code=500, detail="Erreur lors de la mise Ã  jour du critique"
                )

            critique = Critique(updated_critique)

            return JSONResponse(
                content={
                    **critique.to_dict(),
                    "message": "Variantes ajoutÃ©es au critique existant",
                },
                status_code=200,
            )

        # PrÃ©parer les donnÃ©es pour MongoDB (nouveau critique)
        critique_data = Critique.for_mongodb_insert(
            {
                "nom": data["nom"],
                "variantes": variantes,
                "animateur": data.get("animateur", False),
            }
        )

        # InsÃ©rer dans MongoDB
        result = mongodb_service.critiques_collection.insert_one(critique_data)

        # RÃ©cupÃ©rer le critique crÃ©Ã©
        created_critique = mongodb_service.critiques_collection.find_one(
            {"_id": result.inserted_id}
        )

        if not created_critique:
            raise HTTPException(
                status_code=500, detail="Erreur lors de la crÃ©ation du critique"
            )

        critique = Critique(created_critique)

        return JSONResponse(content=critique.to_dict(), status_code=201)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur lors de la crÃ©ation du critique: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.put("/api/critiques/{critique_id}/variantes")
async def add_variante(critique_id: str, request: Request) -> JSONResponse:
    """
    Ajoute une variante Ã  un critique existant.

    Body: {
        "variante": "Nouvelle variante"
    }
    """
    try:
        from bson import ObjectId

        if mongodb_service.critiques_collection is None:
            raise HTTPException(
                status_code=500, detail="Service MongoDB non disponible"
            )

        data = await request.json()
        variante = data.get("variante")

        if not variante:
            raise HTTPException(
                status_code=400, detail="Le champ 'variante' est requis"
            )

        # RÃ©cupÃ©rer le critique
        critique_doc = mongodb_service.critiques_collection.find_one(
            {"_id": ObjectId(critique_id)}
        )

        if not critique_doc:
            raise HTTPException(status_code=404, detail="Critique non trouvÃ©")

        critique = Critique(critique_doc)

        # Ajouter la variante
        critique.add_variante(variante)

        # Mettre Ã  jour dans MongoDB
        result = mongodb_service.critiques_collection.update_one(
            {"_id": ObjectId(critique_id)},
            {
                "$set": {
                    "variantes": critique.variantes,
                    "updated_at": critique.updated_at,
                }
            },
        )

        if result.matched_count == 0:
            raise HTTPException(status_code=404, detail="Critique non trouvÃ©")

        return JSONResponse(content=critique.to_dict())

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur lors de l'ajout de la variante: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.post("/api/emissions")
async def create_emission(request: Request) -> JSONResponse:
    """
    CrÃ©e une nouvelle Ã©mission.

    Body: {
        "episode_id": "ObjectId",
        "avis_critique_id": "ObjectId",
        "animateur_id": "ObjectId" (optionnel)
    }
    """
    try:
        from bson import ObjectId

        if (
            mongodb_service.episodes_collection is None
            or mongodb_service.emissions_collection is None
        ):
            raise HTTPException(
                status_code=500, detail="Service MongoDB non disponible"
            )

        data = await request.json()

        # Valider les donnÃ©es
        if not data.get("episode_id") or not data.get("avis_critique_id"):
            raise HTTPException(
                status_code=400,
                detail="Les champs 'episode_id' et 'avis_critique_id' sont requis",
            )

        # RÃ©cupÃ©rer l'Ã©pisode pour copier date et durÃ©e
        episode = mongodb_service.episodes_collection.find_one(
            {"_id": ObjectId(data["episode_id"])}
        )

        if not episode:
            raise HTTPException(status_code=404, detail="Ã‰pisode non trouvÃ©")

        # PrÃ©parer les donnÃ©es pour MongoDB
        emission_data = Emission.for_mongodb_insert(
            {
                "episode_id": ObjectId(data["episode_id"]),
                "avis_critique_id": ObjectId(data["avis_critique_id"]),
                "date": episode.get("date"),
                "duree": episode.get("duree"),
                "animateur_id": (
                    ObjectId(data["animateur_id"]) if data.get("animateur_id") else None
                ),
                "avis_ids": [],  # Sera rempli dans une future issue
            }
        )

        # InsÃ©rer dans MongoDB
        result = mongodb_service.emissions_collection.insert_one(emission_data)

        # RÃ©cupÃ©rer l'Ã©mission crÃ©Ã©e
        created_emission = mongodb_service.emissions_collection.find_one(
            {"_id": result.inserted_id}
        )

        if not created_emission:
            raise HTTPException(
                status_code=500, detail="Erreur lors de la crÃ©ation de l'Ã©mission"
            )

        emission = Emission(created_emission)

        return JSONResponse(content=emission.to_dict(), status_code=201)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur lors de la crÃ©ation de l'Ã©mission: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


# Variables globales pour la gestion propre du serveur
_server_instance = None


def signal_handler(signum, frame):
    """Gestionnaire de signaux pour arrÃªt propre."""
    print(f"\nðŸ›‘ Signal {signum} reÃ§u - ArrÃªt en cours...")

    # ArrÃªter le serveur proprement si disponible
    if _server_instance is not None:
        _server_instance.should_exit = True
        print("ðŸ“¡ Signal d'arrÃªt envoyÃ© au serveur")

    # Forcer la fermeture des ressources
    with suppress(Exception):
        mongodb_service.disconnect()
        print("ðŸ”Œ MongoDB dÃ©connectÃ©")

    # Nettoyer le fichier de dÃ©couverte de port unifiÃ©
    with suppress(Exception):
        port_file = PortDiscovery.get_unified_port_file_path()
        PortDiscovery.cleanup_unified_port_file(port_file)
        print("ðŸ§¹ Unified port discovery file cleaned up")


def find_free_port_or_default() -> int:
    """Find a free port using priority strategy.

    Priority order:
    1. Environment variable API_PORT if specified (current behavior)
    2. Default preferred port 54321 (try first)
    3. Fallback range 54322-54350 (scan for first available)

    Returns:
        Available port number

    Raises:
        ValueError: If environment variable contains invalid port number
        RuntimeError: If no available port is found in the range
    """
    # 1. Check environment variable first
    if "API_PORT" in os.environ:
        api_port_str = os.getenv("API_PORT")
        if api_port_str is None:
            # Should not happen since we checked the key exists, but satisfy mypy
            raise ValueError("API_PORT environment variable is None")
        try:
            return int(api_port_str)
        except (TypeError, ValueError) as e:
            raise ValueError(
                f"Invalid API_PORT environment variable: {api_port_str}"
            ) from e

    # Get the same host that will be used by the server
    host = os.getenv("API_HOST", "0.0.0.0")

    # 2. Try preferred default port 54321
    try:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind((host, 54321))
            return 54321
    except OSError:
        # Port 54321 is occupied, continue to fallback range
        pass

    # 3. Scan fallback range 54322-54350 (29 attempts)
    return PortDiscovery.find_available_port(
        start_port=54322, max_attempts=29, host=host
    )


def main():
    """Fonction principale pour dÃ©marrer le serveur."""
    global _server_instance

    import signal

    import uvicorn

    # Enregistrer les gestionnaires de signaux
    signal.signal(signal.SIGINT, signal_handler)  # Ctrl+C
    signal.signal(signal.SIGTERM, signal_handler)  # Termination

    host = os.getenv("API_HOST", "0.0.0.0")
    port = find_free_port_or_default()

    # Determine if port was automatically selected
    port_auto_selected = "API_PORT" not in os.environ
    port_message = f"ðŸš€ DÃ©marrage du serveur sur {host}:{port}"
    if port_auto_selected:
        port_message += " (port automatiquement sÃ©lectionnÃ©)"

    print(port_message)
    print("ðŸ›¡ï¸ Garde-fou mÃ©moire activÃ©")

    # Create unified port discovery file for frontend
    port_file = PortDiscovery.get_unified_port_file_path()
    PortDiscovery.write_backend_info_to_unified_file(port_file, port, host)
    print(f"ðŸ“¡ Unified port discovery file created: {port_file}")

    try:
        # CrÃ©er la configuration uvicorn avec des paramÃ¨tres pour un arrÃªt propre
        config = uvicorn.Config(
            app,
            host=host,
            port=port,
            access_log=False,
            server_header=False,
            date_header=False,
            lifespan="on",
            # ParamÃ¨tres pour un arrÃªt plus propre
            timeout_keep_alive=5,
            timeout_graceful_shutdown=10,
        )

        # CrÃ©er le serveur et le garder en rÃ©fÃ©rence globale
        _server_instance = uvicorn.Server(config)

        # DÃ©marrer le serveur
        _server_instance.run()

    except KeyboardInterrupt:
        print("\nâš ï¸ Interruption clavier dÃ©tectÃ©e")
    except Exception as e:
        print(f"âŒ Erreur serveur: {e}")
    finally:
        # Nettoyage final garanti
        print("ðŸ§¹ Nettoyage final...")
        with suppress(Exception):
            mongodb_service.disconnect()
        with suppress(Exception):
            port_file = PortDiscovery.get_unified_port_file_path()
            PortDiscovery.cleanup_unified_port_file(port_file)
        print("âœ… ArrÃªt complet")


if __name__ == "__main__":
    main()
