# Issue #188 - URL dynamique Anna's Archive avec interface fran√ßaise

**Date**: 2026-01-31
**Issue**: [#188](https://github.com/castorfou/back-office-lmelp/issues/188) - Rendre URL anna's archive dynamique
**Commit**: e40f43b
**Branche**: 188-rendre-url-annas-archive-dynamique

## Contexte

L'URL d'Anna's Archive √©tait hardcod√©e (`https://fr.annas-archive.org`) dans le frontend. Cette URL change fr√©quemment (domaines bloqu√©s, migrations), n√©cessitant des modifications manuelles du code √† chaque changement.

**Probl√®me identifi√©**: URL hardcod√©e dans `frontend/src/views/LivreDetail.vue:212` qui ne r√©pondait plus.

## Solution impl√©ment√©e

### Architecture hybrid avec pr√©fixe fran√ßais automatique

Strat√©gie √† 3 niveaux avec ajout automatique du sous-domaine `fr.` pour l'interface fran√ßaise:

1. **Priority 1**: Variable d'environnement (`ANNAS_ARCHIVE_URL`) + health check (2s timeout)
2. **Priority 2**: Wikipedia scraping (cache 24h) + health check
3. **Priority 3**: Hardcoded default fallback (`https://fr.annas-archive.org`)

### Pr√©fixe fran√ßais automatique

**Innovation cl√©**: Ajout automatique du sous-domaine `fr.` pour forcer l'interface fran√ßaise d'Anna's Archive.

Exemples de transformation:
- `https://annas-archive.li` ‚Üí `https://fr.annas-archive.li`
- `https://annas-archive.se` ‚Üí `https://fr.annas-archive.se`
- `https://fr.annas-archive.org` ‚Üí `https://fr.annas-archive.org` (pr√©serv√© si d√©j√† pr√©sent)

Impl√©ment√© dans `src/back_office_lmelp/services/annas_archive_url_service.py:244-268`:

```python
def _normalize_url(self, url: str) -> str:
    """Normalize URL to base domain with 'fr.' subdomain."""
    from urllib.parse import urlparse

    parsed = urlparse(url)
    netloc = parsed.netloc

    # Add 'fr.' prefix if not already present for French interface
    if not netloc.startswith("fr."):
        netloc = f"fr.{netloc}"

    return f"{parsed.scheme}://{netloc}"
```

## Composants cr√©√©s

### Backend

**Nouveau service**: `src/back_office_lmelp/services/annas_archive_url_service.py` (269 lignes)

Fonctionnalit√©s:
- **Health check** (`_health_check_url()`): Timeout 2s pour d√©tecter URLs mortes
- **Wikipedia scraping** (`_scrape_wikipedia_url()`): 2 strat√©gies de parsing
  - Strategy 1: Extraction depuis infobox (`_parse_infobox()`)
  - Strategy 2: Premier lien externe dans contenu principal (`_parse_external_links()`)
- **Cache 24h**: R√©utilise `BabelioCacheService` pour persister URLs scrap√©es
- **Normalisation URL**: Strip path/query + ajout pr√©fixe `fr.` automatique
- **Debug logging**: Variable `ANNAS_ARCHIVE_DEBUG_LOG` pour diagnostics

**Nouveau endpoint**: `src/back_office_lmelp/app.py:2468-2489`

```python
@app.get("/api/config/annas-archive-url")
async def get_annas_archive_url() -> dict[str, str]:
    """Get current Anna's Archive base URL with French interface."""
    try:
        url = await annas_archive_url_service.get_url()
        return {"url": url}
    except Exception as e:
        logger.error(f"Error getting Anna's Archive URL: {e}")
        return {"url": "https://fr.annas-archive.org"}  # Ultimate fallback
```

**Settings**: `src/back_office_lmelp/settings.py:51-63`

Nouvelle propri√©t√© `annas_archive_url` pour acc√®s √† la variable d'environnement.

### Frontend

**Modifications**: `frontend/src/views/LivreDetail.vue`

Changements cl√©s:
- **Nouvelle propri√©t√© data** (`annasArchiveBaseUrl`): Stocke URL dynamique avec fallback
- **Nouvelle m√©thode** (`loadAnnasArchiveUrl()`): Appel API pour r√©cup√©rer URL
- **Chargement parall√®le**: Utilisation de `Promise.all()` dans `mounted()` pour charger livre + URL simultan√©ment
- **M√©thode mise √† jour** (`getAnnasArchiveUrl()`): Utilise `annasArchiveBaseUrl` au lieu de hardcoded

```javascript
// Avant (hardcoded)
return `https://fr.annas-archive.org/search?q=${encodedQuery}`;

// Apr√®s (dynamique avec pr√©fixe fr.)
return `${this.annasArchiveBaseUrl}/search?q=${encodedQuery}`;
// annasArchiveBaseUrl = "https://fr.annas-archive.li" (depuis API)
```

### Tests

**Backend**: 16 tests (3 nouveaux + 13 existants mis √† jour)

Fichiers:
- `tests/test_annas_archive_url_service.py` (304 lignes, 13 tests)
- `tests/test_settings.py` (53 lignes, 3 tests)

**Nouveaux tests pour pr√©fixe fran√ßais** (`tests/test_annas_archive_url_service.py:29-80`):

```python
class TestUrlNormalization:
    def test_normalize_url_should_add_fr_subdomain_when_missing(self, service):
        """Should add 'fr.' subdomain to domains without language prefix."""
        url = "https://annas-archive.li/search?q=test"
        result = service._normalize_url(url)
        assert result == "https://fr.annas-archive.li"

    def test_normalize_url_should_preserve_existing_fr_subdomain(self, service):
        """Should not duplicate 'fr.' if already present."""
        url = "https://fr.annas-archive.se/about"
        result = service._normalize_url(url)
        assert result == "https://fr.annas-archive.se"
```

Sc√©narios couverts:
- ‚úÖ Health check avec URL accessible
- ‚úÖ Health check avec timeout (fallback √† Wikipedia)
- ‚úÖ Cache 24h avec URL saine
- ‚úÖ Re-scraping si URL cach√©e timeout
- ‚úÖ Scraping Wikipedia apr√®s expiration cache
- ‚úÖ Hardcoded default si Wikipedia √©choue
- ‚úÖ Parsing infobox Wikipedia
- ‚úÖ Ajout pr√©fixe `fr.` aux domaines sans langue
- ‚úÖ Pr√©servation pr√©fixe `fr.` existant
- ‚úÖ Suppression path/query avec ajout `fr.`

**Frontend**: 5 tests mis √† jour

Fichier: `frontend/tests/unit/livreDetailAnnasArchive.spec.js`

Tous les mocks API mis √† jour pour utiliser `https://fr.annas-archive.se` au lieu de `https://annas-archive.se`.

**Fixture r√©elle**: `tests/fixtures/annas_archive/wikipedia_page.html` (1836 lignes)

Capture HTML r√©elle de Wikipedia pour garantir que les tests refl√®tent le comportement production (le√ßon de l'Issue #85).

## Configuration

**Fichier**: `docker/deployment/.env.template`

```bash
# Anna's Archive URL (optional)
# Si non d√©finie, l'URL sera automatiquement r√©cup√©r√©e depuis Wikipedia
# Le pr√©fixe 'fr.' est ajout√© automatiquement pour l'interface fran√ßaise
# Exemples:
#   ANNAS_ARCHIVE_URL=https://annas-archive.li  ‚Üí https://fr.annas-archive.li
#   ANNAS_ARCHIVE_URL=https://fr.annas-archive.se  ‚Üí https://fr.annas-archive.se (pr√©serv√©)
ANNAS_ARCHIVE_URL=
```

## Patterns et apprentissages

### 1. TDD avec tests de normalisation URL

**Phase RED**: √âcriture de 3 tests pour v√©rifier ajout pr√©fixe `fr.`

**Phase GREEN**: Impl√©mentation de `_normalize_url()` avec logique conditionnelle

```python
# Pattern: Ajout conditionnel de sous-domaine
if not netloc.startswith("fr."):
    netloc = f"fr.{netloc}"
```

### 2. Strat√©gie de fallback robuste

**Pattern √† 3 niveaux** avec health checks:

```python
async def get_url(self) -> str:
    # Level 1: Env var (if set)
    if self.settings.annas_archive_url:
        if await self._health_check_url(self.settings.annas_archive_url):
            return self.settings.annas_archive_url

    # Level 2: Cached Wikipedia URL
    cached = self.cache_service.get_cached(...)
    if cached and await self._health_check_url(cached_url):
        return cached_url

    # Re-scrape if cache expired or unhealthy
    scraped_url = await self._scrape_wikipedia_url()
    if scraped_url:
        self.cache_service.set_cached(...)
        return scraped_url

    # Level 3: Hardcoded default
    return self.hardcoded_default
```

**Avantages**:
- ‚úÖ R√©silience: Continue de fonctionner m√™me si Wikipedia est down
- ‚úÖ Performance: Cache 24h r√©duit les appels Wikipedia
- ‚úÖ Flexibilit√©: Permet override via env var pour d√©ploiements sp√©cifiques

### 3. Health check avec timeout court

**Pattern**: Timeout 2s pour d√©tection rapide de URLs mortes

```python
async def _health_check_url(self, url: str) -> bool:
    """Health check with 2s timeout."""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.head(url, timeout=2, allow_redirects=True):
                return True
    except (TimeoutError, aiohttp.ClientError):
        return False
```

**Pourquoi 2s?** Compromis entre:
- Trop court (< 1s): Faux n√©gatifs sur connexions lentes
- Trop long (> 5s): Ralentit fallback en cas d'URL morte

### 4. Wikipedia scraping avec 2 strat√©gies

**Strategy 1** (infobox): Structure stable, prioritaire

```python
def _parse_infobox(self, soup: BeautifulSoup) -> str | None:
    """Parse Wikipedia infobox for official URL."""
    infobox = soup.find("table", class_="infobox")
    for row in infobox.find_all("tr"):
        header = row.find("th")
        if "website" in header.get_text(strip=True).lower():
            link = row.find("a", href=True)
            return self._normalize_url(link["href"])
```

**Strategy 2** (external links): Fallback si infobox √©choue

```python
def _parse_external_links(self, soup: BeautifulSoup) -> str | None:
    """Parse external links section for official URL."""
    content = soup.find("div", id="mw-content-text")
    for link in content.find_all("a", href=True):
        href = link["href"]
        if "annas-archive" in href:
            return self._normalize_url(href)
```

**Le√ßon**: Toujours avoir un fallback pour scraping (structure HTML peut changer).

### 5. Cache disk 24h r√©utilis√©

**Pattern**: R√©utilisation de `BabelioCacheService` existant au lieu de cr√©er nouveau cache

```python
# Initialisation dans app.py
annas_archive_url_service = AnnasArchiveUrlService(
    settings=settings,
    cache_service=babelio_cache_service  # R√©utilisation!
)
```

**Avantages**:
- ‚úÖ Pas de duplication de code
- ‚úÖ M√™me TTL (24h) pour tous les caches externes
- ‚úÖ M√™me strat√©gie de persistence (disk-based)

### 6. Chargement parall√®le frontend

**Pattern**: `Promise.all()` pour charger donn√©es ind√©pendantes simultan√©ment

```javascript
async mounted() {
  await Promise.all([
    this.loadLivre(),           // API 1: Donn√©es livre
    this.loadAnnasArchiveUrl()  // API 2: URL Anna's Archive
  ]);
}
```

**Avant** (s√©quentiel):
```javascript
async mounted() {
  await this.loadLivre();           // 500ms
  await this.loadAnnasArchiveUrl(); // 200ms
  // Total: 700ms
}
```

**Apr√®s** (parall√®le):
```javascript
async mounted() {
  await Promise.all([...]);
  // Total: max(500ms, 200ms) = 500ms
}
```

**Gain**: 30% plus rapide dans cet exemple.

### 7. Debug logging contr√¥l√© par env var

**Pattern**: Garder les logs debug dans le code, contr√¥ler via variable d'environnement

```python
def __init__(self, settings, cache_service):
    self._debug_log_enabled = os.getenv("ANNAS_ARCHIVE_DEBUG_LOG", "0").lower() in ("1", "true")

async def get_url(self) -> str:
    if self._debug_log_enabled:
        logger.info(f"üîß Testing env var ANNAS_ARCHIVE_URL: {self.settings.annas_archive_url}")
```

**Avantages**:
- ‚úÖ Logs disponibles pour diagnostic futur (activation via env var)
- ‚úÖ Pas de pollution en production (d√©sactiv√© par d√©faut)
- ‚úÖ Facilite debugging probl√®mes complexes (health checks, scraping)

Voir `CLAUDE.md` section "Debug Logging Strategy" pour convention compl√®te.

## R√©sultats

### Tests
- **Backend**: 16/16 tests passent ‚úÖ
- **Frontend**: 5/5 tests passent ‚úÖ
- **Coverage**: Service √† 66% (lignes non-error principalement)

### Linting & Type checking
- **Ruff**: ‚úÖ Aucune erreur
- **MyPy**: ‚úÖ Aucune erreur de type
- **Pre-commit hooks**: ‚úÖ Tous passent

### Comportement production

**Test manuel** (confirm√© par utilisateur):

```bash
curl http://localhost:8000/api/config/annas-archive-url | jq
# R√©sultat: {"url": "https://fr.annas-archive.li"}
```

Frontend:
- URL g√©n√©r√©e: `https://fr.annas-archive.li/search?q=Marx+en+Am√©rique+-+Christian+Laval`
- Interface: ‚úÖ Affich√©e en fran√ßais automatiquement gr√¢ce au sous-domaine `fr.`

**Avant cette issue**: URL hardcod√©e `https://fr.annas-archive.org` ne r√©pondait plus
**Apr√®s**: URL automatiquement mise √† jour vers `https://fr.annas-archive.li` (depuis Wikipedia)

## M√©triques

- **Fichiers modifi√©s**: 9 (3 backend, 2 frontend, 4 tests)
- **Lignes ajout√©es**: 2606
- **Nouveau service**: 269 lignes
- **Tests backend**: 357 lignes (304 + 53)
- **Fixture Wikipedia**: 1836 lignes
- **Tests frontend**: 82 lignes modifi√©es

## Points d'attention futurs

### 1. Monitoring health checks

Consid√©rer l'ajout de m√©triques pour tracker:
- Taux de succ√®s health checks (env var vs Wikipedia)
- Fr√©quence de fallback au hardcoded default
- Latence moyenne des health checks

### 2. Wikipedia scraping fragile

**Risque**: Structure HTML Wikipedia peut changer et casser le scraping.

**Mitigation actuelle**:
- 2 strat√©gies de parsing (infobox + external links)
- Hardcoded default en dernier recours
- Tests avec fixture HTML r√©elle

**Am√©lioration future**: Monitoring pour alerter si scraping √©choue pendant >24h.

### 3. Cache invalidation manuelle

**Limitation**: Cache 24h fixe, pas de m√©canisme pour invalider manuellement.

**Workaround actuel**: Red√©marrer backend pour forcer re-scraping.

**Am√©lioration future**: Endpoint admin `/api/admin/annas-archive-url/refresh` pour forcer refresh.

### 4. Gestion des sous-domaines multi-langues

**Question**: Que faire si Anna's Archive ajoute d'autres sous-domaines (`en.`, `es.`, etc.)?

**Solution actuelle**: Pr√©fixe `fr.` hardcod√© pour interface fran√ßaise.

**Am√©lioration future**: Variable d'environnement `ANNAS_ARCHIVE_LANG_PREFIX=fr` pour flexibilit√©.

## R√©f√©rences

- **Issue GitHub**: [#188](https://github.com/castorfou/back-office-lmelp/issues/188)
- **Commit**: e40f43b
- **Le√ßons appliqu√©es**:
  - Issue #85: Utiliser fixtures HTML r√©elles pour tests de scraping
  - CLAUDE.md "Debug Logging Strategy": Logs debug contr√¥l√©s par env var
  - CLAUDE.md "Vue.js UI Patterns": `Promise.all()` pour chargement parall√®le

## Commandes utiles

```bash
# Tester l'API
curl http://localhost:8000/api/config/annas-archive-url | jq

# Activer debug logging
export ANNAS_ARCHIVE_DEBUG_LOG=1

# Forcer une URL sp√©cifique
export ANNAS_ARCHIVE_URL=https://annas-archive.se
# ‚Üí API retournera https://fr.annas-archive.se (pr√©fixe ajout√©)

# Tester health check
curl -I https://fr.annas-archive.li  # Devrait retourner 200 OK
```
