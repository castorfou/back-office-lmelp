# G√©n√©ration LLM d'Avis Critiques - Architecture

## Vue d'ensemble

Le syst√®me de g√©n√©ration LLM transforme les transcriptions Whisper d'√©pisodes en r√©sum√©s structur√©s des avis critiques. Le processus utilise Azure OpenAI (GPT-4o) et se d√©roule en deux phases distinctes pour garantir la qualit√© et la pr√©cision des informations.

**Architecture en 2 phases** :
- **Phase 1 (Extraction)** : Extrait les informations depuis la transcription Whisper
- **Phase 2 (Correction)** : Corrige l'orthographe des noms propres via le contenu de la page RadioFrance

**Optimisation parall√®le** : Phase 1 et fetch URL RadioFrance s'ex√©cutent simultan√©ment avec `asyncio.gather()`.

## Service Principal

### `AvisCritiquesGenerationService`

**Fichier** : `src/back_office_lmelp/services/avis_critiques_generation_service.py`

**Singleton** : `avis_critiques_generation_service` (instance globale)

#### Configuration Azure OpenAI

```python
# Variables d'environnement requises
AZURE_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_API_KEY=your_key_here
AZURE_API_VERSION=2024-09-01-preview  # D√©faut
AZURE_DEPLOYMENT_NAME=gpt-4o  # D√©faut

# Debug logging (optionnel)
AVIS_CRITIQUES_DEBUG_LOG=1  # Active logs d√©taill√©s
```

Le client Azure OpenAI est initialis√© dans `__init__()` avec validation des variables d'environnement.

#### M√©thode principale : `generate_full_summary()`

```python
async def generate_full_summary(
    transcription: str,
    episode_date: str,
    episode_page_url: str | None,
    episode_id: str | None = None
) -> dict[str, Any]:
    """
    Orchestrateur complet du processus de g√©n√©ration.

    Returns:
        {
            "summary": str,  # R√©sum√© final (Phase 2 corrig√©)
            "summary_phase1": str,  # R√©sum√© brut (backup)
            "metadata": dict,  # M√©tadonn√©es RadioFrance
            "corrections_applied": list[dict],  # Corrections d√©tect√©es
            "warnings": list[str]  # Avertissements √©ventuels
        }
    """
```

**Workflow** :

1. **Ex√©cution parall√®le** (si URL non disponible) :
   ```python
   summary_phase1, fetched_url = await asyncio.gather(
       self.generate_summary_phase1(transcription, episode_date),
       fetch_episode_url()  # Recherche + mise √† jour MongoDB
   )
   ```

2. **Extraction m√©tadonn√©es** :
   ```python
   metadata = await radiofrance_service.extract_episode_metadata(episode_page_url)
   page_content = metadata.get("page_text", "")  # Contenu complet page
   ```

3. **Phase 2 - Correction** :
   ```python
   summary_phase2 = await self.enhance_summary_phase2(
       summary_phase1, metadata, page_content
   )
   ```

4. **D√©tection corrections** :
   ```python
   corrections_applied = self._detect_corrections(summary_phase1, summary_phase2)
   ```

### Phase 1 : Extraction depuis transcription

#### `generate_summary_phase1(transcription, episode_date)`

**Objectif** : Extraire informations structur√©es depuis transcription Whisper.

**Param√®tres LLM** :
```python
model=self.deployment_name,  # gpt-4o
max_tokens=8000,
temperature=0.1,  # Faible pour consistance
timeout=120  # 2 minutes
```

**Retry logic** :
```python
max_retries = 1  # 2 tentatives au total
for attempt in range(max_retries + 1):
    try:
        response = await asyncio.wait_for(
            asyncio.to_thread(self.client.chat.completions.create, ...),
            timeout=120
        )
        return response.choices[0].message.content
    except TimeoutError:
        if attempt < max_retries:
            await asyncio.sleep(2)  # Attente avant retry
            continue
        raise
```

**Format de sortie attendu** :

```markdown
## 1. LIVRES DISCUT√âS AU PROGRAMME du DD mois YYYY

| Auteur | Titre | √âditeur | Avis d√©taill√©s des critiques | Note moyenne | Nb critiques | Coup de c≈ìur | Chef d'≈ìuvre |
|--------|-------|---------|------------------------------|--------------|-------------|-------------|-------------|
| ... | ... | ... | **Critique 1**: avis (note) <br>**Critique 2**: avis (note) | 8.5 | 3 | Critique 1 | - |

## 2. COUPS DE C≈íUR DES CRITIQUES du DD mois YYYY

| Auteur | Titre | √âditeur | Critique | Note | Commentaire |
|--------|-------|---------|----------|------|-------------|
| ... | ... | ... | Critique 1 | 9.0 | ... |
```

**Validation format** :
```python
def _is_valid_markdown_format(summary: str) -> bool:
    # V√©rifier pr√©sence titre section
    if not re.search(r"## 1\. LIVRES DISCUT", summary):
        return False

    # V√©rifier pr√©sence tableaux markdown
    if "|" not in summary:
        return False

    # Longueur minimale
    return len(summary) >= 200
```

**Formatage dates en fran√ßais** :

```python
mois_fr = {
    1: "janvier", 2: "f√©vrier", 3: "mars", 4: "avril",
    5: "mai", 6: "juin", 7: "juillet", 8: "ao√ªt",
    9: "septembre", 10: "octobre", 11: "novembre", 12: "d√©cembre"
}

date_obj = datetime.strptime(episode_date, "%Y-%m-%d")
date_str = f" du {date_obj.day} {mois_fr[date_obj.month]} {date_obj.year}"
```

### Phase 2 : Correction orthographique

#### `enhance_summary_phase2(summary_phase1, episode_metadata, page_content)`

**Objectif** : Corriger noms propres (auteurs, critiques, titres) avec donn√©es RadioFrance.

**Param√®tres LLM** :
```python
messages=[
    {"role": "system", "content": "Tu es un correcteur orthographique..."},
    {"role": "user", "content": prompt}
],
max_tokens=8000,
temperature=0.1,
timeout=60  # 1 minute
```

**Sources de correction** :

1. **M√©tadonn√©es RadioFrance** :
   ```python
   metadata = {
       "animateur": "J√©r√¥me Garcin",
       "critiques": ["Elisabeth Philippe", "Fr√©d√©ric Beigbeder", ...],
       "date": "2024-01-15",
       "page_text": "..."  # Contenu complet page HTML
   }
   ```

2. **Contenu page RadioFrance** (max 3000 premiers caract√®res) :
   - Section livres du programme : `¬´ Titre ¬ª, de Auteur (√âditeur)`
   - Section coups de c≈ìur : `"Critique: Titre, de Auteur (√âditeur)"`

**Validation pr√©servation structure** :

```python
# Phase 2 doit pr√©server format markdown Phase 1
if not self._is_valid_markdown_format(summary_phase2):
    logger.warning("Phase 2 a cass√© la structure, fallback Phase 1")
    return summary_phase1
```

**Fallback automatique** :

La Phase 2 ne l√®ve jamais d'exception. En cas d'erreur, elle retourne le r√©sultat de Phase 1 :

```python
try:
    summary_phase2 = await self.enhance_summary_phase2(...)
    return summary_phase2
except Exception as e:
    logger.warning(f"Erreur Phase 2, fallback Phase 1: {e}")
    return summary_phase1
```

### D√©tection des corrections

#### `_detect_corrections(summary_phase1, summary_phase2)`

**Algorithme** : Comparaison ligne par ligne avec d√©tection diff√©rences de mots.

```python
corrections = []
lines1 = summary_phase1.split("\n")
lines2 = summary_phase2.split("\n")

for i, (line1, line2) in enumerate(zip(lines1, lines2)):
    if line1 != line2:
        words1 = set(line1.split())
        words2 = set(line2.split())

        removed = words1 - words2
        added = words2 - words1

        if removed and added:
            corrections.append({
                "field": f"ligne {i + 1}",
                "before": " ".join(removed)[:50],
                "after": " ".join(added)[:50]
            })

return corrections[:10]  # Limiter √† 10 affich√©es
```

## API Endpoints

### POST `/api/avis-critiques/generate`

**Objectif** : G√©n√©rer le r√©sum√© d'avis critiques en 2 phases.

**Request Body** :
```json
{
  "episode_id": "507f1f77bcf86cd799439011"  // pragma: allowlist secret
}
```

**Response** :
```json
{
  "summary": "...",  // R√©sum√© final (Phase 2)
  "summary_phase1": "...",  // R√©sum√© brut (backup)
  "metadata": {
    "animateur": "J√©r√¥me Garcin",
    "critiques": ["Elisabeth Philippe", ...],
    "date": "2024-01-15"
  },
  "corrections_applied": [
    {
      "field": "ligne 12",
      "before": "Houllebeck",
      "after": "Houellebecq"
    }
  ],
  "warnings": []
}
```

**Impl√©mentation** (`app.py:2895-2982`) :

```python
@app.post("/api/avis-critiques/generate")
async def generate_avis_critiques(request: Request) -> JSONResponse:
    data = await request.json()
    episode_id = data.get("episode_id")

    # Validation √©pisode
    episode = mongodb_service.get_episode_by_id(episode_id)
    if not episode:
        raise HTTPException(status_code=404, detail="√âpisode non trouv√©")

    # Validation transcription
    if not episode.get("transcription_whisper"):
        raise HTTPException(status_code=400, detail="Transcription manquante")

    # G√©n√©ration compl√®te
    result = await avis_critiques_generation_service.generate_full_summary(
        transcription=episode["transcription_whisper"],
        episode_date=episode_date,
        episode_page_url=episode.get("episode_page_url"),
        episode_id=episode_id
    )

    return JSONResponse(content=result)
```

### POST `/api/avis-critiques/save`

**Objectif** : Sauvegarder le r√©sum√© g√©n√©r√© dans MongoDB.

**Request Body** :
```json
{
  "episode_id": "507f1f77bcf86cd799439011",  // pragma: allowlist secret
  "summary": "## 1. LIVRES DISCUT√âS...",
  "summary_phase1": "...",
  "metadata": {
    "animateur": "...",
    "critiques": [...]
  }
}
```

**Validation backend** (5 crit√®res) :

```python
def _validate_summary(summary: str) -> tuple[bool, str | None]:
    """Valide le r√©sum√© avant sauvegarde."""

    # 1. Non vide
    if not summary or not summary.strip():
        return False, "Le r√©sum√© est vide"

    # 2. Longueur raisonnable (< 50000 chars)
    if len(summary) > 50000:
        return False, "Le r√©sum√© est anormalement long (malform√©)"

    # 3. Pas d'espaces excessifs (bug LLM)
    if re.search(r' {100,}', summary):
        return False, "Le r√©sum√© contient trop d'espaces cons√©cutifs (malform√©)"

    # 4. Section "LIVRES DISCUT√âS" pr√©sente
    if "LIVRES DISCUT√âS" not in summary:
        return False, "Section 'LIVRES DISCUT√âS' manquante"

    # 5. Section "COUPS DE C≈íUR" pr√©sente
    if "COUPS DE C≈íUR" not in summary:
        return False, "Section 'COUPS DE C≈íUR' manquante"

    return True, None
```

**Impl√©mentation** (`app.py:3017-3100`) :

```python
@app.post("/api/avis-critiques/save")
async def save_avis_critiques(request: Request) -> JSONResponse:
    data = await request.json()

    # Validation r√©sum√©
    is_valid, error_msg = _validate_summary(data["summary"])
    if not is_valid:
        raise HTTPException(status_code=400, detail=error_msg)

    # Nettoyage metadata (retirer page_text volumineux)
    metadata_clean = {
        k: v for k, v in data["metadata"].items()
        if k != "page_text"
    }

    avis_data = {
        "episode_oid": data["episode_id"],
        "episode_title": episode_title,
        "episode_date": episode_date,
        "summary": data["summary"],
        "summary_phase1": data["summary_phase1"],
        "summary_origin": "llm_generation_phase2",
        "metadata_source": metadata_clean,
        "updated_at": datetime.now(UTC)
    }

    # Insert ou Update
    existing = mongodb_service.avis_critiques_collection.find_one(
        {"episode_oid": data["episode_id"]}
    )

    if existing:
        mongodb_service.avis_critiques_collection.update_one(
            {"episode_oid": data["episode_id"]},
            {"$set": avis_data}
        )
        avis_id = str(existing["_id"])
    else:
        avis_data["created_at"] = datetime.now(UTC)
        result = mongodb_service.avis_critiques_collection.insert_one(avis_data)
        avis_id = str(result.inserted_id)

    return JSONResponse(content={"success": True, "avis_critique_id": avis_id})
```

### GET `/api/avis-critiques/episodes-sans-avis`

**Objectif** : Liste des √©pisodes √©ligibles pour g√©n√©ration LLM.

**Crit√®res** :
- Episodes **non masqu√©s** (`masque != true`)
- Avec **transcription disponible** (`transcription_whisper` non vide)
- Tri par date d√©croissante (plus r√©cents d'abord)

**Response** :
```json
[
  {
    "id": "507f1f77bcf86cd799439011",  // pragma: allowlist secret
    "titre": "Les nouvelles pages du polar",
    "date": "2024-01-15T00:00:00",
    "has_avis_critiques": false
  }
]
```

### GET `/api/avis-critiques/{episode_id}`

**Objectif** : R√©cup√©rer le r√©sum√© existant d'un √©pisode.

**Response** (si existant) :
```json
{
  "episode_oid": "507f1f77bcf86cd799439011",  // pragma: allowlist secret
  "summary": "## 1. LIVRES DISCUT√âS...",
  "summary_phase1": "...",
  "metadata_source": {...}
}
```

**Response** (si non existant) : HTTP 404

## Structure MongoDB

### Collection `avis_critiques`

**Sch√©ma** :

```javascript
{
  "_id": ObjectId,
  "episode_oid": String,  // R√©f√©rence vers episodes._id (STRING, pas ObjectId)
  "episode_title": String,
  "episode_date": String,  // Format YYYY-MM-DD
  "summary": String,  // R√©sum√© final (Phase 2)
  "summary_phase1": String,  // R√©sum√© brut (backup)
  "summary_origin": String,  // "llm_generation_phase2"
  "metadata_source": {
    "animateur": String,
    "critiques": [String],
    "date": String
    // Note: "page_text" retir√© pour √©conomie stockage
  },
  "created_at": DateTime,
  "updated_at": DateTime
}
```

**Indexes** :
```javascript
{ "episode_oid": 1 }  // Unique, recherche rapide par √©pisode
```

## Gestion des erreurs

### Erreurs Azure OpenAI

**Timeout apr√®s retry** :
```python
# Phase 1: max_retries=1 (2 tentatives), timeout=120s
try:
    response = await asyncio.wait_for(..., timeout=120)
except TimeoutError:
    if attempt < max_retries:
        await asyncio.sleep(2)
        continue
    raise TimeoutError("Timeout g√©n√©ration Phase 1")
```

**Client non configur√©** :
```python
if not self.client:
    raise ValueError("Client Azure OpenAI non configur√©")
```

**Validation format √©chou√©e** :
```python
if not self._is_valid_markdown_format(summary):
    raise ValueError("Format markdown invalide")
```

### Erreurs API

**Episode non trouv√©** : HTTP 404
```python
if not episode:
    raise HTTPException(status_code=404, detail="√âpisode non trouv√©")
```

**Transcription manquante** : HTTP 400
```python
if not episode.get("transcription_whisper"):
    raise HTTPException(status_code=400, detail="Transcription manquante")
```

**Validation r√©sum√© √©chou√©e** : HTTP 400
```python
is_valid, error_msg = _validate_summary(summary)
if not is_valid:
    raise HTTPException(status_code=400, detail=error_msg)
```

**Erreur interne** : HTTP 500
```python
except Exception as e:
    logger.error(f"Erreur g√©n√©ration: {e}")
    raise HTTPException(status_code=500, detail=str(e))
```

## Debug et logging

### Activation logs debug

```bash
export AVIS_CRITIQUES_DEBUG_LOG=1
```

**Logs additionnels** :
- Nombre de caract√®res g√©n√©r√©s par phase
- Extraits des r√©sum√©s Phase 1 et Phase 2 (500 premiers chars)
- Comparaison d√©taill√©e Phase 1 vs Phase 2
- Tentatives de retry

**Exemple** :
```python
if self._debug_log_enabled:
    logger.info(f"Phase 1 r√©ussie: {len(summary)} caract√®res g√©n√©r√©s")
    logger.info(f"üìÑ PHASE 1 OUTPUT (extrait):\n{summary[:500]}...")

    logger.info("=" * 80)
    logger.info("üîç COMPARAISON PHASE 1 vs PHASE 2:")
    logger.info("üìÑ summary_phase1 (brut):")
    logger.info(summary_phase1[:500] + "...")
    logger.info("-" * 80)
    logger.info("üìÑ summary (phase2, corrig√©):")
    logger.info(summary_phase2[:500] + "...")
    logger.info("=" * 80)
```

### Logs standards (toujours actifs)

```python
logger.info("‚úÖ Phase 1 et Fetch URL termin√©s")
logger.info(f"üîç Extraction m√©tadonn√©es depuis: {episode_page_url}")
logger.info(f"‚úÖ M√©tadonn√©es extraites: animateur={...}, critiques={...}")
logger.info(f"üìÑ Contenu page RadioFrance: {len(page_content)} caract√®res")
logger.info(f"üìù {len(corrections_applied)} correction(s) d√©tect√©e(s)")
logger.info("‚úÖ Phase 2 termin√©e")
```

## Tests

### Tests unitaires

**Fichier** : `tests/test_avis_critiques_generation_service.py`

**Skip conditionnel** (si Azure OpenAI non configur√©) :
```python
skip_if_no_azure = pytest.mark.skipif(
    os.getenv("AZURE_ENDPOINT") is None,
    reason="Azure OpenAI non configur√© (CI/CD)"
)

class TestAvisCritiquesGenerationService:
    @skip_if_no_azure
    async def test_generate_summary_phase1(self):
        # Test avec vraie API Azure OpenAI
        ...
```

**Coverage** :
- Phase 1 : g√©n√©ration, retry logic, validation format
- Phase 2 : correction, fallback, pr√©servation structure
- `generate_full_summary()` : orchestration parall√®le
- Validation : 5 crit√®res de validation r√©sum√©
- D√©tection corrections

### Tests d'int√©gration

**Fichier** : `tests/test_api_avis_critiques.py`

**Mocking** :
```python
@pytest.fixture
def mock_generation_service(monkeypatch):
    """Mock du service de g√©n√©ration."""
    mock_result = {
        "summary": "## 1. LIVRES DISCUT√âS...",
        "summary_phase1": "...",
        "metadata": {...},
        "corrections_applied": [],
        "warnings": []
    }

    async def mock_generate(*args, **kwargs):
        return mock_result

    monkeypatch.setattr(
        avis_critiques_generation_service,
        "generate_full_summary",
        mock_generate
    )
```

**Coverage endpoints** :
- `POST /api/avis-critiques/generate`
- `POST /api/avis-critiques/save` avec validation
- `GET /api/avis-critiques/episodes-sans-avis`
- `GET /api/avis-critiques/{episode_id}`

## Optimisations

### Ex√©cution parall√®le

**Pattern `asyncio.gather()`** :

```python
# Lancer Phase 1 et fetch URL en PARALL√àLE
summary_phase1, fetched_url = await asyncio.gather(
    self.generate_summary_phase1(transcription, episode_date),
    fetch_episode_url()  # Recherche + update MongoDB
)
```

**Gain de temps** : ~10-15 secondes √©conomis√©es par rapport √† ex√©cution s√©quentielle.

### Mise √† jour MongoDB automatique

Si URL RadioFrance non pr√©sente dans l'√©pisode :

```python
async def fetch_episode_url():
    episode = mongodb_service.get_episode_by_id(episode_id)
    titre = episode.get("titre")
    date = episode.get("date")
    duree = episode.get("duree")  # Dur√©e en secondes (optionnel)

    # Filtrage par dur√©e minimale pour √©viter les clips de livres individuels
    # RadioFrance publie deux types d'URLs par √©mission :
    # - √âmission compl√®te (~47 min) : URL souhait√©e
    # - Clip livre individuel (~9 min) : √† ignorer
    min_duration_seconds = None
    if duree and isinstance(duree, int) and duree > 0:
        min_duration_seconds = duree // 2  # 50% de la dur√©e connue

    # Recherche via RadioFrance service avec filtre dur√©e + date ¬±7j
    url = await radiofrance_service.search_episode_page_url(
        titre, date, min_duration_seconds=min_duration_seconds
    )

    if url:
        # Mise √† jour MongoDB imm√©diate
        mongodb_service.episodes_collection.update_one(
            {"_id": ObjectId(episode_id)},
            {"$set": {"episode_page_url": url}}
        )

    return url
```

**Logique de s√©lection de l'URL** (m√©thode `search_episode_page_url`) :

RadioFrance peut publier plusieurs types d'URLs pour la m√™me date de diffusion :
- **√âmission compl√®te** (ex: `le-masque-et-la-plume-du-dimanche-15-fevrier-2026-...`) : ~47 min
- **Clip livre individuel** (ex: `aqua-de-gaspard-koenig-...`) : ~9 min

Le service applique un double filtre sur chaque URL candidate :
1. **Date** : la date de la page RadioFrance doit √™tre dans un intervalle de ¬±7 jours par rapport √† la date de l'√©pisode (RadioFrance publie parfois 1-2 jours apr√®s diffusion)
2. **Dur√©e** (si `min_duration_seconds` fourni) : la dur√©e extraite de la page doit √™tre ‚â• `min_duration_seconds`

Structure JSON-LD RadioFrance (r√©elle) :
```json
{
  "@type": "RadioEpisode",
  "dateCreated": "2026-02-15T09:12:30.000Z",
  "mainEntity": {
    "@type": "AudioObject",
    "duration": "P0Y0M0DT0H47M40S"
  }
}
```

Note : la dur√©e est dans `mainEntity.duration` (format ISO 8601 complet), et la date dans `dateCreated` (champ `RadioEpisode`).

### Nettoyage metadata

**Probl√®me** : `page_text` peut contenir 10000+ caract√®res ‚Üí stockage excessif.

**Solution** : Retirer `page_text` avant sauvegarde MongoDB.

```python
metadata_clean = {
    k: v for k, v in metadata.items()
    if k != "page_text"
}

avis_data = {
    ...
    "metadata_source": metadata_clean,  # Sans page_text
    ...
}
```

## D√©pendances

### Services externes

**Requis** :
- `mongodb_service` : Acc√®s collections `episodes`, `avis_critiques`
- `radiofrance_service` : Recherche URL + extraction m√©tadonn√©es

**Import** :
```python
from .mongodb_service import mongodb_service
from .radiofrance_service import radiofrance_service
```

### Biblioth√®ques

```python
import asyncio  # Ex√©cution parall√®le
import openai  # Client Azure OpenAI
import re  # Validation format markdown
from datetime import datetime  # Timestamps, formatage dates
```

## √âvolutions futures possibles

1. **Support multi-mod√®les** : Permettre s√©lection GPT-4o vs GPT-4-turbo via configuration
2. **M√©triques de qualit√©** : Score de confiance par livre (bas√© sur consensus critiques)
3. **Export formats** : JSON structur√©, CSV pour analyse statistique
4. **Cache r√©sultats** : √âviter r√©g√©n√©ration si transcription inchang√©e
5. **Am√©lioration d√©tection corrections** : Algorithme diff plus pr√©cis (Levenshtein distance)
6. **Retry intelligent** : Backoff exponentiel au lieu de d√©lai fixe
7. **Streaming progressif** : Retourner r√©sum√© par chunks (Server-Sent Events)
8. **Validation syntaxe HTML** : V√©rifier balances `<span>` pour notes color√©es
